[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "smacof at 50",
    "section": "",
    "text": "Note: This manual is a working manuscript which will be expanded/updated frequently. All suggestions for improvement are welcome. All Rmd, tex, html, pdf, R, and C files are in the public domain and can be copied, modified, and used by anybody in any way they see fit. Attribution will be appreciated, but is not required. The files can be found at https://github.com/deleeuw in the repositories smacofCode, smacofManual, and smacofExamples.\n\nPreface\nThis manual is definitely not an impartial and balanced review of all of multidimensional scaling (MDS) theory and history. It emphasizes computation, and the mathematics needed for computation. In addition, it is a summary of over 50 years of MDS work by me, either solo or together with my many excellent current or former co-workers and co-authors. It is heavily biased in favor of the smacof formulation of MDS (De Leeuw (1977), De Leeuw and Heiser (1977), De Leeuw and Mair (2009), Mair, Groenen, and De Leeuw (2022)), and the corresponding majorization (or MM) algorithms. And, moreover, I am shamelessly squeezing in as many references to my published and unpublished work as possible, with links to the corresponding pdf’s if they are available. Thus this book is also a jumpstation into my bibliography.\nI have not organized the book along historical lines because most of the early techniques and results have been either drastically improved or completely abandoned. Nevertheless, some personal historical perspective may be useful. I will put most of it in this preface, so uninterested readers can easily skip it.\nI got involved in MDS in 1968 when John van de Geer returned from a visit to Clyde Coombs in Michigan and started the Department of Data Theory in the Division of Social Sciences at Leiden University. I was John’s first hire, although I was still a graduate student at the time.\nRemember that Clyde Coombs was chairing the Michigan Mathematical Psychology Program, and he had just published his remarkable book “A Theory of Data” (Coombs (1964)). The name of the new department in Leiden was taken from the title of that book, and Coombs was one of the first visitors to give a guest lecture there.\nThis is maybe the place to clear up some possible misunderstandings about the name “Data Theory”. Coombs was mainly interested in a taxonomy of data types, and in pointing out that “data” were not limited to a table or data-frame of objects by variables. In addition, there were also similarity ratings, paired comparisons, and unfolding data. Coombs also emphasized that data were often non-metric, i.e. ordinal or categorical, and that it was possible to analyze these ordinal or categorical relationships directly, without first constructing numerical scales to which classical techniques could be applied. One of the new techniques discussed in Coombs (1964) was a ordinal form of MDS, in which not only the data but also the representation of the data in Euclidean space were non-metric.\nJohn van de Geer had just published Van de Geer (1967). In that book, and in the subsequent book Van de Geer (1971), he developed his unique geometric approach to multivariate analysis. Relationship between variables, and between variables and individuals, were not just discussed using matrix algebra, but were also visualized in diagrams. This was related to the geometric representations in Coombs’ Theory of Data, but it concentrated on numerical data in the form of rectangular matrices of objects by variables.\nLooking back it is easy to see that both Van de Geer and Coombs influenced my approach to data analysis. I inherited the emphasis on non-metric data and on visualization. But, from the beginning, I interpreted “Data Theory” as “Data Analysis”, with my emphasis shifting to techniques, loss functions, implementations, algorithms, optimization, computing, and programming. This is of interest because in 2020 my former Department of Statistics at UCLA, together with the Department of Mathematics, started a bachelor’s program in Data Theory, in which “Emphasis is placed on the development and theoretical support of a statistical model or algorithmic approach. Alternatively, students may undertake research on the foundations of data science, studying advanced topics and writing a senior thesis.” This sounds like a nice hybrid of Data Theory and Data Analysis, with a dash of computer science mixed in.\nComputing and optimization were in the air in 1968, not so much because of Coombs, but mainly because of Roger Shepard, Joe Kruskal, and Doug Carroll at Bell Labs in Murray Hill. John’s other student Eddie Roskam and I were fascinated by getting numerical representations from ordinal data by minimizing explicit least squares loss functions. Eddie wrote his dissertation in 1968 (Roskam (1968)). In 1973 I went to Bell Labs for a year, and Eddie went to Michigan around the same time to work with Jim Lingoes, resulting in Lingoes and Roskam (1973).\nMy first semi-publication was De Leeuw (1968), quickly followed by a long sequence of other, admittedly rambling, internal reports. Despite this very informal form of publication the sheer volume of them got the attention of Joe Kruskal and Doug Carroll, and I was invited to spend the academic year 1973-1974 at Bell Laboratories. That visit somewhat modified my cavalier approach to publication, but I did not become half-serious in that respect until meeting with Forrest Young and Yoshio Takane at the August 1975 US-Japan seminar on MDS in La Jolla. Together we used the alternating least squares approach to algorithm construction that I had developed since 1968 into a quite formidable five-year publication machine, with at its zenith Takane, Young, and De Leeuw (1977).\nIn La Jolla I gave the first presentation of the majorization method for MDS, later known as smacof, with the first formal convergence proof. The canonical account of smacof was published in a conference paper (De Leeuw (1977)). Again I did not bother to get the results into a journal or into some other more effective form of publication. The basic theory for what became known as smacof was also presented around the same time in another book chapter De Leeuw and Heiser (1977).\nIn 1978 I was invited to the Fifth International Symposium on Multivariate Analysis in Pittsburgh to present what eventually became De Leeuw and Heiser (1980). There I met Nan Laird, one of the authors of the basic paper on the EM algorithm (Dempster, Laird, and Rubin (1977)). I remember enthusiastically telling her on the conference bus that EM and smacof were both special case of the general majorization approach to algorithm construction, which was consequently born around the same time. But that is a story for a companion volume, which currently only exists in a very preliminary stage (https://github.com/deleeuw/bras).\nMy 1973 PhD thesis (De Leeuw (1973), reprinted as De Leeuw (1984)) was actually my second attempt at a dissertation. I had to get a PhD, any PhD, before going to Bell Labs, because of the difference between the Dutch and American academic title and reward systems. I started writing a dissertation on MDS, in the spirit of what later became De Leeuw and Heiser (1982). But halfway through I lost interest and got impatient, and I decided to switch to nonlinear multivariate analysis. This second attempt did produced a finished dissertation (De Leeuw (1973)), which grew over time, with the help of multitudes, into Gifi (1990). But that again is a different history, which I will tell some other time in yet another companion volume (https://github.com/deleeuw/gifi). For a long time I did not do much work on MDS, until the arrival of Patrick Mair and the R language led to a resurgence of my interest, and ultimately to De Leeuw and Mair (2009) and Mair, Groenen, and De Leeuw (2022).\nI consider this MDS book to be a summary and extension of the basic papers De Leeuw (1977), De Leeuw and Heiser (1977), De Leeuw and Heiser (1980), De Leeuw and Heiser (1982), and De Leeuw (1988), all written 30-40 years ago. Footprints in the sands of time. It can also be seen as an elaboration of the more mathematical and computational sections of the excellent and comprehensive textbook of Borg and Groenen (2005). That book has much more information about the origins, the data, and the applications of MDS, as well as on the interpretation of MDS solutions. In this book I concentrate almost exclusively on the mathematical, computational, and programming aspects of MDS.\nFor those who cannot get enough of me, there is a data base of my published and unpublished reports and papers since 1965, with links to pdf’s, at https://jansweb.netlify.app/publication/.\nThere are many, many people I have to thank for my scientific education. Sixty years is a long time, and consequently many excellent teachers and researchers have crossed my path. I will gratefully mention the academics who had a major influence on my work and who are not with us any more, since I will join them in the not too distant future: Louis Guttman (died 1987), Clyde Coombs (died 1988), Warren Torgerson (died 1999), Forrest Young (died 2006), John van de Geer (died 2008), Joe Kruskal (died 2010), Doug Carroll (died 2011), and Rod McDonald (died 2012).\nI will also use this preface to thank Rstudio, in particular J.J. Allaire, Hadley Wickham, and Yihui Xi, for their contributions to the R universe, and for their promotion of open source software and open access publications. Not too long ago I was an ardent LaTeX user, firmly convinced I would never use anything else again in my lifetime. In the same way that I was convinced before that I would never use anything besides, in that order, FORTRAN, PL/I, APL, and (X)Lisp. And PHP/Apache/MySQL. But I lived too long. And then, in my dotage, lo and behold, R, Rstudio, (R)Markdown, Quarto, ggplot, bookdown, blogdown, Git, Github, and Netlify came along.\n\n\n\n\n\nForrest Young, Bepi Pinner, Jean-Marie Bouroche, Yoshio Takane, Jan de Leeuw at La Jolla, August 1975\n\n\n\n\nIn this manual we study the smacof family of Multidimensional Scaling (MDS) techniques. In MDS the data consist of some type of information about the dissimilarities between a pairs of objects. These objects can be anything: individuals, variables, colors, locations, chemicals, molecules, works of Plato, political parties, Morse code signals, and so on. The dissimilarities can be approximate or imprecise distances, dissimilarity judgments, import/export tables, sociometric choices, and so on. They generally are distance-like, but we do not expect them to satisfy the triangle inequality, and in general not even non-negativity and symmetry. Similarities, such as confusion probabilities, correlations, or preferences, are always converted in some way or another to dissimilarities before they can serve as data for MDS.\nThe information we have about these dissimilarities can be numerical, ordinal, or categorical. Thus we may have the actual values of some or all of the dissimilarities, we may know their rank order, or we may have a classification of them into a small number of qualitative bins.\nLet’s formalize this, and introduce some notation at the same time. The set of ojects is \\(\\mathfrak{O}\\). For example, it can be the set of all cities with more than 10,000 inhabitants. In our MDS analysis we only use \\(O:=(o_1,\\cdots,o_n)\\), an n-tuple (i.e. a finite sequence) of \\(n\\) different elements of \\(\\mathfrak{O}\\), for example \\(n\\) capital cities selected from \\(\\mathfrak{O}\\). If you want to, you can call \\(O\\) a sample from \\(\\mathfrak{O}\\). It is entirely possible, however, that \\(\\mathfrak{O}\\) has only \\(n\\) elements, in which case \\(O\\) is just an permutation of the elements of \\(\\mathfrak{O}\\).\nA dissimilarity is a function \\(\\delta\\) on all pairs of objects, with values in a set \\(\\mathfrak{D}\\). It can be, for example, the time in seconds for an airline flight from city one to city two. Thus \\(\\delta:\\mathfrak{O}\\otimes\\mathfrak{O}\\Rightarrow\\mathfrak{D}\\). A dissimilaritry is numerical if \\(\\mathfrak{D}\\) is subset of real line, it is ordinal if \\(\\mathfrak{D}\\) is a partially ordered set, and it is nominal if \\(\\mathfrak{D}\\) is neither. Or a dissimilarty is nominal if \\(\\mathfrak{D}\\) is any set, and we choose to ignore the ordinal and numerical information if it is there. No matter what \\(\\mathfrak{D}\\) is, we suppose it always has the element \\(\\mathit{NA}\\) to indicate missing dissimilarities. Cities may not have airports, for example, or we just don’t have the information about the airline distances. Define \\(\\delta_{ij}:=\\delta(o_i,o_j)\\) and \\(\\Delta:=\\delta(O\\times O)\\). We can think of \\(\\Delta\\) and an \\(n\\times n\\) matrix with elements in \\(\\mathfrak{D}\\).\nMDS techniques map the objects \\(o_i\\) into points \\(x_i\\) in some metric space \\(\\langle\\mathfrak{X},d\\rangle\\) in such a way that the distances between pairs of points approximate the dissimilarities of the corresponding pairs of objects. Thus we want to find a map \\(x:\\mathfrak{O}\\rightarrow\\mathfrak{X}\\) that produces an n-tuple \\(X=(x_1,\\cdots,x_n)\\) of elements of \\(\\mathfrak{X}\\), where \\(x_i:=x(o_i)\\). Also define \\(d_{ij}:=d(x_i,x_j)\\) and \\(D(X):=d(X\\times X\\). Unlike the dissimilarities the \\(d_{ij}\\) are always numerical, because distances are. So MDS finds \\(X\\) such that \\(D(X)\\approx\\Delta\\).\nFor numerical dissimilarities it is clear what “approximation” means, we simply want the distances and the corresponding dissimilarities to be numerically close. Because there are generally many dissimilarities and distances a combined measure of closeness can still be defined in many different ways. For ordinal and nominal dissimilarities the notion of approximation is less clear, and we have to develop more specialized techniques to measure how well the distances fit the dissimilarities.\n\n\n\n\nBorg, I., and P. J. F. Groenen. 2005. Modern Multidimensional Scaling. Second Edition. Springer.\n\n\nCoombs, C. H. 1964. A Theory of Data. Wiley.\n\n\nDe Leeuw, J. 1968. “Nonmetric Multidimensional Scaling.” Research Note 010-68. Department of Data Theory FSW/RUL. https://jansweb.netlify.app/publication/deleeuw-r-68-g/deleeuw-r-68-g.pdf.\n\n\n———. 1973. “Canonical Analysis of Categorical Data.” PhD thesis, University of Leiden, The Netherlands. https://jansweb.netlify.app/publication/deleeuw-b-73/deleeuw-b-73.pdf.\n\n\n———. 1977. “Applications of Convex Analysis to Multidimensional Scaling.” In Recent Developments in Statistics, edited by J. R. Barra, F. Brodeau, G. Romier, and B. Van Cutsem, 133–45. Amsterdam, The Netherlands: North Holland Publishing Company.\n\n\n———. 1984. Canonical Analysis of Categorical Data. Leiden, The Netherlands: DSWO Press. https://jansweb.netlify.app/publication/deleeuw-b-84/deleeuw-b-84.pdf.\n\n\n———. 1988. “Convergence of the Majorization Method for Multidimensional Scaling.” Journal of Classification 5: 163–80.\n\n\nDe Leeuw, J., and W. J. Heiser. 1977. “Convergence of Correction Matrix Algorithms for Multidimensional Scaling.” In Geometric Representations of Relational Data, edited by J. C. Lingoes, 735–53. Ann Arbor, Michigan: Mathesis Press.\n\n\n———. 1980. “Multidimensional Scaling with Restrictions on the Configuration.” In Multivariate Analysis, Volume V, edited by P. R. Krishnaiah, 501–22. Amsterdam, The Netherlands: North Holland Publishing Company.\n\n\n———. 1982. “Theory of Multidimensional Scaling.” In Handbook of Statistics, Volume II, edited by P. R. Krishnaiah and L. Kanal. Amsterdam, The Netherlands: North Holland Publishing Company.\n\n\nDe Leeuw, J., and P. Mair. 2009. “Multidimensional Scaling Using Majorization: SMACOF in R.” Journal of Statistical Software 31 (3): 1–30. https://www.jstatsoft.org/article/view/v031i03.\n\n\nDempster, A. P., N. M. Laird, and D. B. Rubin. 1977. “Maximum Likelihood for Incomplete Data via the EM Algorithm.” Journal of the Royal Statistical Society B39: 1–38.\n\n\nGifi, A. 1990. Nonlinear Multivariate Analysis. New York, N.Y.: Wiley.\n\n\nLingoes, J. C., and E. E. Roskam. 1973. “A Mathematical and Empirical Analysis of Two Multidimensional Scaling Algorithms.” Psychometrika 38: Monograph Supplement.\n\n\nMair, P., P. J. F. Groenen, and J. De Leeuw. 2022. “More on Multidimensional Scaling in R: smacof Version 2.” Journal of Statistical Software 102 (10): 1–47. https://www.jstatsoft.org/article/view/v102i10.\n\n\nRoskam, E. E. 1968. “Metric Analysis of Ordinal Data in Psychology.” PhD thesis, University of Leiden.\n\n\nTakane, Y., F. W. Young, and J. De Leeuw. 1977. “Nonmetric Individual Differences in Multidimensional Scaling: An Alternating Least Squares Method with Optimal Scaling Features.” Psychometrika 42: 7–67.\n\n\nVan de Geer, J. P. 1967. Inleiding in de Multivariate Analyse. Van Loghum Slaterus.\n\n\n———. 1971. Introduction to Multivariate Analysis for the Social Sciences. San Francisco, CA: Freeman.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "2 Brief History\nIn this manual we study the smacof family of Multidimensional Scaling (MDS) techniques. In MDS the data consist of some type of information about the dissimilarities between a pairs of objects. These objects can be anything: individuals, variables, colors, locations, chemicals, molecules, works of Plato, political parties, Morse code signals, and so on. The dissimilarities can be approximate or imprecise distances, dissimilarity judgments, import/export tables, sociometric choices, and so on. They generally are distance-like, but we do not expect them to satisfy the triangle inequality, and in general not even non-negativity and symmetry. Similarities, such as confusion probabilities, correlations, or preferences, are always converted in some way or another to dissimilarities before they can serve as data for MDS.\nThe information we have about these dissimilarities can be numerical, ordinal, or categorical. Thus we may have the actual values of some or all of the dissimilarities, we may know their rank order, or we may have a classification of them into a small number of qualitative bins.\nLet’s formalize this, and introduce some notation at the same time. The set of ojects is \\(\\mathfrak{O}\\). For example, it can be the set of all cities with more than 10,000 inhabitants. In our MDS analysis we only use \\(O:=(o_1,\\cdots,o_n)\\), an n-tuple (i.e. a finite sequence) of \\(n\\) different elements of \\(\\mathfrak{O}\\), for example \\(n\\) capital cities selected from \\(\\mathfrak{O}\\). If you want to, you can call \\(O\\) a sample from \\(\\mathfrak{O}\\). It is entirely possible, however, that \\(\\mathfrak{O}\\) has only \\(n\\) elements, in which case \\(O\\) is just an permutation of the elements of \\(\\mathfrak{O}\\).\nA dissimilarity is a function \\(\\delta\\) on all pairs of objects, with values in a set \\(\\mathfrak{D}\\). It can be, for example, the time in seconds for an airline flight from city one to city two. Thus \\(\\delta:\\mathfrak{O}\\otimes\\mathfrak{O}\\Rightarrow\\mathfrak{D}\\). A dissimilaritry is numerical if \\(\\mathfrak{D}\\) is subset of real line, it is ordinal if \\(\\mathfrak{D}\\) is a partially ordered set, and it is nominal if \\(\\mathfrak{D}\\) is neither. Or a dissimilarty is nominal if \\(\\mathfrak{D}\\) is any set, and we choose to ignore the ordinal and numerical information if it is there. No matter what \\(\\mathfrak{D}\\) is, we suppose it always has the element \\(\\mathit{NA}\\) to indicate missing dissimilarities. Cities may not have airports, for example, or we just don’t have the information about the airline distances. Define \\(\\delta_{ij}:=\\delta(o_i,o_j)\\) and \\(\\Delta:=\\delta(O\\times O)\\). We can think of \\(\\Delta\\) and an \\(n\\times n\\) matrix with elements in \\(\\mathfrak{D}\\).\nMDS techniques map the objects \\(o_i\\) into points \\(x_i\\) in some metric space \\(\\langle\\mathfrak{X},d\\rangle\\) in such a way that the distances between pairs of points approximate the dissimilarities of the corresponding pairs of objects. Thus we want to find a map \\(x:\\mathfrak{O}\\rightarrow\\mathfrak{X}\\) that produces an n-tuple \\(X=(x_1,\\cdots,x_n)\\) of elements of \\(\\mathfrak{X}\\), where \\(x_i:=x(o_i)\\). Also define \\(d_{ij}:=d(x_i,x_j)\\) and \\(D(X):=d(X\\times X\\). Unlike the dissimilarities the \\(d_{ij}\\) are always numerical, because distances are. So MDS finds \\(X\\) such that \\(D(X)\\approx\\Delta\\).\nFor numerical dissimilarities it is clear what “approximation” means, we simply want the distances and the corresponding dissimilarities to be numerically close. Because there are generally many dissimilarities and distances a combined measure of closeness can still be defined in many different ways. For ordinal and nominal dissimilarities the notion of approximation is less clear, and we have to develop more specialized techniques to measure how well the distances fit the dissimilarities.\nDe Leeuw and Heiser (1980)\nThis section has a different emphasis. We limit ourselves to developments in Euclidean MDS, and to contributions with direct computational consequences that have a direct or indirect link to psychometrics, and to work before 1960. This is reviewed ably in the presidential address of W. S. Torgerson (1965).\nOur history review takes the form of brief summaries of what we consider to be milestone papers or books.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#prehistory",
    "href": "intro.html#prehistory",
    "title": "1  Introduction",
    "section": "2.1 Prehistory",
    "text": "2.1 Prehistory\nThe prehistory of MDS is defined as any publication before Young and Householder (1938).\nstumpf 1883\n\nUnter Distanzen aber verstehen wir, das Wort hier ebenfalls in einem für Manche ungewohnt weiten Sinne nehmend, nicht blos räumliche und zeitliche sondern auch qualitative und solche der Intensität, und definieren das Wort durch: Grade der Unähnlichkeit.\n\nFrom the translation\n\nTaking the word “distances” in a sense uncommonly broad for many, however, we mean here not only spatial and temporal ones, but also qualitative ones as well as ones of intensity, and define the word by degrees of dissimilarity.\n\nfisher 1922 boyden 1932/1935 goldmeier 1937\nrichardson 1938 klingberg 1941 gulliksen 1946 attneave 1950 ekman 1954\ntorgerson 1951 torgerson 1952 messick_abelson 1956\nreviewed in De Leeuw and Heiser (1980).\nYoung-Householder, etc.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#torgerson",
    "href": "intro.html#torgerson",
    "title": "1  Introduction",
    "section": "2.2 Torgerson",
    "text": "2.2 Torgerson\nW. S. Torgerson (1952) W. S. Torgerson (1965)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#bell-laboratories",
    "href": "intro.html#bell-laboratories",
    "title": "1  Introduction",
    "section": "2.3 Bell Laboratories",
    "text": "2.3 Bell Laboratories\nShepard (1962a) Shepard (1962b)\nKruskal (1964a) Kruskal (1964b)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#guttman-lingoes",
    "href": "intro.html#guttman-lingoes",
    "title": "1  Introduction",
    "section": "2.4 Guttman-Lingoes",
    "text": "2.4 Guttman-Lingoes\nGuttman (1968)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#alternating-least-squares",
    "href": "intro.html#alternating-least-squares",
    "title": "1  Introduction",
    "section": "2.5 Alternating Least Squares",
    "text": "2.5 Alternating Least Squares",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#majorization",
    "href": "intro.html#majorization",
    "title": "1  Introduction",
    "section": "2.6 Majorization",
    "text": "2.6 Majorization\nDe Leeuw (1977) De Leeuw and Heiser (1977)\nThere was some early work by Richardson, Messick, Abelson and Torgerson who combined Thurstonian scaling of similarities with the mathematical results of Schoenberg (1935) and Young and Householder (1938).\nDespite these early contributions it makes sense, certainly from the point of view of my personal history, but probably more generally, to think of MDS as starting as a widely discussed, used, and accepted technique since the book by W. S. Torgerson (1958). This was despite the fact that in the fifties and sixties computing eigenvalues and eigenvectors of a matrix of size 20 or 30 was still a considerable challenge.\nA few years later the popularity of MDS got a large boost by developments centered at Bell Telephone Laboratories in Murray Hill, New Jersey, the magnificent precursor of Silicon Valley. First there was nonmetric MDS by Shepard (1962a), Shepard (1962b) and Kruskal (1964a), Kruskal (1964b), And later another major development was the introduction of individual difference scaling by Carroll and Chang (1970) and Harshman (1970). Perhaps even more important was the development of computer implementations of these new techniques. Some of the early history of nonmetric MDS is in De Leeuw (2017a).\nAround the same time there were interesting theoretical contributions in Coombs (1964), which however did not much influence the practice of MDS. ….. And several relatively minor variations of the Bell Laboratories approach were proposed by Guttman (1968), but Guttman’s influence on further MDS implementations turned out to be fairly localized and limited.\nThe main development in comptational MDS after the Bell Laboratories surge was probably smacof. Initially, in De Leeuw (1977), this stood for Scaling by Maximizing a Convex Function. Later it was also used to mean Scaling by Majorizing a Complicated Function. Whatever. In this book smacof just stands for smacof. No italics, no boldface, no capitals.\nThe first smacof programs were written in 1977 in FORTRAN at the Department of Data Theory in Leiden (Heiser and De Leeuw (1977)). Eventually they migrated to SPSS (for example, Meulman and Heiser (2012)) and to R (De Leeuw and Mair (2009)). The SPSS branch, now the IBM SPSS branch, and the R branch have diverged somewhat, and they continue to be developed independently.\nParallel to this book there is an attempt to rewrite the various smacof programs in C, with the necessary wrappers to call them from R (De Leeuw (2017b)). The C code, with makefiles and test routines, is at github.com/deleeuw/smacof",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#kruskals-stress",
    "href": "intro.html#kruskals-stress",
    "title": "1  Introduction",
    "section": "3.1 Kruskal’s stress",
    "text": "3.1 Kruskal’s stress\nDefinition @ref(eq:stressall) differs from Kruskal’s original stress in at least three ways: in Kruskal’s use of the square root, in our use of weights, and in our different approach to normalization.\nWe have paid so much attention to Kruskal’s original definition, because the choices made there will play a role in the normalization discussion in the ordinal scaling chapter (section @ref(nmdsnorm)), in the comparison of Kruskal’s and Guttman’s approach to ordinal MDS (sections @ref(nmdskruskal) and @ref(nmdsguttman)), and in our discussions about the differences between Kruskal’s stress @ref(eq:kruskalstressfinal) and smacof’s stress @ref(eq:stressall) in the next three sections of this chapter.\n\n3.1.0.1 Square root\nLet’s discuss the square root first. Using it or not using it does not make a difference for the minimization problem. Using the square root, however, does give a more sensible root-mean-square scale, in which stress is homogeneous of degree one, instead of degree two. But I do not want to compute all those unnecessary square roots in my algorithms, and I do not want to drag them along through my derivations. Moreover the square root potentially causes problems with differentiability at those \\(X\\) where \\(\\sigma(X)\\) is zero. Thus, througout the book, we do not use the square root in our formulas and derivations. In fact, we do not even use it in our computer programs, except at the very last moment when we return the final stress after the algorithm has completed.\n\n\n3.1.0.2 Weights\nThere were no weights \\(W=\\{w_{ij}\\}\\) in the original definition of stress by Kruskal (1964a), and neither are they there in most of the basic later contributions to MDS by Guttman, Lingoes, Roskam, Ramsay, or Young. We will use weights throughout the book, because they have various interesting applications within basic MDS, without unduly complicating the derivations and computations. In Groenen and Van de Velden (2016), section 6, the various uses of weights in the stress loss function are enumerated. They generously, and correctly, attribute the consistent use of weights in MDS to me. I quote from their paper:\n\n\nHandling missing data is done by specifying \\(w_{ij} = 0\\) for missings and 1 otherwise thereby ignoring the error corresponding to the missing dissimilarities.\nCorrecting for nonuniform distributions of the dissimilarities to avoid dominance of the most frequently occurring dissimilarities.\nMimicking alternative fit functions for MDS by minimizing Stress with \\(w_{ij}\\) being a function of the dissimilarities.\nUsing a power of the dissimilarities to emphasize the ﬁtting of either large or small dissimilarities.\nSpecial patterns of weights for speciﬁc models.\nUsing a speciﬁc choice of weights to avoid nonuniqueness.\n\n\nIn some situations, for example for huge data sets, it is computationally convenient, or even necessary, to minimize the influence of the weights on the computations. We can use majorization to turn the problem from a weighted least squares problem to an iterative unweighted least squares problem. The technique, which we call unweighting, is discussed in detail in section @ref(minunweight).\n\n\n3.1.0.3 Normalization\nThis section deals with a rather trivial problem, which has however caused problems in various stages of smacof’s 50-year development history. Because the problem is trivial, and the choices that must be made are to a large extent arbitrary, it has been overlooked and somewhat neglected.\nIn basic MDS we scale the weights and dissimilarities. It is clear that if we multiply the weights or dissimilarities by a constant, then the optimal approximating distances \\(D(X)\\) and the optimal configuration \\(X\\) will be multiplied by the same constant. That is exactly why Kruskal’s raw stress had to be normalized. Consequently we in basic MDS we always scale weights and dissimilarities by\n\\[\\begin{align}\n\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}&=1,(\\#eq:scaldiss1)\\\\\n\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}^{\\ }\\delta_{ij}^2&=1.(\\#eq:scaldiss2)\n\\end{align}\\]\nThis simplifies our formulas and makes them look better (see, for example, section @ref(propexpand) and section @ref(secrhostress)). It presupposes, of course, that \\(w_{ij}\\delta_{ij}\\not=0\\) for at least one \\(i\\not= j\\), which we will happily assume in the sequel, because otherwise the MDS problem is trivial. Note that if all weights are equal (which we call the unweighted case) then they are equal to \\(1/\\binom{n}{2}\\) and thus we require \\(\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}\\delta_{ij}^2=\\frac12n(n-1)\\).\nUsing normalized dissimilarities amounts to the same defining stress as \\[\\begin{equation}\n\\sigma(X)=\\frac12\\frac{\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}(\\delta_{ij}^2-d_{ij}(X))^2}{\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}\\delta_{ij}^2}.\n(\\#eq:stressrat)\n\\end{equation}\\]\nThis is useful to remember when we discuss the various normalizations for non-metric MDS in section @ref(nmdsnorm).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#seclocglob",
    "href": "intro.html#seclocglob",
    "title": "1  Introduction",
    "section": "3.2 Local and Global Minima",
    "text": "3.2 Local and Global Minima\nIn basic MDS our goal is to compute both \\(\\min_X\\sigma(X)\\) and \\(\\mathop{\\text{Argmin}}_X\\sigma(X)\\), where \\(\\sigma(X)\\) is defined as @ref(eq:stressall), and where we minimize over all configurations in \\(\\mathbb{R}^{n\\times p}\\).\nIn this book we study both the properties of the stress loss function and a some of its generalizations, and the various ways to minimize these loss functions over configurations (and sometimes over transformations of the dissimilarities as well).\nEmphasis local minima\nCompute stationary points",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#partitioning-loss",
    "href": "intro.html#partitioning-loss",
    "title": "1  Introduction",
    "section": "3.3 Partitioning Loss",
    "text": "3.3 Partitioning Loss",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#non-linear-mds",
    "href": "intro.html#non-linear-mds",
    "title": "1  Introduction",
    "section": "4.1 Non-linear MDS",
    "text": "4.1 Non-linear MDS",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#gennonmetric",
    "href": "intro.html#gennonmetric",
    "title": "1  Introduction",
    "section": "4.2 Non-metric MDS",
    "text": "4.2 Non-metric MDS\nBasic MDS is a form of Metric Multidimensional Scaling or MMDS, in which dissimilarities are either known or missing. In chapter @ref(nonmtrmds) we relax this assumption. Dissimilarities may be partly known, for example we may know they are in some interval, we may only know their order, or we may know them up to some smooth transformation. MDS with partly known dissimilarities is Non-metric Multidimensional Scaling or NMDS. Completely unknown (missing) dissimilarities are an exception, because we can just handle this in basic MDS by setting the corresponding weights equal to zero.\nIn NMDS we minimize stress over all configurations, but also over the unknown dissimilarities. What we know about them (the interval they are in, the transformations that are allowed, the order they are in) defines a subset of the space of non-negative, hollow, and symmetric matrices. Any matrix in that subset is a matrix of what Takane, Young, and De Leeuw (1977) call disparities, i.e. imputed dissimilarities. The imputation provides the missing information and transforms the non-numerical information we have about the dissimilarities into a numerical matrix of disparities. Clearly this is an optimistic imputation, in the sense that it chooses from the set of admissible disparities to minimize stress (for a given configuration).\nOne more terminological point. Often non-metric is reserved for ordinal MDS, in which we only know a (partial or complete) order of the dissimilarities. Allowing linear or polynomial transformations of the dissimilarities, or estimating an additive constant, is then supposed to be a form of metric MDS. There is something to be said for that. Maybe it makes sense to distinguish non-metric in the wide sense (in which stress must be minimized over both \\(X\\) and \\(\\Delta\\)) and non-metric in the narrow sense in which the set of admissible disparities is defined by linear inequalities. Nonmetric in the narrow sense will also be called ordinal MDS or OMDS.\nIt is perhaps useful to remember that Kruskal (1964a) introduced explicit loss functions in MDS to put the somewhat heuristic NMDS techniques of Shepard (1962a) onto a firm mathematical and computational foundation. Thus, more or less from the beginning of iterative least squares MDS, there was a focus on non-metric rather than metric MDS, and this actually contributed a great deal to the magic and success of the technique. In this book most of the results are derived for basic MDS, which is metric MDS, with non-metric MDS as a relatively straightforward extension not discussed until chapter @ref(nonmtrmds). So, at least initially, we take the numerical values of the dissimilarities seriously, as do W. S. Torgerson (1958) and Shepard (1962a), Shepard (1962b).\nIt may be the case that in the social and behavioural sciences only the ordinal information in the dissimilarities is reliable and useful. But, since 1964, MDS has also been applied in molecular conformation, chemometrics, genetic sequencing, archelogical seriation, and in network design and location analysis. In these areas the numerical information in the dissimilarities is usually meaningful and should not be thrown out right away. Also, the use of the Shepard plot, with dissimilarities on the horizontal axis and fitted distances on the vertical axis, suggests there is more to dissimilarities than just their rank order.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#genfstress",
    "href": "intro.html#genfstress",
    "title": "1  Introduction",
    "section": "4.3 Fstress and Friends",
    "text": "4.3 Fstress and Friends\nInstead of defining the residuals in the least squares loss function as \\(\\delta_{ij}-d_{ij}(X)\\) chapter @ref(chrstress) discusses the more general cases where the residuals are \\(f(\\delta_{ij})-g(d_{ij}(X))\\) for some known non-negative increasing function \\(f\\). This defines the fstress loss function.\nIf \\(f(x)=x^r\\) with \\(r&gt;0\\) then fstress is called rstress. Thus stress is rstress with \\(r=1\\), also written as 1stress or \\(\\sigma_1\\). In more detail we will also look at \\(r=2\\), which is called sstress by Takane, Young, and De Leeuw (1977). In chapter @ref(chsstressstrain) we look at the problem of minimizing sstress and weighted version strain. The case of rstress with \\(r\\rightarrow 0\\) is also of interest, because it leads to the loss function in Ramsay (1977).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#gencons",
    "href": "intro.html#gencons",
    "title": "1  Introduction",
    "section": "4.4 Constraints",
    "text": "4.4 Constraints\nInstead of minimizing stress over all \\(X\\) in \\(\\mathbb{R}^{n\\times p}\\) we will look in chapter @ref(cmds) at various generalizations where minimization is over a subset \\(\\mathcal{\\Omega}\\) of \\(\\mathbb{R}^{n\\times p}\\). This is often called Constrained Multidimensional Scaling or CMDS.\nThe distinction may be familiar from factor analysis, where we distinguish between exploratory and confirmatory factor analysis. If we have prior information about the parameters then incorporating that prior information in the analysis will generally lead to more precise and more interpretable estimates. The risk is, of course that if our prior information is wrong, if it is just prejudice, then we will have a solution which is precise but incorrect. We have the famous trade-off between bias and variance. In MDS this trade-off does not seem to apply directly, because the necessary replication frameworks are missing.\nand we do not attach much value to locating the true configuration.\nPrimal and Dual\n\\[\n\\min_{X\\in\\Omega}\\sigma(X)\n\\]\n\\[\n\\min_X\\sigma(X)+\\lambda\\kappa(X,\\Omega)\n\\] where \\(\\kappa(X,\\Omega)\\geq 0\\) and \\(\\kappa(X,\\Omega)=0\\) if and only if \\(X\\in\\Omega\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#inreplic",
    "href": "intro.html#inreplic",
    "title": "1  Introduction",
    "section": "4.5 Individual Differences",
    "text": "4.5 Individual Differences\nNow consider the situation in which we have \\(m\\) different dissimilarity matrices \\(\\Delta_k\\) and \\(m\\) different weight matrices \\(W_k\\). We generalize basic MDS by defining \\[\\begin{equation}\n\\sigma(X_1,\\cdots,X_m):=\\frac12\\sum_{k=1}^m\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ijk}(\\delta_{ijk}-d_{ij}(X_k))^2,\n(\\#eq:replistress)\n\\end{equation}\\] and minimize this over the \\(X_k\\).\nThere are two simple ways to deal with this generalization. The first is to put no further constraints on the \\(X_k\\). This means solving \\(m\\) separate basic MDS problems, one for each \\(k\\). The second way is to require that all \\(X_k\\) are equal. As shown in more detail in section @ref(indifrepl) this reduced to a single basic MDS problem with dissimilarities that are a weighted sum of the \\(\\Delta_k\\). So both these approaches do not really bring anything new.\nMinimizing @ref(eq:replistress) becomes more interesting if we constrain the \\(X_k\\) in various ways. Usually this is done by making sure they have a component that is common to all \\(k\\) and a component that is specific or unique to each \\(k\\). This approach, which generalizes constrained MDS, is discussed in detail in chapter @ref(chindif).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#genasym",
    "href": "intro.html#genasym",
    "title": "1  Introduction",
    "section": "4.6 Asymmetry",
    "text": "4.6 Asymmetry\nWe have seen in section @ref(datasym) of this chapter that in basic MDS the assumption that \\(W\\) and \\(\\Delta\\) are symmetric and hollow can be made without loss of generality. The simple partitioning which proved this was based on the fact that \\(D(X)\\) is always symmetric and hollow. By the way, the assumption that \\(W\\) and \\(D\\) are non-negative cannot be made without loss of generality, as we will see below.\nIn chapter @ref(asymmds) we relax the assumption that \\(D(X)\\) is symmetric (still requiring it to be non-negative and hollow). This could be called Asymmetric MDS, or AMDS. I was reluctant at first to include this chapter, because asymmetric distances do not exist. And certainly are not Euclidean distances, so they are not covered by the title of this book. But as long as we stay close to Euclidean distances, least squares, and the smacof approach, I now feel reasonably confident the chapter is not too much of a foreign body.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#non-euclidean-distances",
    "href": "intro.html#non-euclidean-distances",
    "title": "1  Introduction",
    "section": "4.7 Non-Euclidean Distances",
    "text": "4.7 Non-Euclidean Distances\nWhen Kruskal introduced gradient-based methods to minimize stress he also discussed the possibility to use Minkovski metrics other than the Euclidean metric. This certainly was part of the appeal of the new methods, in fact it seemed as if the gradient methods made it possible to use any distance function whatsoever. This initial feeling of empowerment was somewhat naive, because it ignored the seriousness of the local minimum problem, the combinatorial nature of one-dimensional and city block scaling, the problems with nonmetric unfolding, and the problematic nature of gradient methods if the distances are not everywhere differentiable. All these complications will be discussed in this book. But it made me decide to ignore Minkovski distances (and hyperbolic and elliptic non-Euclidean distances), because life with stress is complicated and challenging enough as it is.\nGroenen, Mathar, and Heiser (1995), Mathar and Meyer (1994)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#alternating-least-squares-1",
    "href": "intro.html#alternating-least-squares-1",
    "title": "1  Introduction",
    "section": "5.1 Alternating Least Squares",
    "text": "5.1 Alternating Least Squares",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#majorization-1",
    "href": "intro.html#majorization-1",
    "title": "1  Introduction",
    "section": "5.2 Majorization",
    "text": "5.2 Majorization",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#introduction-to-majorization",
    "href": "intro.html#introduction-to-majorization",
    "title": "1  Introduction",
    "section": "5.3 Introduction to Majorization",
    "text": "5.3 Introduction to Majorization\nMajorization, these days better known as MM (Lange (2016)), is a general approach for the construction of minimization algorithms. There is also minorization, which leads to maximization algorithms, which explains the MM acronym: minorization for maximization and majorization for minimization.\nBefore the MM principle was formulated as a general approach to algorithm construction there were some important predecessors. Major classes of MM algorithms avant la lettre were the EM Algorithm for maximum likelihood estimation of Dempster, Laird, and Rubin (1977), the Smacof Algorithm for MDS of De Leeuw (1977), the Generalized Weiszfeldt Method* of Vosz and Eckhardt (1980), and the Quadratic Approximation Method of Böhning and Lindsay (1988). The first formulation of the general majorization principle seems to be De Leeuw (1994).\nLet’s start with a brief introduction to majorization. Minimize a real valued function \\(\\sigma\\) over \\(x\\in\\mathbb{S}\\), where \\(\\mathbb{S}\\) is some subset of \\(\\mathbb{R}^n\\). There are obvious extensions of majorization to functions defined on more general spaces, with values in any partially ordered set, but we do not need that level of generality in this manual. Also majorization applied to \\(\\sigma\\) is minorization applied to \\(-\\sigma\\), so concentrating on majorization-minimization and ignoring minorization-maximization causes no loss of generality\nSuppose there is a real-valued function \\(\\omega\\) on \\(\\mathbb{S}\\otimes\\mathbb{S}\\) such that \\[\\begin{align}\n\\sigma(x)&\\leq\\omega(x,y)\\qquad\\forall x,y\\in\\mathbb{S},\\label{eq-maj1}\\\\\n\\sigma(x)&=\\omega(x,x)\\qquad\\forall x\\in\\mathbb{S}.\\label{eq-maj2}\n\\end{align}\\] The function \\(\\omega\\) is called a majorization scheme for \\(\\sigma\\) on \\(S\\). A majorization scheme is strict if \\(\\sigma(x)&lt;\\omega(x,y)\\) for all \\(x,y\\in S\\) withj \\(x\\not=y\\).\nDefine \\[\nx^{(k+1)}\\in\\mathop{\\text{argmin}}_{x\\in\\mathbb{S}}\\omega(x,x^{(k)}),\n\\tag{5.1}\\] assuming that \\(\\omega\\) attains its (not necessarily unique) minimum over \\(x\\in\\mathbb{S}\\) for each \\(y\\). If \\(x^{(k)}\\in\\mathop{\\text{argmin}}_{x\\in\\mathbb{S}}\\omega(x,x^{(k)})\\) then we stop.\nBy majorization property \\(\\eqref{eq-maj1}\\) \\(\\sigma(x^{(k+1)})\\leq\\omega(x^{(k+1)},x^{(k)})\\). Because we did not stop update rule Equation 5.1 implies \\(\\omega(x^{(k+1)},x^{(k)})&lt;\\omega(x^{(k)},x^{(k)})\\). and finally by majorization property \\(\\eqref{eq-maj2}\\) \\(\\omega(x^{(k)},x^{(k)})=\\sigma(x^{(k)})\\).\nIf the minimum in Equation 5.1 is attained for a unique \\(x\\) then \\(\\omega(x^{(k+1)},x^{(k)})&lt;\\omega(x^{(k)},x^{(k)})\\). If the majorization scheme is strict then \\(\\sigma(x^{(k+1)})&lt;\\omega(x^{(k+1)},x^{(k)})\\). Under either of these two additional conditions \\(\\sigma(x^{(k+1)})&lt;\\sigma(x^{(k)})\\), which means that the majorization algorithm is a monotone descent algorithm, and if \\(\\sigma\\) is bounded below on \\(\\mathbb{S}\\) the sequence \\(\\sigma(x^{(k)})\\) converges.\nNote that we only use the order relation to prove convergence of the sequence of function values. To prove convergence of the \\(x^{(k)}\\) we need stronger compactness and continuity assumptions to apply the general theory of Zangwill (1969). For such a proof the argmin in update formula Equation 5.1 can be generalized to \\(x^{(k+1)}=\\phi(x^{(k)})\\), where \\(\\phi\\) maps \\(\\mathbb{S}\\) into \\(\\mathbb{S}\\) such that \\(\\omega(\\phi(x),x)\\leq\\sigma(x)\\) for all \\(x\\).\nWe give a small illustration in which we minimize \\(\\sigma\\) with \\(\\sigma(x)=\\sqrt{x}-\\log{x}\\) over \\(x&gt;0\\). Obviously we do not need majorization here, because solving \\(\\mathcal{D}\\sigma(x)=0\\) immediately gives \\(x=4\\) as the solution we are looking for.\nTo arrive at this solution using majorization we start with \\[\\begin{equation}\n\\sqrt{x}\\leq\\sqrt{y}+\\frac12\\frac{x-y}{\\sqrt{y}},\n(\\#eq:sqrtmaj)\n\\end{equation}\\] which is true because a differentiable concave function such as the square root is majorized by its tangent everywhere. Inequality @ref(eq:sqrtmaj) implies \\[\\begin{equation}\n\\sigma(x)\\leq\\eta(x,y):=\\sqrt{y}+\\frac12\\frac{x-y}{\\sqrt{y}}-\\log{x}.\n(\\#eq:examplemaj)\n\\end{equation}\\] Note that \\(\\eta(\\bullet,y)\\) is convex in its first argument for each \\(y\\). We have \\(\\mathcal{D}_1\\eta(x,y)=0\\) if and only if \\(x=2\\sqrt{y}\\) and thus the majorization algorithm is \\[\\begin{equation}\nx^{(k+1)}=2\\sqrt{x^{(k)}}\n(\\#eq:examplealg)\n\\end{equation}\\] The sequence \\(x^{(k)}\\) converges monotonically to the fixed point \\(x=2\\sqrt{x}\\), i.e. to \\(x=4\\). If \\(x^{(0)}&lt;4\\) the sequence is increasing, if \\(x^{(0)}&lt;4\\) it is decreasing. Also, by l’Hôpital, \\[\\begin{equation}\n\\lim_{x\\rightarrow 4}\\frac{2\\sqrt{x}-4}{x-4}=\\frac12\n(\\#eq:hopi1)\n\\end{equation}\\] and thus convergence to the minimizer is linear with asymptotic convergence rate \\(\\frac12\\). By another application of l’Hôpital \\[\\begin{equation}\n\\lim_{x\\rightarrow 4}\\frac{\\sigma(2\\sqrt{x)})-\\sigma(4)}{\\sigma(x)-\\sigma(4)}=\\frac14,\n(\\#eq:hopi2)\n\\end{equation}\\] and convergence to the minimum is linear with asymptotic convergence rate \\(\\frac14\\). Linear convergence to the minimizer is typical for majorization algorithms, as is the twice-as-fast linear convergence to the minimum value.\nThis small example is also of interest, because we minimize a DC function, the difference of two convex functions. In our example the convex functions are minus the square root and minus the logarithm. Algorithms for minimizing DC functions define other important subclasses of MM algorithms, the DC Algorithm of Tao Pham Dinh (see Le Thi and Tao (2018) for a recent overview), the Concave-Convex Procedure of Yuille and Rangarajan (2003), and the Half-Quadratic Method of Donald Geman (see Niikolova and Ng (2005) for a recent overview). For each of these methods there is a huge literature, with surprisingly little non-overlapping literatures. The first phase of the smacof algorithm, in which we improve the configuration for given disparities, is DC, concave-convex, and half-quadratic.\nIn the table below we show convergence of @ref(eq:examplealg) starting at \\(x=1.5\\). The first column show how far \\(x^{(k)}\\) deviates from the minimizer (i.e. from 4), the second shows how far\\(\\sigma(x^{(k)})\\) deviates from the minimum (i.e. from \\(2-\\log 4\\)). We clearly see the convergence rates \\(\\frac12\\) and \\(\\frac14\\) in action.\n\n\nitel   1 2.5000000000 0.2055741244 \nitel   2 1.5505102572 0.0554992066 \nitel   3 0.8698308399 0.0144357214 \nitel   4 0.4615431837 0.0036822877 \nitel   5 0.2378427379 0.0009299530 \nitel   6 0.1207437506 0.0002336744 \nitel   7 0.0608344795 0.0000585677 \nitel   8 0.0305337787 0.0000146606 \nitel   9 0.0152961358 0.0000036675 \nitel  10 0.0076553935 0.0000009172 \nitel  11 0.0038295299 0.0000002293 \nitel  12 0.0019152235 0.0000000573 \nitel  13 0.0009577264 0.0000000143 \nitel  14 0.0004788919 0.0000000036 \nitel  15 0.0002394531 0.0000000009 \n\n\nThe first three iterations are shown in the figure below. The vertical lines indicate the value of \\(x\\), function is in red, and the first three majorizations are in blue.\n\n\n\n\n\n\n\n\n\n\n\n\n\nBöhning, D., and B. G. Lindsay. 1988. “Monotonicity of Quadratic-approximation Algorithms.” Annals of the Institute of Statistical Mathematics 40 (4): 641–63.\n\n\nCarroll, J. D., and J. J. Chang. 1970. “Analysis of Individual Differences in Multidimensional scaling via an N-way generalization of \"Eckart-Young\" Decomposition.” Psychometrika 35: 283–319.\n\n\nCoombs, C. H. 1964. A Theory of Data. Wiley.\n\n\nDatorro, J. 2018. Convex Optimization and Euclidean Distance Geometry. Second Edition. Palo Alto, CA: Meebo Publishing. https://ccrma.stanford.edu/~dattorro/0976401304.pdf.\n\n\nDe Leeuw, J. 1977. “Applications of Convex Analysis to Multidimensional Scaling.” In Recent Developments in Statistics, edited by J. R. Barra, F. Brodeau, G. Romier, and B. Van Cutsem, 133–45. Amsterdam, The Netherlands: North Holland Publishing Company.\n\n\n———. 1994. “Block Relaxation Algorithms in Statistics.” In Information Systems and Data Analysis, edited by H. H. Bock, W. Lenski, and M. M. Richter, 308–24. Berlin: Springer Verlag. https://jansweb.netlify.app/publication/deleeuw-c-94-c/deleeuw-c-94-c.pdf.\n\n\n———. 2017a. “Shepard Non-metric Multidimensional Scaling.” 2017. https://jansweb.netlify.app/publication/deleeuw-e-17-e/deleeuw-e-17-e.pdf.\n\n\n———. 2017b. “Tweaking the SMACOF Engine.” 2017. https://jansweb.netlify.app/publication/deleeuw-e-17-p/deleeuw-e-17-p.pdf.\n\n\nDe Leeuw, J., and W. J. Heiser. 1977. “Convergence of Correction Matrix Algorithms for Multidimensional Scaling.” In Geometric Representations of Relational Data, edited by J. C. Lingoes, 735–53. Ann Arbor, Michigan: Mathesis Press.\n\n\n———. 1980. “Multidimensional Scaling with Restrictions on the Configuration.” In Multivariate Analysis, Volume V, edited by P. R. Krishnaiah, 501–22. Amsterdam, The Netherlands: North Holland Publishing Company.\n\n\n———. 1982. “Theory of Multidimensional Scaling.” In Handbook of Statistics, Volume II, edited by P. R. Krishnaiah and L. Kanal. Amsterdam, The Netherlands: North Holland Publishing Company.\n\n\nDe Leeuw, J., and P. Mair. 2009. “Multidimensional Scaling Using Majorization: SMACOF in R.” Journal of Statistical Software 31 (3): 1–30. https://www.jstatsoft.org/article/view/v031i03.\n\n\nDempster, A. P., N. M. Laird, and D. B. Rubin. 1977. “Maximum Likelihood for Incomplete Data via the EM Algorithm.” Journal of the Royal Statistical Society B39: 1–38.\n\n\nGroenen, P. J. F., R. Mathar, and W. J. Heiser. 1995. “The Majorization Approach to Multidimensional Scaling for Minkowski Distances.” Journal of Classification 12: 3–19.\n\n\nGroenen, P. J. F., and M. Van de Velden. 2016. “Multidimensional Scaling by Majorization: A Review.” Journal of Statistical Software 73 (8): 1–26. https://www.jstatsoft.org/index.php/jss/article/view/v073i08.\n\n\nGuttman, L. 1968. “A General Nonmetric Technique for Fitting the Smallest Coordinate Space for a Configuration of Points.” Psychometrika 33: 469–506.\n\n\nHarshman, R. A. 1970. “Foundations of the PARAFAC Procedure.” Working Papers in Phonetics 16. UCLA.\n\n\nHeiser, W. J., and J. De Leeuw. 1977. “How to Use SMACOF-I.” Department of Data Theory FSW/RUL.\n\n\nKruskal, J. B. 1964a. “Multidimensional Scaling by Optimizing Goodness of Fit to a Nonmetric Hypothesis.” Psychometrika 29: 1–27.\n\n\n———. 1964b. “Nonmetric Multidimensional Scaling: a Numerical Method.” Psychometrika 29: 115–29.\n\n\nLange, K. 2016. MM Optimization Algorithms. SIAM.\n\n\nLe Thi, H. A., and P. D. Tao. 2018. “DC Programming and DCA: Thirty Years of Developments.” Mathematical Programming, Series B.\n\n\nMathar, R., and R. Meyer. 1994. “Algorithms in Convex Analysis to Fit l_p -Distance Matrices.” Journal of Multivariate Analysis 51: 102–20.\n\n\nMeulman, J. J., and W. J. Heiser. 2012. IBM SPSS Categories 21. IBM Corporation.\n\n\nNiikolova, M., and M. Ng. 2005. “Analysis of Half-Quadratic Minimization Methods for Signal and Image Recovery.” SIAM Journal Scientific Computing 27 (3): 937–66.\n\n\nRamsay, J. O. 1977. “Maximum Likelihood Estimation in Multidimensional Scaling.” Psychometrika 42: 241–66.\n\n\nSchoenberg, I. J. 1935. “Remarks to Maurice Frechet’s article: Sur la Definition Axiomatique d’une Classe d’Espaces Vectoriels Distancies Applicables Vectoriellement sur l’Espace de Hllbert.” Annals of Mathematics 36: 724–32.\n\n\nShepard, R. N. 1962a. “The Analysis of Proximities: Multidimensional Scaling with an Unknown Distance Function. I.” Psychometrika 27: 125–40.\n\n\n———. 1962b. “The Analysis of Proximities: Multidimensional Scaling with an Unknown Distance Function. II.” Psychometrika 27: 219–46.\n\n\nTakane, Y., F. W. Young, and J. De Leeuw. 1977. “Nonmetric Individual Differences in Multidimensional Scaling: An Alternating Least Squares Method with Optimal Scaling Features.” Psychometrika 42: 7–67.\n\n\nTorgerson, W. S. 1958. Theory and Methods of Scaling. New York: Wiley.\n\n\nTorgerson, W. S. 1952. “Multidimensional Scaling: I. Theory and Method.” Psychometrika 17 (4): 401–19.\n\n\n———. 1965. “Multidimensional Scaling of Similarity.” Psychometrika 30 (4): 379–93.\n\n\nVosz, H., and U. Eckhardt. 1980. “Linear Convergence of Generalized Weiszfeld’s Method.” Computing 25: 243–51.\n\n\nYoung, G., and A. S. Householder. 1938. “Discussion of a Set of Points in Terms of Their Mutual Distances.” Psychometrika 3 (19-22).\n\n\nYuille, A. L., and A. Rangarajan. 2003. “The Concave-Convex Procedure.” Neural Computation 15: 915–36.\n\n\nZangwill, W. I. 1969. Nonlinear Programming: a Unified Approach. Englewood-Cliffs, N.J.: Prentice-Hall.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "basics.html",
    "href": "basics.html",
    "title": "2  smacof Basics",
    "section": "",
    "text": "2.1 Loss Function\nIn the pioneering papers Kruskal (1964a) and Kruskal (1964b) the MDS problem was formulated for the first time as minimization of an explicit loss function or badness-of-fit function, which measures the quality of the approximation of the dissimilarities by the distances. To be historically accurate, we should mention that the non-metric MDS technique proposed by Shepard (1962a) and Shepard (1962b) can be reformulated as minimization of an explicit loss function (see, for example, De Leeuw (2017)). And the classical Young-Householder-Torgerson MDS technique (Torgerson (1952)) for metric MDS can be reformulated as minimizing an explicit least squares loss function (De Leeuw and Heiser (1982)) as well. But neither of these two predecessors was formulated originally as an explicit minimization problem for a specific loss function",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>smacof Basics</span>"
    ]
  },
  {
    "objectID": "basics.html#metric-mds",
    "href": "basics.html#metric-mds",
    "title": "2  smacof Basics",
    "section": "2.2 Metric MDS",
    "text": "2.2 Metric MDS\nThe loss function in least squares metric Euclidean MDS is called raw stress and is defined as \\[\\begin{equation}\n\\sigma_R(X):=\\frac12\\mathop{\\sum\\sum}_{1\\leq j&lt;i\\leq n}w_{ij}(\\delta_{ij}-d_{ij}(X))^2.\n(\\#eq:stressdef)\n\\end{equation}\\] The subscript R in \\(\\sigma_R\\) stands for “raw”, because we will discuss other least squares loss functions for which we will also use the symbol \\(\\sigma\\), but with other subscripts.\nIn definition @ref(eq:stressdef) the \\(w_{ij}\\) are known non-negative weights, the \\(\\delta_{ij}\\) are the known non-negative dissimilarities between objects \\(o_i\\) and \\(o_j\\), and the \\(d_{ij}(X)\\) are the distances between the corresponding points \\(x_i\\) and \\(x_j\\). The summation is over all pairs \\((i,j)\\) with \\(w_{ij}&gt;0\\). From now on we use “metric MDS” to mean the minimization of \\(\\sigma_R\\).\nThe \\(n\\times p\\) matrix \\(X\\), which has the coordinates \\(x_i\\) of the \\(n\\) points as its rows, is called the configuration, where \\(p\\) is the dimension of the Euclidean space in which we make the map. The metric MDS problem (of dimension \\(p\\), for given \\(W\\) and \\(\\Delta\\)) is the minimization of @ref(eq:stressdef) over the \\(n\\times p\\) configurations \\(X\\).\nThe weights \\(w_{ij}\\) can be used to quantify information about the precision or importance of the corresponding dissimilarities. Some of the weights may be zero, which can be used to code missing data. If all weights are positive we have complete data. If we have complete data, and all weights are equal to one, we have unweighted metric MDS. The pioneering papers by Shepard, Kruskal, and Guttman only consider the unweighted case. Weights were only introduced in MDS in De Leeuw (1977).\nWe assume throughout that the weights are irreducible (De Leeuw (1977)). This means there is no partitioning of the index set \\(I_n:=\\{1,2,\\cdots,n\\}\\) into subsets for which all between-subset weights are zero. A reducible metric MDS problems decomposes into a number of smaller independent metric MDS problems, so the irreducibility assumption causes no real loss of generality.\nThe fact that the summation in @ref(eq:stressdef) is over all \\(j&lt;i\\) indicates that the diagonal elements of \\(\\Delta\\) are not used (they are assumed to be zero) and the elements above the diagonal are not used either (they are assumed to be equal to the corresponding elements below the diagonal). The somewhat mysterious factor \\(\\frac12\\) in definition @ref(eq:stressdef) is there because it simplifies some of the formulas in later sections of this paper.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>smacof Basics</span>"
    ]
  },
  {
    "objectID": "basics.html#non-linear-mds",
    "href": "basics.html#non-linear-mds",
    "title": "2  smacof Basics",
    "section": "2.3 Non-linear MDS",
    "text": "2.3 Non-linear MDS\nKruskal was not really interested in metric MDS and the “raw” loss function @ref(eq:stressdef). His papers are really about non-metric MDS, by which we mean least squares non-metric Euclidean MDS. Non-metric MDS differs from metric MDS because we have incomplete information about the dissimilarities. As we have seen, that if some dissimilarities are missing metric MDS can handle this by using zero weights. In some situations, however, we only know the rank order of the non-missing dissimilarities. We do not know, or we refuse to use, their actual numeric values. Or, to put it differently, even if we have numerical dissimilarities we are looking for a transformation of the non-missing dissimilarities, where the transformation is chosen from a set of admissible transformations (for instance from all linear or monotone transformations). If the dissimilarities are non-numerical, for example rank orders or partitionings, we choose from the set of admissible quantifications.\nIn non-metric MDS raw stress becomes \\[\\begin{equation}\n\\sigma_R(X,\\Delta):=\\frac12\\sum w_{ij}(\\delta_{ij}-d_{ij}(X))^2,\n(\\#eq:rawstressdef)\n\\end{equation}\\] where \\(\\Delta\\) varies over the quantified or transformed dissimilarities. In MDS parlance they are also called pseudo-distances or disparities. Loss function @ref(eq:rawstressdef) must be minimized over both configurations and disparities, with the condition that the disparities \\(\\Delta\\) are an admissible transformation or quantification of the data. In Kruskal’s non-metric MDS this means requiring monotonicity. In this paper we will consider various other choices for the set of admissible transformations. We will use the symbol \\(\\mathfrak{D}\\) for the set of admissible transformations\nThe most familiar examples of \\(\\mathfrak{D}\\) (linear, polynomial, splines, monotone) define convex cones with apex at the origin. This means that if \\(\\Delta\\in\\mathfrak{D}\\) then so is \\(\\lambda\\Delta\\) for all \\(\\lambda\\geq 0\\). But consequently minimizing @ref(eq:rawstressdef) over all \\(\\Delta\\in\\mathfrak{D}\\) and over all configurations has the trivial solution \\(\\Delta=0\\) and \\(X=0\\), corresponding with the global minimum \\(\\sigma(X,\\Delta)=0\\). We need additional constraints to rule out this trivial solution, and in non-metric MDS this is done by choosing a normalization that keeps the solution away from zero.\nKruskal’s original solution is to define normalized stress as \\[\\begin{equation}\n\\sigma(X,\\Delta):=\\frac{\\sum w_{ij}(\\delta_{ij}-d_{ij}(X))^2}{\\sum w_{ij}d_{ij}^2(X)}.\n(\\#eq:nstressdef)\n\\end{equation}\\] To be precise, in Kruskal’s formulation there are no weights, and he actually takes the square root of @ref(eq:nstressdef) to define Kruskal’s stress. The non-metric Euclidean MDS problem now is to minimize loss function @ref(eq:nstressdef) over all \\(n\\times p\\) configurations \\(X\\) and all admissible disparities \\(\\Delta\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>smacof Basics</span>"
    ]
  },
  {
    "objectID": "basics.html#non-metric-mds",
    "href": "basics.html#non-metric-mds",
    "title": "2  smacof Basics",
    "section": "2.4 Non-metric MDS",
    "text": "2.4 Non-metric MDS",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>smacof Basics</span>"
    ]
  },
  {
    "objectID": "basics.html#normalization",
    "href": "basics.html#normalization",
    "title": "2  smacof Basics",
    "section": "2.5 Normalization",
    "text": "2.5 Normalization\nEquation @ref(eq:nstressdef) is only one way to normalize raw stress. Some obvious alternatives are discussed in detail in Kruskal and Carroll (1969) and De Leeuw (1975). In the terminology of De Leeuw (1975) there are explicit and implicit normalizations.\nIn implicit normalization we minimize either \\[\\begin{equation}\n\\sigma(X,\\hat D):=\\frac{\\sum  w_{ij}(\\hat d_{ij} -d_{ij}(X))^2}{\\sum   w_{ij}^{\\ }\\hat d_{ij}^2}\n(\\#eq:implicit1)\n\\end{equation}\\] or \\[\\begin{equation}\n\\sigma(X,\\hat D):=\\frac{\\sum   w_{ij}(\\hat d_{ij}-d_{ij}(X))^2}{\\sum   w_{ij}^{\\ }d_{ij}^2(X) }\n(\\#eq:implicit2)\n\\end{equation}\\] over \\(X\\) and \\(\\Delta\\in\\mathfrak{D}\\).\nAs we have seen, Kruskal (1964a) chooses definition @ref(eq:implicit2) and calls the explicitly normalized loss function normalized stress. Note that we overload the symbol \\(\\sigma\\) to denote any one of the least squares loss functions. It will always be clear from the text which \\(\\sigma\\) we are talking about.\nIn explicit normalization we minimize the raw stress \\(\\sigma_R(X,\\hat D)\\) from @ref(eq:rawstressdef), but we add the explicit constraint \\[\\begin{equation}\n\\sum   w_{ij}^{\\ }d_{ij}^2(X)=1,\n(\\#eq:explicit1)\n\\end{equation}\\] or the constraint \\[\\begin{equation}\n\\sum   w_{ij}^{\\ }\\hat d_{ij}^2=1.\n(\\#eq:explicit2)\n\\end{equation}\\] Kruskal and Carroll (1969) and De Leeuw (2019) show that these four normalizations all lead to essentially the same solution for \\(X\\) and \\(\\hat D\\), up to scale factors dictated by the choice of the particular normalization. It is also possible to normalize both \\(X\\) and \\(\\hat D\\), either explicitly or implicitly, and again this will give the same solutions, suitably normalized. These invariance results assume the admissible transformations form a closed cone with apex at the origin, i.e. if \\(\\hat D\\) is admissible and \\(\\lambda\\geq 0\\) then \\(\\lambda\\hat D\\) is admissible as well. The matrices of Euclidean distances \\(D(X)\\) form a similar closed cone as well. The non-metric MDS problem is to find an element of the \\(\\hat D\\) cone \\(\\mathcal{D}\\) and an element of the \\(D(X)\\) cone where the angle between the two is a small as possible.\nIn the R version of smacof (De Leeuw and Mair (2009), Mair, Groenen, and De Leeuw (2022)) we use explicit normalization @ref(eq:explicit2). This is supported by the result, also due to De Leeuw (1975), that projection on the intersection of the cone of disparities and the sphere defined by @ref(eq:explicit2) is equivalent to first projecting on the cone and then normalizing the projection (see also Bauschke, Bui, and Wang (2018)).\nIn the version of non-metric MDS discussed in this manual we need more flexibility. For algorithmic reasons that may become clear later on, we will go with the original @ref(eq:nstressdef), i.e. with the implicitly normalized Kruskal’s stress. For the final results the choice between normalizations should not make a difference, but the iterative computations will be different for the different choices.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>smacof Basics</span>"
    ]
  },
  {
    "objectID": "basics.html#some-thoughts-on-als",
    "href": "basics.html#some-thoughts-on-als",
    "title": "2  smacof Basics",
    "section": "2.6 Some thoughts on ALS",
    "text": "2.6 Some thoughts on ALS\nThe formulation in equations @ref(eq:gmdsdef1) and @ref(eq:gmdsdef2) neatly separates the metric MDS part @ref(eq:gmdsdef1) and the transformation/quantification part @ref(eq:gmdsdef2). This second part is also often called the optimal scaling part.\nEquations @ref(eq:gmdsdef1) and @ref(eq:gmdsdef2) corresponds with the way most iterative non-linear and non-metric MDS techniques are implemented. The algorithms use Alternating Least Squares (ALS). There have been quite a few ALS algorithms avant-la-lettre, but as far as I know both the name and ALS as a general approach to algorithm construction were first introduced in De Leeuw (1968), and then widely disseminated in a series of papers by De Leeuw, Young, and Takane in the 1970’s (work summarized in Young, De Leeuw, and Takane (1980) and Young (1981)).\nIn the ALS implementation of MDS two sub-algorithms are used in each iteration: one to improve the fit of the distances to the current disparities \\(\\Delta\\) and one to improve the fit of the disparities to the current distances. The two sub-algorithms define one major iteration of the MDS technique. In formulas (using superscript \\((k)\\) for major iteration number) we start with \\((X^{(0)},\\Delta^{(0)})\\) and then alternate the mimization problems where \\(\\ni\\) is short for “such that”. In MDS it is more realistic not to minimize loss in the sub-steps but merely to decrease it. Minimization in one or both of the two subproblems may itself require an infinite iterative method, which we have to truncate anyway. Thus \n\n2.6.1 The Single-Phase approach\nIn Kruskal (1964a) defines \\[\\begin{equation}\n\\sigma(X):=\\min_{\\hat D\\in\\mathfrak{D}}\\ \\sigma(\\hat D,X)=\\sigma(X,\\hat D(X)),\n(\\#eq:project)\n\\end{equation}\\] where \\(\\sigma(\\hat D,X)\\) is defined by @ref(eq:implicit2). The minimum in @ref(eq:project) is over admissible transformations. In definition @ref(eq:project) \\[\\begin{equation}\n\\hat D(X):=\\mathop{\\text{argmin}}_{\\hat D\\in\\mathfrak{D}}\\sigma(X, \\hat D).\n(\\#eq:optscal)\n\\end{equation}\\] Normalized stress defined by @ref(eq:project) is now a function of \\(X\\) only. Under some conditions, which are true in Kruskal’s definition of non-metric MDS, there is a simple relation between the partials of @ref(eq:implicit2) and those of @ref(eq:project). \\[\\begin{equation}\n\\mathcal{D}\\sigma(X)=\\mathcal{D}_1\\sigma(X,\\hat D(X)),\n(\\#eq:partials)\n\\end{equation}\\] where \\(\\mathcal{D}\\sigma(X)\\) are the derivatives of \\(\\sigma\\) from @ref(eq:project) and \\(\\mathcal{D}_1\\sigma(X,\\hat D(X))\\) are the partial derivatives of \\(\\sigma\\) from @ref(eq:implicit2) with respect to \\(X\\). Thus the partials of \\(\\sigma\\) from @ref(eq:project) can be computed by evaluating the partials of \\(\\sigma\\) from @ref(eq:implicit2) with respect to \\(X\\) at \\((X,\\hat D(X))\\). This has created much confusion in the past. The non-metric MDS problem in Kruskal’s original formulation is now to minimize \\(\\sigma\\) from @ref(eq:project), which is a function of \\(X\\) alone.\nGuttman (1968) calls this the single-phase approach. A variation of Kruskal’s single-phase approach defines \\[\\begin{equation}\n\\sigma(X)=\\sum w_{ij}(d_{ij}^\\#(X)-d_{ij}(X))^2,\n(\\#eq:rankimage)\n\\end{equation}\\] where the \\(d_{ij}^\\#(X)\\) are Guttman’s rank images, i.e. the permutation of the \\(d_{ij}(X)\\) that makes them monotone with the \\(\\delta_{ij}\\) (Guttman (1968)). Or, alternatively, define \\[\\begin{equation}\n\\sigma(X):=\\sum   w_{ij}(d_{ij}^\\%(X)-d_{ij}(X))^2,\n(\\#eq:shepard)\n\\end{equation}\\] where the \\(\\hat d_{ij}^\\%(X)\\) are Shepard’s rank images, i.e. the permutation of the \\(\\delta_{ij}\\) that makes them monotone with the \\(d_{ij}(X)\\) (Shepard (1962a), Shepard (1962b), De Leeuw (2017)).\nMinimizing the Shepard or Guttman single-phase loss functions is computationally more complicated than Kruskal’s monotone regression approach, mostly because the rank-image transformations are not differentiable, and there is no analog of @ref(eq:partials) and of the equivalence of the different implicit and explicit normalizations.\n\n\n2.6.2 The Two-Phase Approach\nThe two-phase approach or alternating least squares (ALS) approach alternates minimization of \\(\\sigma(\\hat D,X)\\) over \\(X\\) for our current best estimate of \\(\\hat D\\) with minimization of \\(\\sigma(\\hat D,X)\\) over \\(\\Delta\\in\\mathfrak{D}\\) for our current best value of \\(X\\). Thus an update from iteration \\(k\\) to iteration \\(k+1\\) looks like This ALS approach to MDS was in the air since the early (unsuccessful) attempts around 1968 of Young and De Leeuw to combine Torgerson’s classic metric MDS method with Kruskal’s monotone regression transformation. All previous implementations of non-metric smacof use the two-phase approach, and we will do the same in this paper.\nAs formulated, however, there are some problems with the ALS algorithm. Step @ref(eq:step1) is easy to carry out, using monotone regression. Step @ref(eq:step2) means solving a metric scaling problem, which is an iterative proces that requires an infinite number of iterations. Thus, in the usual implementations, step @ref(eq:step1) is combined with one of more iterations of a convergent iterative procedure for metric MDS, such as smacof. If we take only one of these inner iterations the algorithm becomes indistinguishable from Kruskal’s single-phase method. This has also created much confusion in the past.\nIn the usual implementations of the ALS approach we solve the first subproblem @ref(eq:step1) exactly, while we take only a single step towards the solution for given \\(\\hat D\\) in the second phase @ref(eq:step2). If we have an infinite iterative procedure to compute the optimal \\(\\hat D\\in\\mathfrak{D}\\) for given \\(X\\), then a more balanced approach would be to take several inner iterations in the first phase and several inner iterations in the second phase. How many of each, nobody knows. In our current implementation of smacof we take several inner iteration steps in the first phase and a single inner iteration step in the second phase.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>smacof Basics</span>"
    ]
  },
  {
    "objectID": "basics.html#smacof-notation-and-terminology",
    "href": "basics.html#smacof-notation-and-terminology",
    "title": "2  smacof Basics",
    "section": "2.7 Smacof Notation and Terminology",
    "text": "2.7 Smacof Notation and Terminology\nWe discuss some the MDS notation used in smacof, which was first introduced in De Leeuw (1977) and De Leeuw and Heiser (1977). More detailed De Leeuw and Heiser (1980), De Leeuw (1988), Borg and Groenen (2005), Groenen and Van de Velden (2016)\nThis notation is useful for the second phase of the ALS algorithm, in which solve the metric MDS problem of we minimizing unnormalized \\(\\sigma(X,\\hat D)\\) over \\(X\\) for fixed \\(\\hat D\\). We will discuss the first ALS phase later in the paper.\nStart with the unit vectors \\(e_i\\) of length \\(n\\). They have a non-zero element equal to one in position \\(i\\), all other elements are zero. Think of the \\(e_i\\) as the columns of the identity matrix.\nUsing the \\(e_i\\) we define for all \\(i\\not= j\\) the matrices \\[\\begin{equation}\nA_{ij}:=(e_i-e_j)(e_i-e_j)'.\n\\end{equation}\\] The \\(A_{ij}\\) are of order \\(n\\), symmetric, doubly-centered, and of rank one. They have four non-zero elements. Elements \\((i,i)\\) and \\((j,j)\\) are equal to \\(+1\\), elements \\((i,j)\\) and \\((j,i)\\) are \\(-1\\).\nThe importance of \\(A_{ij}\\) in MDS comes from the equation \\[\\begin{equation}\nd_{ij}^2(X)=\\text{tr}\\ X'A_{ij}X.\n(\\#eq:dfroma)\n\\end{equation}\\] In addition we use the fact that the \\(A_{ij}\\) form a basis for the \\(binom{n}{2}\\)-dimensional linear space of all doubly-centered symmetric matrices.\nExpanding the square in the definition of stress gives \\[\\begin{equation}\n\\sigma(X)=\\frac12\\{\\sum   w_k\\delta_k^2-2\\ \\sum   w_k\\delta_kd_k(X)+\\sum   w_kd_k^2(X)\\}.\n(\\#eq:expand)\n\\end{equation}\\] It is convenient to have notation for the three separate components of stress from equation @ref(eq:expand). Define \\[\\begin{align}\n\\eta_{\\hat D}^2&=\\sum   w_{ij}\\hat d_{ij}^2,(\\#eq:condef)\\\\\n\\rho(X)&=\\sum   w_{ij}\\hat d_{ij}d_{ij}(X),(\\#eq:rhodef)\\\\\n\\eta^2(X)&=\\sum   w_{ij}d_{ij}(X)^2.(\\#eq:etadef)\n\\end{align}\\] which lead to \\[\\begin{equation}\n\\sigma(X)=\\frac12\\left\\{\\eta_{\\hat D}^2-2\\rho(X)+\\eta^2(X)\\right\\}.\n(\\#eq:stressshort)\n\\end{equation}\\] We also need \\[\\begin{equation}\n\\lambda(X)=\\frac{\\rho(X)}{\\eta(X)}.\n(\\#eq:lambdadef)\n\\end{equation}\\]\nUsing the \\(A_{ij}\\) makes it possible to give matrix expressions for \\(\\rho\\) and \\(\\eta^2\\). First \\[\\begin{equation}\n\\eta^2(X)=\\text{tr}\\ X'VX,\n(\\#eq:etamat)\n\\end{equation}\\] with \\[\\begin{equation}\nV:=\\sum   w_{ij}A_{ij}.\n(\\#eq:vdef)\n\\end{equation}\\] In the same way \\[\\begin{equation}\n\\rho(X)=\\text{tr}\\ X'B(X)X,\n(\\#eq:rhomat)\n\\end{equation}\\] with \\[\\begin{equation}\nB(X):=\\sum   w_{ij}r_{ij}(X)A_{ij},\n(\\#eq:bdef)\n\\end{equation}\\] with \\[\\begin{equation}\nr_{ij}(X):=\\begin{cases}\\frac{\\delta_{ij}}{d_{ij}(X)}&\\text{ if }d_{ij}(X)&gt;0,\\\\\n0&\\text{ if }d_{ij}(X)=0.\n\\end{cases}\n\\end{equation}\\] Note that \\(B\\) is a function from the set of \\(n\\times p\\) configurations into the set of symmetric doubly-dentered matrices of order \\(n\\). All matrices of the form \\(\\sum x_{ij}A_{ij}\\), where summation is over all pairs \\((i,j)\\) with \\(j&lt;i\\), are symmetric and doubly-centered. They have \\(-x_{ij}\\) as off-diagonal elements while the diagonal elements \\((i,i)\\) are \\(\\sum_{j=1}^nx_{ij}\\).\nBecause \\(B(X)\\) and \\(V\\) are non-negative linear combinations of the \\(A_{ij}\\) they are both positive semi-definite. Because \\(W\\) is assumed to be irreducible the matrix \\(V\\) has rank \\(n-1\\), with only vectors proportional to the vector \\(e\\) with all elements equal to one in its null-space (De Leeuw (1977)).\nSummarizing the results so far we have \\[\\begin{equation}\n\\sigma(X)=\\frac12\\{\\eta_{\\hat D}^2-\\text{tr}\\ X'B(X)X+\\text{tr}\\ X'VX\\}.\n(\\#eq:sigmat)\n\\end{equation}\\]\nNext we define the Guttman transform of a configuration \\(X\\), for given \\(W\\) and \\(\\Delta\\), as \\[\\begin{equation}\nG(X)=V^+B(X)X,\n(\\#eq:gudef)\n\\end{equation}\\] with \\(V^+\\) the Moore-Penrose inverse of \\(V\\). In our computations we use \\[\\begin{equation}\nV^+=(V+\\frac{1}{n}ee')^{-1}-\\frac{1}{n}ee'\n\\end{equation}\\] Also note that in the unweighted case with complete data \\(V=nJ\\), where \\(J\\) is the centering matrix \\(I-\\frac{1}{n}ee'\\), and thus \\(V^+=\\frac{1}{n}J\\). The Guttman transform is then simply \\(G(X)=n^{-1}B(X)X\\).\nWe have defined stress as a function on \\(\\mathbb{R}^{n\\times p}\\), the space of \\(n\\times p\\) matrices. For some purposes it is convenient to use an alternative, but equivalent, definition of stress on \\(\\mathbb{R}^{np}\\), the space of all vectors of length \\(np\\). Define \\(\\mathfrak{A}_{ij}\\) as the direct sum of \\(p\\) copies of \\(A_{ij}\\). Thus \\(\\mathfrak{A}_{ij}\\) is block-diagonal of order \\(np\\). Now redefine stress as \\[\n\\sigma(x):=\\frac12\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}(\\delta_{ij}-x'\\mathfrak{A}_{ij}x)^2.\n\\] \\[\n\\sigma(x)=1-x'\\mathfrak{B}(x)x+\\frac12 x'\\mathfrak{V}x\n\\] where \\(\\mathfrak{B}(x)\\) and \\(\\mathfrak{V}\\) are direct sums of \\(p\\) copies of our previous \\(B(X)\\) and \\(V\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>smacof Basics</span>"
    ]
  },
  {
    "objectID": "basics.html#stress-formula-two",
    "href": "basics.html#stress-formula-two",
    "title": "2  smacof Basics",
    "section": "4.1 Stress formula two",
    "text": "4.1 Stress formula two\nMinorization result",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>smacof Basics</span>"
    ]
  },
  {
    "objectID": "basics.html#interval",
    "href": "basics.html#interval",
    "title": "2  smacof Basics",
    "section": "5.1 Interval",
    "text": "5.1 Interval\n\\((i, j, \\text{lower bound}, \\text{upper bound}, \\text{weight})\\)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>smacof Basics</span>"
    ]
  },
  {
    "objectID": "basics.html#ordinal",
    "href": "basics.html#ordinal",
    "title": "2  smacof Basics",
    "section": "5.2 Ordinal",
    "text": "5.2 Ordinal\n\\((i, j, \\text{tied}, text{weight})\\)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>smacof Basics</span>"
    ]
  },
  {
    "objectID": "basics.html#paired-comparisons",
    "href": "basics.html#paired-comparisons",
    "title": "2  smacof Basics",
    "section": "5.3 Paired Comparisons",
    "text": "5.3 Paired Comparisons\n\\((i, j, k, l, \\text{tied}, \\text{weight})\\)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>smacof Basics</span>"
    ]
  },
  {
    "objectID": "basics.html#complete-triads",
    "href": "basics.html#complete-triads",
    "title": "2  smacof Basics",
    "section": "5.4 Complete triads",
    "text": "5.4 Complete triads\n\\[(i, j, k, \\text{smallest}, \\text{largest}, \\text{weight})\\] ## Indicator\n\\[(i, l, \\text{weight})\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>smacof Basics</span>"
    ]
  },
  {
    "objectID": "metric.html",
    "href": "metric.html",
    "title": "3  Metric smacof",
    "section": "",
    "text": "3.1 Program\nIn this part of the manual we discuss metric MDS, and the program smacofME. Metric MDS is the core of all smacof programs, because they all have the majorization algorithm based on the Guttman transform in common.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Metric smacof</span>"
    ]
  },
  {
    "objectID": "metric.html#program",
    "href": "metric.html#program",
    "title": "3  Metric smacof",
    "section": "",
    "text": "3.1.1 Parameters\n\nsmacofME &lt;- function(thedata,\n                     ndim = 2,\n                     xold = NULL,\n                     labels = NULL,\n                     width = 15,\n                     precision = 10,\n                     itmax = 1000,\n                     eps = 1e-10,\n                     verbose = TRUE,\n                     jitmax = 20,\n                     jeps = 1e-10,\n                     jverbose = FALSE,\n                     kitmax = 20,\n                     keps = 1e-10,\n                     kverbose = FALSE,\n                     init = 1)\n\nParameter ndim is the number of dimensions, and init tells if an initial configuration is read from a file (init = 1), is computed using classical scaling (init = 2), or is a random configuration (init = 3). Parameters itmax, epsi, and verbose control the iterations. The maximum number of iterations is itmax, the iterations stop if the decrease of stress in an iteration is less than 1E-epsi, and if verbose is one intermediate iteration results are written to stdout. These intermediate iteration results are formatted with the R function formatC(), using width for the width argument and precision for the digits argument.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Metric smacof</span>"
    ]
  },
  {
    "objectID": "metric.html#algorithm",
    "href": "metric.html#algorithm",
    "title": "3  Metric smacof",
    "section": "3.2 Algorithm",
    "text": "3.2 Algorithm",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Metric smacof</span>"
    ]
  },
  {
    "objectID": "metric.html#output",
    "href": "metric.html#output",
    "title": "3  Metric smacof",
    "section": "3.3 Output",
    "text": "3.3 Output\n\n  h &lt;- list(\n    delta = delta,\n    nobj = nobj,\n    ndim = ndim,\n    snew = snew,\n    itel = itel,\n    xnew = xnew,\n    dmat = dmat,\n    labels = labels\n  )",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Metric smacof</span>"
    ]
  },
  {
    "objectID": "metric.html#plots",
    "href": "metric.html#plots",
    "title": "3  Metric smacof",
    "section": "3.4 Plots",
    "text": "3.4 Plots",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Metric smacof</span>"
    ]
  },
  {
    "objectID": "metric.html#degruijter_67",
    "href": "metric.html#degruijter_67",
    "title": "3  Metric smacof",
    "section": "4.1 De Gruijter (1967)",
    "text": "4.1 De Gruijter (1967)\n\n\n\n\nDe Gruijter, D. N. M. 1967. “The Cognitive Structure of Dutch Political Parties in 1966.” Report E019-67. Psychological Institute, University of Leiden.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Metric smacof</span>"
    ]
  },
  {
    "objectID": "notation.html",
    "href": "notation.html",
    "title": "Notations",
    "section": "",
    "text": "Conventions\nI number and label all displayed equations. Equations are displayed, instead of inlined, if and only if one of the following is true.\nAll code chunks in the text are named. Theorems, lemmas, chapters, sections, subsections, and so on are also named and numbered. I use the serial comma.\nThe dilemma of whether to use “we” or “I” throughout the book is solved in the usual way. If I feel that a result is the work of a group (me, my co-workers, and the giants on whose shoulders we stand) then I use “we”. If it’s an individual decision, or something personal, then I use “I”. The default is “we”, as it always should be in scientific writing.\nMost of the individual chapters also have some of the necessary mathematical background material, both notation and results, sometimes with specific elaborations that seem useful for the book. Sometimes this background material is quite extensive. Examples are splines, majorization, unweighting, monotone regression, and the basic fixed point theorems we need for convergence analysis of our algorithms.",
    "crumbs": [
      "Notations"
    ]
  },
  {
    "objectID": "notation.html#conventions",
    "href": "notation.html#conventions",
    "title": "Notations",
    "section": "",
    "text": "They are important.\nThey are referred to elsewhere in the text.\nNot displaying them messes up the line spacing.",
    "crumbs": [
      "Notations"
    ]
  },
  {
    "objectID": "notation.html#notations-and-reserved-symbols",
    "href": "notation.html#notations-and-reserved-symbols",
    "title": "Notations",
    "section": "Notations and Reserved Symbols",
    "text": "Notations and Reserved Symbols\n\nSpaces\n\n\\(\\mathbb{R}^n\\) is the space of all real vectors, i.e. all \\(n\\)-element tuples of real numbers. Typical elements of \\(\\mathbb{R}^n\\) are \\(x,y,z\\). The element of \\(x\\) in position \\(i\\) is \\(x_i\\). Defining a vector by its elements is done with \\(x=\\{x_i\\}\\).\n\\(\\mathbb{R}^n\\) is equipped with the inner product \\(\\langle x,y\\rangle=x'y=\\sum_{i=1}^nx_iy_i\\) and the norm \\(\\smash{\\|x\\|=\\sqrt{x'x}}\\).\nThe canonical basis for \\(\\mathbb{R}^n\\) is the \\(n-\\)tuple \\((e_1,cdots,e_n)\\), where \\(e_i\\) has element \\(i\\) equal to \\(+1\\) and all other elements equal to zero. Thus \\(\\|e_i\\|=1\\) and \\(\\langle e_i,e_j\\rangle=\\delta^{ij}\\), with \\(\\delta^{ij}\\) the Kronecker delta (equal to one if \\(i=j\\) and zero otherwise). Note that \\(x_i=\\langle e_i,x\\rangle\\).\n\\(\\mathbb{R}\\) is the real line and \\(\\mathbb{R}_+\\) is the half line of non-negative numbers. The postive reals are \\(\\mathbb{R}_{++}\\).\n\\(\\mathbb{R}^{n\\times m}\\) is the space of all \\(n\\times m\\) real matrices. Typical elements of \\(\\mathbb{R}^{n\\times m}\\) are \\(A,B,C\\). The element of \\(A\\) in row \\(i\\) and column \\(j\\) is \\(a_{ij}\\). Defining a matrix by its elements is done with \\(A=\\{a_{ij}\\}\\).\n\\(\\mathbb{R}^{n\\times m}\\) is equipped with the inner product \\(\\langle A,B\\rangle=\\text{tr} A'B=\\sum_{i=1}^n\\sum_{j=1}^ma_{ij}b_{ij}\\) and the norm \\(\\|A\\|=\\sqrt{\\text{tr}\\ A'A}\\).\nThe canonical basis for \\(\\mathbb{R}^{n\\times m}\\) is the \\(nm-\\)tuple \\((E_{11},cdots,E_{nm})\\), where \\(E_{ij}\\) has element \\((i,j)\\) equal to \\(+1\\) and all other elements equal to zero. Thus \\(\\|E_{ij}\\|=1\\) and \\(\\langle E_{ij},E_{kl}\\rangle=\\delta^{ik}\\delta^{jl}\\).\n\n\\(\\text{vec}\\) and \\(\\text{vec}^{-1}\\)\n\n\nMatrices\n\n\\(a_{i\\bullet}\\) is row \\(i\\) of matrix \\(A\\), \\(a_{\\bullet j}\\) is column \\(j\\).\n\\(a_{i\\star}\\) is the sum of row \\(i\\) of matrix \\(A\\), \\(a_{\\star j}\\) is the sum of column \\(j\\).\n\\(A'\\) is the transpose of \\(A\\), and \\(\\text{diag}(A)\\) is the diagonal matrix with the diagonal elements of \\(A\\). The inverse of a square matrix \\(A\\) is \\(A^{-1}\\), the Moore-Penrose generalized inverse of any matrix \\(A\\) is \\(A^+\\). The transpose of the inverse, and the inverse of the transpose, are \\(A^{-T}\\).\nIf \\(A\\) and \\(B\\) are two \\(n\\times m\\) matrices then their Hadamard (or elementwise) product \\(C=A\\times B\\) has elements \\(c_{ij}=a_{ij}b_{ij}\\). The Hadamard quotient is \\(C=A/B\\), with elements \\(c_{ij}=a_{ij}/b_{ij}\\). The Hadamard power is \\(A^{(k)}=A^{(p-1)}\\times A\\).\nDC matrices. Centering matrix. \\(J_n=I_n-n^{-1}E_n\\). We do not use the subscripts if the order is obvious from the context.\nMatrices of matrices. Partitioned matrices. \\(A_{ij}\\) and thus \\(\\{A_{ij}\\}_{kl}\\).\nDirect sum and Product\n\n\n\nFunctions\n\n\\(f,g,h,\\cdots\\) are used for functions or mappings. \\(f:X\\rightarrow Y\\) says that \\(f\\) maps \\(X\\) into \\(Y\\).\n\\(\\sigma\\) is used for all real-valued least squares loss functions.\n\n\n\nMDS\n\n\\(\\Delta=\\{\\delta_{ij\\cdots}\\}\\) is a matrix or array of dissimilarities.\n\\(\\langle \\mathbb{X},d\\rangle\\) is a metric space, with \\(d:\\mathcal{X}\\otimes\\mathcal{X}\\rightarrow\\mathbb{R}_+\\) the distance function. If \\(X\\) is is an ordered n-tuple \\((x_1,\\cdots,x_n)\\) of elements of \\(\\mathcal{X}\\) then \\(D(X)\\) is \\(\\{d(x_i,x_j)\\}\\), the elements of which we also write as \\(d_{ij}(X)\\).\nSummation over the elements of vector \\(x\\in\\mathbb{R}^n\\) is \\(\\sum_{i=1}^n x_i\\). Summation over the elements of matrix \\(A\\in\\mathbb{R}^{n\\times m}\\) is \\(\\sum_{i=1}^n\\sum_{j=1}^m a_{ij}\\). Summation over the elements above the diagonal of \\(A\\) is \\(\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}a_{ij}\\).\nConditional summation is, for example, \\(\\sum_{i=1}^n \\{x_i\\mid x_i&gt;0\\}\\).",
    "crumbs": [
      "Notations"
    ]
  },
  {
    "objectID": "interval.html",
    "href": "interval.html",
    "title": "4  Interval smacof",
    "section": "",
    "text": "5 Program\nIn this part of the manual we discuss metric MDS, and the program smacofAC. Metric MDS is the core of all smacof programs, because they all have the majorization algorithm based on the Guttman transform in common.\nThere are two options, bounds and constant, to make smacofAC more widely applicable. Using these options the metric MDS problem becomes minimization of \\[\\begin{equation}\n\\sigma(X,\\hat D)=\\sum\\sum w_{ij}(\\hat d_{ij}-d_{ij}(X))^2\n(\\#eq:sdefac)\n\\end{equation}\\] over both \\(X\\) and \\(\\hat D\\), allowing some limited “metric” transformations of the data \\(\\Delta\\). Here \\(\\Delta^-\\) and \\(\\Delta^+\\) are known matrices with bounds, and \\(c\\) is an unknown additive constant. The four “metric” types of transformations relating disparities \\(\\hat d_{ij}\\) to dissimilarities \\(\\delta_{ij}\\) are\nAll four types of transformations also require that \\(\\hat d_{ij}\\geq 0\\) for all \\((i,j)\\). There are extensions of the smacof theory (Heiser (1991)) that do not require non-negativity of the disparities, but in the implementations in this manual we always force them to be non-negative. Note that AC3 is AC4 with \\(c=0\\) and AC2 is AC4 with \\(\\Delta^-=\\Delta^+=\\Delta\\).\nNote that for types AC2 and AC4 the data \\(\\Delta\\) do not need to be non-negative. In fact, the original motivation for the additive constant in classical scaling (Messick and Abelson (1956)) was that Thurstonian analysis of tetrad or triad comparisons produced dissimilarities on an interval scale, and thus could very well include negative values.\nIn AC3 and AC4 there is no mention of \\(\\Delta\\), which means the bounds \\(\\Delta^-\\) and \\(\\Delta^+\\) are actually the data. There are several possible uses of the bounds. We could collect dissimilarity data by asking subjects for interval judgments. Instead of a rating scale with possible responses from one to ten we could ask for a mark on a line between zero and ten, and then interpret the marks as a choice of one of the intervals \\([k, k+1]\\). These finite precision or interval type of data could even come from physical measurements of distances. Thus the bounds parameter provides one way to incorporate uncertainty into MDS, similar to interval analysis, fuzzy computing, or soft computing.\nThe non-negativity requirement for \\(\\hat D\\) implies bounds for the additive constant \\(c\\). In AC2 we need \\(c\\geq-\\min\\delta_{ij}\\) to maintain non-negativity. For AC4 we must have \\(c\\geq-\\min\\delta_{ij}^+\\), otherwise the constraints on the transformation are inconsistent. Clearly for consistency of AC3 and AC4 we require that \\(\\delta_{ij}^-\\leq\\delta_{ij}^+\\) for all \\((i,j)\\). It makes sense in most situations to choose \\(\\Delta^-\\) and \\(\\Delta^+\\) to be monotone with \\(\\Delta\\), but there is no requirement to do so.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Interval smacof</span>"
    ]
  },
  {
    "objectID": "interval.html#parameters",
    "href": "interval.html#parameters",
    "title": "4  Interval smacof",
    "section": "5.1 Parameters",
    "text": "5.1 Parameters\n\nsmacofAC &lt;- function(delta,\n                     ndim = 2,\n                     wmat = NULL,\n                     xold = NULL,\n                     bounds = FALSE,\n                     constant = FALSE,\n                     deltalw = NULL,\n                     deltaup = NULL,\n                     alpha = 2,\n                     labels = row.names(delta),\n                     width = 15,\n                     precision = 10,\n                     itmax = 1000,\n                     eps = 1e-10,\n                     verbose = TRUE,\n                     kitmax = 5,\n                     keps = 1e-10,\n                     kverbose = FALSE,\n                     init = 1)\n\nThe parameters constant, bounds, alpha, kitmax, kepsi, and kverbose are only relevant for AC2, AC3, and AC4. Nevertheless even for AC1 they should have integer values, it just doesn’t matter what these values are. Parameter ndim is the number of dimensions, and init tells if an initial configuration is read from a file (init = 1), is computed using classical scaling (init = 2), or is a random configuration (init = 3). Parameters itmax, epsi, and verbose control the iterations. The maximum number of iterations is itmax, the iterations stop if the decrease of stress in an iteration is less than 1E-epsi, and if verbose is one intermediate iteration results are written to stdout. These intermediate iteration results are formatted with the R function formatC(), using width for the width argument and precision for the digits argument.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Interval smacof</span>"
    ]
  },
  {
    "objectID": "interval.html#algorithm",
    "href": "interval.html#algorithm",
    "title": "4  Interval smacof",
    "section": "5.2 Algorithm",
    "text": "5.2 Algorithm\n\n5.2.1 Type AC1\nThis is standard non-metric smacof, no bells and whistles.\n\n\n5.2.2 Type AC2\nFor AC2 we also optimize over the additive constant \\(c\\), and thus the ALS algorithm has two sub-steps. The first sub-step consists of a number of Guttman iterations to update \\(X\\) for given \\(\\hat D\\) (i.e. for given \\(c\\)) and the second sub-step updates \\(c\\) for given \\(X\\). Parameters kitmax, kepsi, and kverbose control the inner iterations in the first sub-step in the same way as itmax, epsi, and verbose control the outer iterations that include both sub-steps. No inner iterations are used to update the additive constant, which only requires computing a weighted average. \\[\\begin{equation}\nc=-\\frac{\\sum\\sum w_{ij}(\\delta_{ij}-d_{ij}(X))}{\\sum\\sum w_{ij}}\n(\\#eq:updac2)\n\\end{equation}\\] AC2 should give the same results as the MDS method of Cooper (1972).\n\n\n5.2.3 Type AC3\nThe algorithm for AC3 has the same structure as that for AC2. Instead of a second sub-step computing the additive constant, the second sub-step computes \\(\\hat D\\) by squeezing the \\(D(X)\\) into the bounds. Thus \\[\\begin{equation}\n\\hat d_{ij}=\\begin{cases}\n\\delta_{ij}^-&\\text{ if }d_{ij}(X)&lt;\\delta_{ij}^-,\\\\\n\\delta_{ij}^+&\\text{ if }d_{ij}(X)&gt;\\delta_{ij}^+,\\\\\nd_{ij}(X)&\\text{ otherwise }.\n\\end{cases}\n(\\#eq:updac3)\n\\end{equation}\\] Obviously no iterations are required in the second sub-step.\n\n\n5.2.4 Type AC4\nOf the four regression problems in the second ALS sub-step only the one for AC4 with both bounds and additive constant is non-trivial. We’ll give it some extra attention.\nIt may help to give an example of what it actually requires. We use the De Gruijter example with nine Dutch political parties from 1967 (De Gruijter (1967)). For ease of reference we include the data here. Dissimilarities are averages over a group of 100 students from an introductory psychology course.\n\n\n      KVP PvdA  VVD  ARP  CHU  CPN  PSP   BP  D66\nKVP  0.00 5.63 5.27 4.60 4.80 7.54 6.73 7.18 6.17\nPvdA 5.63 0.00 6.72 5.64 6.22 5.12 4.59 7.22 5.47\nVVD  5.27 6.72 0.00 5.46 4.97 8.13 7.55 6.90 4.67\nARP  4.60 5.64 5.46 0.00 3.20 7.84 6.73 7.28 6.13\nCHU  4.80 6.22 4.97 3.20 0.00 7.80 7.08 6.96 6.04\nCPN  7.54 5.12 8.13 7.84 7.80 0.00 4.08 6.34 7.42\nPSP  6.73 4.59 7.55 6.73 7.08 4.08 0.00 6.88 6.36\nBP   7.18 7.22 6.90 7.28 6.96 6.34 6.88 0.00 7.36\nD66  6.17 5.47 4.67 6.13 6.04 7.42 6.36 7.36 0.00\n\n\nWe compute distances from the Torgerson solution. The Shepard plot for \\(c=0\\) and the Torgerson distances is in figure @ref(fig:bandplot). The two blue lines are connecting the \\(\\delta_{ij}^-\\) and the \\(\\delta_{ij}^+\\), i.e. they give the bounds for \\(c=0\\). In our example the lines are parallel, because \\(\\delta_{ij}^+-\\delta_{ij}^-=2\\) for all \\((i,j)\\), but in general this may not be the case. The points between the two lines do not contribute to the loss, and the points outside the band contribute by how much they are outside, as indicated by the black vertical fitlines.\nBy varying \\(c\\) we shift the region between the two parallel lines upwards or downwards. The width of the region, or more generally the shape, always remains the same, because it is determined by the difference of \\(\\delta^+\\) and \\(\\delta^-\\) and does not depend on \\(c\\). The optimal \\(c\\) is that shift for which the red \\((\\delta_{ij},d_{ij}(X))\\) points are as much as possible within the strip between the \\(\\delta^-\\) and \\(\\delta^+\\) lines. This is in the least squares sense, which means that we minimize the horizontal squared distances from the points outside the strip to the \\(\\delta^-\\) and \\(\\delta^+\\) lines (i.e. the black vertical lines).\nLet’s formalize this. Define \\[\\begin{equation}\n\\phi_{ij}(c):=\\min_{\\delta_{ij}\\geq 0}\\{(\\delta_{ij}-d_{ij}(X))^2\\mid \\delta^-_{ij}+c\\leq\\delta_{ij}\\leq\\delta^+_{ij}+c\\}\n(\\#eq:phiijdef)\n\\end{equation}\\] and \\[\\begin{equation}\n\\phi(c):=\\sum\\sum w_{ij}\\phi_{ij}(c)\n(\\#eq:phidef)\n\\end{equation}\\] The constraints are consistent if \\(\\delta_{ij}^++c\\geq 0\\), i.e. if \\(c\\geq c_0:=-\\min\\delta_{ij}^+\\). The regression problem is to minimize \\(\\phi\\) over \\(c\\geq c_0:=-\\min\\delta_{ij}^+\\).\nFigure has an example of one of the \\(\\phi_{ij}\\). The value of the \\(d_{ij}(X)\\) we used is , \\(\\delta_{ij}\\) is , \\(\\delta_{ij}^-\\) is , and \\(\\delta_{ij}^+\\) is . The two red vertical lines are at \\(c=d_{ij}(X)-\\delta_{ij}^+\\) and \\(c=d_{ij}(X)-\\delta_{ij}^-\\).\nNow \\[\\begin{equation}\n\\hat d_{ij}=\\begin{cases}\n\\delta_{ij}^-+c&\\text{ if }c\\geq d_{ij}(X)-\\delta_{ij}^-,\\\\\n\\delta_{ij}^++c&\\text{ if }c\\leq d_{ij}(X)-\\delta_{ij}^+,\\\\\nd_{ij}(X)&\\text{ otherwise}.\n\\end{cases}\n(\\#eq:solves)\n\\end{equation}\\] and thus \\[\\begin{equation}\n\\phi_{ij}(c)=\\begin{cases}\n(d_{ij}(X)-(\\delta_{ij}^-+c))^2&\\text{ if }c\\geq d_{ij}(X)-\\delta_{ij}^-,\\\\\n(d_{ij}(X)-(\\delta_{ij}^++c))^2&\\text{ if }c\\leq d_{ij}(X)-\\delta_{ij}^+,\\\\\n0&\\text{ otherwise}.\n\\end{cases}\n(\\#eq:funcs)\n\\end{equation}\\] It follows that \\(\\phi_{ij}\\) is piecewise quadratic, convex, and continuously differentiable. The derivative is piecewise linear, continuous, and increasing. In fact \\[\\begin{equation}\n\\mathcal{D}\\phi_{ij}(c)=\\begin{cases}\n2(c-(d_{ij}(X)-\\delta_{ij}^-))&\\text{ if }c\\geq d_{ij}(X)-\\delta_{ij}^-,\\\\\n2(c-(d_{ij}(X)-\\delta_{ij}^+))&\\text{ if }c\\leq d_{ij}(X)-\\delta_{ij}^+,\\\\\n0&\\text{ otherwise}.\n\\end{cases}\n(\\#eq:derivs)\n\\end{equation}\\]\nSince \\(\\phi\\) is a positive linear combination of the \\(\\phi_{ij}\\) it is also piecewise quadratic, convex, and continuously differentiable, with a continuous piecewise linear increasing derivative. Note \\(\\phi\\) is not twice-differentiable and not strictly convex. Figure @ref(fig:morefunc) has a plot of \\(\\phi\\) for the De Gruijter example. The red vertical lines are at \\(c=c_0\\) and at \\(c_1:=\\max\\{d_{ij}(X)-\\delta_{ij}^-\\}\\). From @ref(eq:derivs) we see that if \\(c\\geq c_1\\) then \\(\\mathcal{D}\\phi(c_1)\\geq 0\\) and thus we can look for the optimum \\(c\\) in the interval \\([c_0,c_1]\\).\nWe minimize \\(\\phi\\) by using the R function optimize().",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Interval smacof</span>"
    ]
  },
  {
    "objectID": "interval.html#output",
    "href": "interval.html#output",
    "title": "4  Interval smacof",
    "section": "5.3 Output",
    "text": "5.3 Output",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Interval smacof</span>"
    ]
  },
  {
    "objectID": "interval.html#plots",
    "href": "interval.html#plots",
    "title": "4  Interval smacof",
    "section": "5.4 Plots",
    "text": "5.4 Plots",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Interval smacof</span>"
    ]
  },
  {
    "objectID": "interval.html#degruijter_67",
    "href": "interval.html#degruijter_67",
    "title": "4  Interval smacof",
    "section": "6.1 De Gruijter (1967)",
    "text": "6.1 De Gruijter (1967)\nIt may help to give an example of what it actually requires. We use the De Gruijter example with nine Dutch political parties from 1967 (De Gruijter (1967)). Dissimilarities are averages over a group of 100 students from an introductory psychology course.\nWe compute optimal solutions for all four types AC1-AC4 (two dimensions, Torgerson initial estimate, no weights). We iterate until the difference in consecutive stress values is less than 1e-10. For each of the four runs we give the number of iterations, the final stress, and the additive constant in case of AC2 and AC4. We also make three plots: the Shepard plot with points \\((\\delta_{ij},d_{ij}(X))\\) in blue and with points \\((\\delta_{ij},\\hat d_{ij})\\) in red, the configuration plot with a labeled \\(X\\), and the dist-dhat plot with points \\((d_{ij}(X),\\hat d_{ij})\\) scattered around the line \\(d=\\hat d\\). Line segments are drawn in the plots to show the fit of all pairs \\((i,j)\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Interval smacof</span>"
    ]
  },
  {
    "objectID": "interval.html#type-ac1-1",
    "href": "interval.html#type-ac1-1",
    "title": "4  Interval smacof",
    "section": "6.2 Type AC1",
    "text": "6.2 Type AC1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor AC1 we find a minimum stress of 32.2208145 after 112 iterations. The Shepard plot has a substantial intercept, which suggest that an additive constant may improve the fit. This is typical for average similarity judgments over heterogeneous groups of individuals. It is the reason why Ekman (1954) linearly transformed his average similarities so that the smallest became zero and the largest became one. That amounts to applying the additive constant before the MDS analysis.\nTo give some content to the configuration plot: CPN (communists), PSP (pacifists), and PvdA (social democrats) are leftists parties, ARP (protestants), CHU (other protestants), KVP (catholics) are religious parties, BP (farmers) is a right-wing protest party, VVD (classical liberals) is a conservative party, and D’66 (pragmatists, centrists) was brand new in 1967 and was supposedly beyond left and right.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Interval smacof</span>"
    ]
  },
  {
    "objectID": "interval.html#type-ac2-1",
    "href": "interval.html#type-ac2-1",
    "title": "4  Interval smacof",
    "section": "6.3 Type AC2",
    "text": "6.3 Type AC2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs expected, the additive constant improves the fit. We have convergence after 25 iterations to stress 3.6661492. The additive constant is -3.2, which means the smallest \\(\\delta_{ij}+c\\), between ARP and CHU, is now zero. The configuration shows the same three groups, but they cluster a bit more tightly. This is to be expected. Without the additive constant the dissimilarities are more equal and consequently the distances are more equal to. The configuration tends more to what we see if all dissimilarities are equal, i.e. to points regularly spaced on a circle (De Leeuw and Stoop (1984)).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Interval smacof</span>"
    ]
  },
  {
    "objectID": "interval.html#type-ac3-1",
    "href": "interval.html#type-ac3-1",
    "title": "4  Interval smacof",
    "section": "6.4 Type AC3",
    "text": "6.4 Type AC3\n\n\nitel  1 sold    97.4130852810 smid    41.9550246691 snew     8.4543139218 \nitel  2 sold     8.4543139218 smid     7.4098292411 snew     6.6451369923 \nitel  3 sold     6.6451369923 smid     5.8490240542 snew     5.2489286201 \nitel  4 sold     5.2489286201 smid     4.6072125696 snew     4.1502164078 \nitel  5 sold     4.1502164078 smid     3.6168200275 snew     3.2322363271 \nitel  6 sold     3.2322363271 smid     2.7379930587 snew     2.4414269275 \nitel  7 sold     2.4414269275 smid     2.1593556178 snew     1.9752760141 \nitel  8 sold     1.9752760141 smid     1.8092882729 snew     1.6764432789 \nitel  9 sold     1.6764432789 smid     1.5582549338 snew     1.4557443279 \nitel  10 sold     1.4557443279 smid     1.3629036613 snew     1.2793695305 \nitel  11 sold     1.2793695305 smid     1.2025292754 snew     1.1324574520 \nitel  12 sold     1.1324574520 smid     1.0674066563 snew     1.0078467610 \nitel  13 sold     1.0078467610 smid     0.9522276636 snew     0.9013064460 \nitel  14 sold     0.9013064460 smid     0.8535746120 snew     0.8098200136 \nitel  15 sold     0.8098200136 smid     0.7685362919 snew     0.7305939018 \nitel  16 sold     0.7305939018 smid     0.6945836967 snew     0.6615054424 \nitel  17 sold     0.6615054424 smid     0.6300498204 snew     0.6011559114 \nitel  18 sold     0.6011559114 smid     0.5735956560 snew     0.5483309878 \nitel  19 sold     0.5483309878 smid     0.5241981027 snew     0.5020869304 \nitel  20 sold     0.5020869304 smid     0.4809112562 snew     0.4614299495 \nitel  21 sold     0.4614299495 smid     0.4425815492 snew     0.4251667367 \nitel  22 sold     0.4251667367 smid     0.4082440772 snew     0.3925990995 \nitel  23 sold     0.3925990995 smid     0.3773608282 snew     0.3632632528 \nitel  24 sold     0.3632632528 smid     0.3494646668 snew     0.3366780605 \nitel  25 sold     0.3366780605 smid     0.3241244799 snew     0.3124866156 \nitel  26 sold     0.3124866156 smid     0.3010402505 snew     0.2904279430 \nitel  27 sold     0.2904279430 smid     0.2799638410 snew     0.2702326078 \nitel  28 sold     0.2702326078 smid     0.2606115288 snew     0.2516575222 \nitel  29 sold     0.2516575222 smid     0.2427970344 snew     0.2345497021 \nitel  30 sold     0.2345497021 smid     0.2263873344 snew     0.2187862641 \nitel  31 sold     0.2187862641 smid     0.2112642137 snew     0.2042558938 \nitel  32 sold     0.2042558938 smid     0.1973219792 snew     0.1908667641 \nitel  33 sold     0.1908667641 smid     0.1844985022 snew     0.1785576365 \nitel  34 sold     0.1785576365 smid     0.1726814275 snew     0.1672223800 \nitel  35 sold     0.1672223800 smid     0.1618749397 snew     0.1568871640 \nitel  36 sold     0.1568871640 smid     0.1520054971 snew     0.1474133814 \nitel  37 sold     0.1474133814 smid     0.1429220273 snew     0.1386839988 \nitel  38 sold     0.1386839988 smid     0.1345340757 snew     0.1306172696 \nitel  39 sold     0.1306172696 smid     0.1267823207 snew     0.1231570280 \nitel  40 sold     0.1231570280 smid     0.1195991686 snew     0.1162371624 \nitel  41 sold     0.1162371624 smid     0.1129422950 snew     0.1098197491 \nitel  42 sold     0.1098197491 smid     0.1067564820 snew     0.1038529288 \nitel  43 sold     0.1038529288 smid     0.1009986094 snew     0.0982958229 \nitel  44 sold     0.0982958229 smid     0.0956394236 snew     0.0931205429 \nitel  45 sold     0.0931205429 smid     0.0906400571 snew     0.0882903384 \nitel  46 sold     0.0882903384 smid     0.0859714475 snew     0.0837774185 \nitel  47 sold     0.0837774185 smid     0.0816103623 snew     0.0795595988 \nitel  48 sold     0.0795595988 smid     0.0775350522 snew     0.0756158480 \nitel  49 sold     0.0756158480 smid     0.0737069060 snew     0.0718997892 \nitel  50 sold     0.0718997892 smid     0.0700922565 snew     0.0683803581 \nitel  51 sold     0.0683803581 smid     0.0666579412 snew     0.0650295657 \nitel  52 sold     0.0650295657 smid     0.0633889911 snew     0.0618371988 \nitel  53 sold     0.0618371988 smid     0.0602695635 snew     0.0587893711 \nitel  54 sold     0.0587893711 smid     0.0572852862 snew     0.0558726314 \nitel  55 sold     0.0558726314 smid     0.0544335762 snew     0.0530845479 \nitel  56 sold     0.0530845479 smid     0.0517076720 snew     0.0504187686 \nitel  57 sold     0.0504187686 smid     0.0490986716 snew     0.0478667578 \nitel  58 sold     0.0478667578 smid     0.0466025175 snew     0.0454246636 \nitel  59 sold     0.0454246636 smid     0.0442126511 snew     0.0430859071 \nitel  60 sold     0.0430859071 smid     0.0419238186 snew     0.0408458015 \nitel  61 sold     0.0408458015 smid     0.0397344293 snew     0.0387034342 \nitel  62 sold     0.0387034342 smid     0.0376390715 snew     0.0366529538 \nitel  63 sold     0.0366529538 smid     0.0356351002 snew     0.0346922685 \nitel  64 sold     0.0346922685 smid     0.0337172756 snew     0.0328162870 \nitel  65 sold     0.0328162870 smid     0.0318859969 snew     0.0310252529 \nitel  66 sold     0.0310252529 smid     0.0301365454 snew     0.0293146845 \nitel  67 sold     0.0293146845 smid     0.0284661882 snew     0.0276818686 \nitel  68 sold     0.0276818686 smid     0.0268736638 snew     0.0261255295 \nitel  69 sold     0.0261255295 smid     0.0253515999 snew     0.0246386243 \nitel  70 sold     0.0246386243 smid     0.0239018541 snew     0.0232227235 \nitel  71 sold     0.0232227235 smid     0.0225233461 snew     0.0218768243 \nitel  72 sold     0.0218768243 smid     0.0212105688 snew     0.0205956586 \nitel  73 sold     0.0205956586 smid     0.0199609330 snew     0.0193766033 \nitel  74 sold     0.0193766033 smid     0.0187743001 snew     0.0182194345 \nitel  75 sold     0.0182194345 smid     0.0176476992 snew     0.0171212966 \nitel  76 sold     0.0171212966 smid     0.0165791103 snew     0.0160801664 \nitel  77 sold     0.0160801664 smid     0.0155665396 snew     0.0150940641 \nitel  78 sold     0.0150940641 smid     0.0146080603 snew     0.0141610730 \nitel  79 sold     0.0141610730 smid     0.0137017748 snew     0.0132793044 \nitel  80 sold     0.0132793044 smid     0.0128460418 snew     0.0124471117 \nitel  81 sold     0.0124471117 smid     0.0120383862 snew     0.0116620659 \nitel  82 sold     0.0116620659 smid     0.0112774626 snew     0.0109227851 \nitel  83 sold     0.0109227851 smid     0.0105587161 snew     0.0102248820 \nitel  84 sold     0.0102248820 smid     0.0098838778 snew     0.0095698425 \nitel  85 sold     0.0095698425 smid     0.0092487574 snew     0.0089536963 \nitel  86 sold     0.0089536963 smid     0.0086521341 snew     0.0083751510 \nitel  87 sold     0.0083751510 smid     0.0080919781 snew     0.0078322169 \nitel  88 sold     0.0078322169 smid     0.0075666147 snew     0.0073232264 \nitel  89 sold     0.0073232264 smid     0.0070742856 snew     0.0068464430 \nitel  90 sold     0.0068464430 smid     0.0066132474 snew     0.0064001456 \nitel  91 sold     0.0064001456 smid     0.0061817952 snew     0.0059826528 \nitel  92 sold     0.0059826528 smid     0.0057802663 snew     0.0055942116 \nitel  93 sold     0.0055942116 smid     0.0054047931 snew     0.0052311501 \nitel  94 sold     0.0052311501 smid     0.0050539660 snew     0.0048920336 \nitel  95 sold     0.0048920336 smid     0.0047266122 snew     0.0045756432 \nitel  96 sold     0.0045756432 smid     0.0044211081 snew     0.0042792136 \nitel  97 sold     0.0042792136 smid     0.0041326493 snew     0.0039985613 \nitel  98 sold     0.0039985613 smid     0.0038606482 snew     0.0037337299 \nitel  99 sold     0.0037337299 smid     0.0036026143 snew     0.0034825438 \nitel  100 sold     0.0034825438 smid     0.0033580833 snew     0.0032445924 \nitel  101 sold     0.0032445924 smid     0.0031268398 snew     0.0030196948 \nitel  102 sold     0.0030196948 smid     0.0029084545 snew     0.0028074476 \nitel  103 sold     0.0028074476 smid     0.0027025336 snew     0.0026074631 \nitel  104 sold     0.0026074631 smid     0.0025086875 snew     0.0024193512 \nitel  105 sold     0.0024193512 smid     0.0023262643 snew     0.0022424654 \nitel  106 sold     0.0022424654 smid     0.0021553797 snew     0.0020768877 \nitel  107 sold     0.0020768877 smid     0.0019953227 snew     0.0019219223 \nitel  108 sold     0.0019219223 smid     0.0018458400 snew     0.0017772963 \nitel  109 sold     0.0017772963 smid     0.0017057245 snew     0.0016446713 \nitel  110 sold     0.0016446713 smid     0.0015838944 snew     0.0015303032 \nitel  111 sold     0.0015303032 smid     0.0014763882 snew     0.0014277542 \nitel  112 sold     0.0014277542 smid     0.0013787501 snew     0.0013340577 \nitel  113 sold     0.0013340577 smid     0.0012890894 snew     0.0012477951 \nitel  114 sold     0.0012477951 smid     0.0012064204 snew     0.0011681595 \nitel  115 sold     0.0011681595 smid     0.0011299951 snew     0.0010944827 \nitel  116 sold     0.0010944827 smid     0.0010585786 snew     0.0010256055 \nitel  117 sold     0.0010256055 smid     0.0009925089 snew     0.0009618597 \nitel  118 sold     0.0009618597 smid     0.0009313870 snew     0.0009028660 \nitel  119 sold     0.0009028660 smid     0.0008742102 snew     0.0008476717 \nitel  120 sold     0.0008476717 smid     0.0008210857 snew     0.0007963649 \nitel  121 sold     0.0007963649 smid     0.0007716538 snew     0.0007485214 \nitel  122 sold     0.0007485214 smid     0.0007255726 snew     0.0007038565 \nitel  123 sold     0.0007038565 smid     0.0006822460 snew     0.0006618474 \nitel  124 sold     0.0006618474 smid     0.0006414818 snew     0.0006223115 \nitel  125 sold     0.0006223115 smid     0.0006031260 snew     0.0005851042 \nitel  126 sold     0.0005851042 smid     0.0005670511 snew     0.0005501055 \nitel  127 sold     0.0005501055 smid     0.0005331016 snew     0.0005171660 \nitel  128 sold     0.0005171660 smid     0.0005012225 snew     0.0004862336 \nitel  129 sold     0.0004862336 smid     0.0004712550 snew     0.0004571555 \nitel  130 sold     0.0004571555 smid     0.0004430868 snew     0.0004298229 \nitel  131 sold     0.0004298229 smid     0.0004166114 snew     0.0004041327 \nitel  132 sold     0.0004041327 smid     0.0003917278 snew     0.0003799870 \nitel  133 sold     0.0003799870 smid     0.0003683391 snew     0.0003572921 \nitel  134 sold     0.0003572921 smid     0.0003463530 snew     0.0003359581 \nitel  135 sold     0.0003359581 smid     0.0003256809 snew     0.0003158995 \nitel  136 sold     0.0003158995 smid     0.0003062400 snew     0.0002970358 \nitel  137 sold     0.0002970358 smid     0.0002879569 snew     0.0002792959 \nitel  138 sold     0.0002792959 smid     0.0002707632 snew     0.0002626133 \nitel  139 sold     0.0002626133 smid     0.0002545945 snew     0.0002469255 \nitel  140 sold     0.0002469255 smid     0.0002393903 snew     0.0002321738 \nitel  141 sold     0.0002321738 smid     0.0002250933 snew     0.0002183027 \nitel  142 sold     0.0002183027 smid     0.0002116497 snew     0.0002052599 \nitel  143 sold     0.0002052599 smid     0.0001990090 snew     0.0001929963 \nitel  144 sold     0.0001929963 smid     0.0001871232 snew     0.0001814652 \nitel  145 sold     0.0001814652 smid     0.0001756810 snew     0.0001703685 \nitel  146 sold     0.0001703685 smid     0.0001649346 snew     0.0001599422 \nitel  147 sold     0.0001599422 smid     0.0001548371 snew     0.0001501463 \nitel  148 sold     0.0001501463 smid     0.0001453494 snew     0.0001409426 \nitel  149 sold     0.0001409426 smid     0.0001364348 snew     0.0001322952 \nitel  150 sold     0.0001322952 smid     0.0001280586 snew     0.0001241703 \nitel  151 sold     0.0001241703 smid     0.0001201882 snew     0.0001165364 \nitel  152 sold     0.0001165364 smid     0.0001127934 snew     0.0001093639 \nitel  153 sold     0.0001093639 smid     0.0001058460 snew     0.0001026255 \nitel  154 sold     0.0001026255 smid     0.0000993192 snew     0.0000962953 \nitel  155 sold     0.0000962953 smid     0.0000931880 snew     0.0000903490 \nitel  156 sold     0.0000903490 smid     0.0000874289 snew     0.0000847637 \nitel  157 sold     0.0000847637 smid     0.0000820194 snew     0.0000795176 \nitel  158 sold     0.0000795176 smid     0.0000769388 snew     0.0000745906 \nitel  159 sold     0.0000745906 smid     0.0000721671 snew     0.0000699633 \nitel  160 sold     0.0000699633 smid     0.0000676859 snew     0.0000656178 \nitel  161 sold     0.0000656178 smid     0.0000634777 snew     0.0000615371 \nitel  162 sold     0.0000615371 smid     0.0000595259 snew     0.0000577052 \nitel  163 sold     0.0000577052 smid     0.0000558151 snew     0.0000541071 \nitel  164 sold     0.0000541071 smid     0.0000523308 snew     0.0000507287 \nitel  165 sold     0.0000507287 smid     0.0000490594 snew     0.0000475568 \nitel  166 sold     0.0000475568 smid     0.0000459880 snew     0.0000445788 \nitel  167 sold     0.0000445788 smid     0.0000431046 snew     0.0000417832 \nitel  168 sold     0.0000417832 smid     0.0000403977 snew     0.0000391588 \nitel  169 sold     0.0000391588 smid     0.0000379805 snew     0.0000368139 \nitel  170 sold     0.0000368139 smid     0.0000357070 snew     0.0000346100 \nitel  171 sold     0.0000346100 smid     0.0000335698 snew     0.0000325382 \nitel  172 sold     0.0000325382 smid     0.0000315605 snew     0.0000305904 \nitel  173 sold     0.0000305904 smid     0.0000296716 snew     0.0000287591 \nitel  174 sold     0.0000287591 smid     0.0000278957 snew     0.0000270375 \nitel  175 sold     0.0000270375 smid     0.0000262261 snew     0.0000254189 \nitel  176 sold     0.0000254189 smid     0.0000246563 snew     0.0000238972 \nitel  177 sold     0.0000238972 smid     0.0000231804 snew     0.0000224647 \nitel  178 sold     0.0000224647 smid     0.0000217632 snew     0.0000210923 \nitel  179 sold     0.0000210923 smid     0.0000204335 snew     0.0000198033 \nitel  180 sold     0.0000198033 smid     0.0000191846 snew     0.0000185926 \nitel  181 sold     0.0000185926 smid     0.0000180116 snew     0.0000174556 \nitel  182 sold     0.0000174556 smid     0.0000169099 snew     0.0000163878 \nitel  183 sold     0.0000163878 smid     0.0000158752 snew     0.0000153849 \nitel  184 sold     0.0000153849 smid     0.0000149035 snew     0.0000144430 \nitel  185 sold     0.0000144430 smid     0.0000139910 snew     0.0000135586 \nitel  186 sold     0.0000135586 smid     0.0000131340 snew     0.0000127280 \nitel  187 sold     0.0000127280 smid     0.0000123292 snew     0.0000119480 \nitel  188 sold     0.0000119480 smid     0.0000115735 snew     0.0000112155 \nitel  189 sold     0.0000112155 smid     0.0000108639 snew     0.0000105277 \nitel  190 sold     0.0000105277 smid     0.0000101975 snew     0.0000098819 \nitel  191 sold     0.0000098819 smid     0.0000095718 snew     0.0000092755 \nitel  192 sold     0.0000092755 smid     0.0000089843 snew     0.0000087061 \nitel  193 sold     0.0000087061 smid     0.0000084327 snew     0.0000081715 \nitel  194 sold     0.0000081715 smid     0.0000079148 snew     0.0000076696 \nitel  195 sold     0.0000076696 smid     0.0000074285 snew     0.0000071984 \nitel  196 sold     0.0000071984 smid     0.0000069720 snew     0.0000067559 \nitel  197 sold     0.0000067559 smid     0.0000065434 snew     0.0000063406 \nitel  198 sold     0.0000063406 smid     0.0000061410 snew     0.0000059506 \nitel  199 sold     0.0000059506 smid     0.0000057633 snew     0.0000055845 \nitel  200 sold     0.0000055845 smid     0.0000054083 snew     0.0000052405 \nitel  201 sold     0.0000052405 smid     0.0000050750 snew     0.0000049176 \nitel  202 sold     0.0000049176 smid     0.0000047623 snew     0.0000046145 \nitel  203 sold     0.0000046145 smid     0.0000044687 snew     0.0000043301 \nitel  204 sold     0.0000043301 smid     0.0000041932 snew     0.0000040631 \nitel  205 sold     0.0000040631 smid     0.0000039346 snew     0.0000038125 \nitel  206 sold     0.0000038125 smid     0.0000036919 snew     0.0000035773 \nitel  207 sold     0.0000035773 smid     0.0000034642 snew     0.0000033566 \nitel  208 sold     0.0000033566 smid     0.0000032504 snew     0.0000031494 \nitel  209 sold     0.0000031494 smid     0.0000030498 snew     0.0000029550 \nitel  210 sold     0.0000029550 smid     0.0000028615 snew     0.0000027726 \nitel  211 sold     0.0000027726 smid     0.0000026845 snew     0.0000026011 \nitel  212 sold     0.0000026011 smid     0.0000025174 snew     0.0000024393 \nitel  213 sold     0.0000024393 smid     0.0000023599 snew     0.0000022866 \nitel  214 sold     0.0000022866 smid     0.0000022111 snew     0.0000021425 \nitel  215 sold     0.0000021425 smid     0.0000020706 snew     0.0000020064 \nitel  216 sold     0.0000020064 smid     0.0000019381 snew     0.0000018781 \nitel  217 sold     0.0000018781 smid     0.0000018130 snew     0.0000017570 \nitel  218 sold     0.0000017570 smid     0.0000016951 snew     0.0000016428 \nitel  219 sold     0.0000016428 smid     0.0000015840 snew     0.0000015352 \nitel  220 sold     0.0000015352 smid     0.0000014793 snew     0.0000014338 \nitel  221 sold     0.0000014338 smid     0.0000013806 snew     0.0000013422 \nitel  222 sold     0.0000013422 smid     0.0000013125 snew     0.0000012735 \nitel  223 sold     0.0000012735 smid     0.0000012451 snew     0.0000012080 \nitel  224 sold     0.0000012080 smid     0.0000011811 snew     0.0000011458 \nitel  225 sold     0.0000011458 smid     0.0000011202 snew     0.0000010867 \nitel  226 sold     0.0000010867 smid     0.0000010624 snew     0.0000010305 \nitel  227 sold     0.0000010305 smid     0.0000010074 snew     0.0000009771 \nitel  228 sold     0.0000009771 smid     0.0000009551 snew     0.0000009264 \nitel  229 sold     0.0000009264 smid     0.0000009055 snew     0.0000008783 \nitel  230 sold     0.0000008783 smid     0.0000008584 snew     0.0000008326 \nitel  231 sold     0.0000008326 smid     0.0000008137 snew     0.0000007892 \nitel  232 sold     0.0000007892 smid     0.0000007713 snew     0.0000007480 \nitel  233 sold     0.0000007480 smid     0.0000007310 snew     0.0000007089 \nitel  234 sold     0.0000007089 smid     0.0000006927 snew     0.0000006718 \nitel  235 sold     0.0000006718 smid     0.0000006564 snew     0.0000006366 \nitel  236 sold     0.0000006366 smid     0.0000006220 snew     0.0000006033 \nitel  237 sold     0.0000006033 smid     0.0000005897 snew     0.0000005723 \nitel  238 sold     0.0000005723 smid     0.0000005599 snew     0.0000005439 \nitel  239 sold     0.0000005439 smid     0.0000005326 snew     0.0000005175 \nitel  240 sold     0.0000005175 smid     0.0000005072 snew     0.0000004925 \nitel  241 sold     0.0000004925 smid     0.0000004681 snew     0.0000004546 \nitel  242 sold     0.0000004546 smid     0.0000004332 snew     0.0000004206 \nitel  243 sold     0.0000004206 smid     0.0000004013 snew     0.0000003894 \nitel  244 sold     0.0000003894 smid     0.0000003720 snew     0.0000003610 \nitel  245 sold     0.0000003610 smid     0.0000003451 snew     0.0000003349 \nitel  246 sold     0.0000003349 smid     0.0000003204 snew     0.0000003108 \nitel  247 sold     0.0000003108 smid     0.0000002976 snew     0.0000002887 \nitel  248 sold     0.0000002887 smid     0.0000002767 snew     0.0000002683 \nitel  249 sold     0.0000002683 smid     0.0000002573 snew     0.0000002496 \nitel  250 sold     0.0000002496 smid     0.0000002396 snew     0.0000002323 \nitel  251 sold     0.0000002323 smid     0.0000002231 snew     0.0000002164 \nitel  252 sold     0.0000002164 smid     0.0000002080 snew     0.0000002017 \nitel  253 sold     0.0000002017 smid     0.0000001941 snew     0.0000001881 \nitel  254 sold     0.0000001881 smid     0.0000001812 snew     0.0000001757 \nitel  255 sold     0.0000001757 smid     0.0000001694 snew     0.0000001643 \nitel  256 sold     0.0000001643 smid     0.0000001585 snew     0.0000001538 \nitel  257 sold     0.0000001538 smid     0.0000001486 snew     0.0000001441 \nitel  258 sold     0.0000001441 smid     0.0000001393 snew     0.0000001352 \nitel  259 sold     0.0000001352 smid     0.0000001308 snew     0.0000001269 \nitel  260 sold     0.0000001269 smid     0.0000001228 snew     0.0000001192 \nitel  261 sold     0.0000001192 smid     0.0000001155 snew     0.0000001121 \nitel  262 sold     0.0000001121 smid     0.0000001086 snew     0.0000001054 \nitel  263 sold     0.0000001054 smid     0.0000001023 snew     0.0000000993 \nitel  264 sold     0.0000000993 smid     0.0000000964 snew     0.0000000936 \nitel  265 sold     0.0000000936 smid     0.0000000910 snew     0.0000000883 \nitel  266 sold     0.0000000883 smid     0.0000000859 snew     0.0000000834 \nitel  267 sold     0.0000000834 smid     0.0000000812 snew     0.0000000788 \nitel  268 sold     0.0000000788 smid     0.0000000768 snew     0.0000000746 \nitel  269 sold     0.0000000746 smid     0.0000000727 snew     0.0000000706 \nitel  270 sold     0.0000000706 smid     0.0000000689 snew     0.0000000670 \nitel  271 sold     0.0000000670 smid     0.0000000654 snew     0.0000000637 \nitel  272 sold     0.0000000637 smid     0.0000000622 snew     0.0000000606 \nitel  273 sold     0.0000000606 smid     0.0000000593 snew     0.0000000577 \nitel  274 sold     0.0000000577 smid     0.0000000565 snew     0.0000000550 \nitel  275 sold     0.0000000550 smid     0.0000000539 snew     0.0000000525 \nitel  276 sold     0.0000000525 smid     0.0000000515 snew     0.0000000502 \nitel  277 sold     0.0000000502 smid     0.0000000492 snew     0.0000000479 \nitel  278 sold     0.0000000479 smid     0.0000000471 snew     0.0000000459 \nitel  279 sold     0.0000000459 smid     0.0000000451 snew     0.0000000439 \nitel  280 sold     0.0000000439 smid     0.0000000432 snew     0.0000000421 \nitel  281 sold     0.0000000421 smid     0.0000000414 snew     0.0000000404 \nitel  282 sold     0.0000000404 smid     0.0000000397 snew     0.0000000388 \nitel  283 sold     0.0000000388 smid     0.0000000382 snew     0.0000000373 \nitel  284 sold     0.0000000373 smid     0.0000000367 snew     0.0000000359 \nitel  285 sold     0.0000000359 smid     0.0000000354 snew     0.0000000345 \nitel  286 sold     0.0000000345 smid     0.0000000341 snew     0.0000000333 \nitel  287 sold     0.0000000333 smid     0.0000000328 snew     0.0000000321 \nitel  288 sold     0.0000000321 smid     0.0000000317 snew     0.0000000310 \nitel  289 sold     0.0000000310 smid     0.0000000306 snew     0.0000000300 \nitel  290 sold     0.0000000300 smid     0.0000000296 snew     0.0000000290 \nitel  291 sold     0.0000000290 smid     0.0000000286 snew     0.0000000281 \nitel  292 sold     0.0000000281 smid     0.0000000278 snew     0.0000000274 \nitel  293 sold     0.0000000274 smid     0.0000000271 snew     0.0000000268 \nitel  294 sold     0.0000000268 smid     0.0000000265 snew     0.0000000262 \nitel  295 sold     0.0000000262 smid     0.0000000260 snew     0.0000000258 \nitel  296 sold     0.0000000258 smid     0.0000000256 snew     0.0000000255 \nitel  297 sold     0.0000000255 smid     0.0000000253 snew     0.0000000251 \nitel  298 sold     0.0000000251 smid     0.0000000250 snew     0.0000000249 \nitel  299 sold     0.0000000249 smid     0.0000000247 snew     0.0000000246 \nitel  300 sold     0.0000000246 smid     0.0000000245 snew     0.0000000244 \nitel  301 sold     0.0000000244 smid     0.0000000243 snew     0.0000000242 \nitel  302 sold     0.0000000242 smid     0.0000000241 snew     0.0000000241 \nitel  303 sold     0.0000000241 smid     0.0000000239 snew     0.0000000240 \nitel  304 sold     0.0000000240 smid     0.0000000238 snew     0.0000000238 \nitel  305 sold     0.0000000238 smid     0.0000000237 snew     0.0000000237 \nitel  306 sold     0.0000000237 smid     0.0000000236 snew     0.0000000236 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe bounds we use are \\(\\delta_{ij}\\pm 1\\). After 306 iterations we arrive at stress 2.3629831^{-8}. In the configuration plot the centrists have moved to the center. ## Type AC4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfter 306 iterations stress is 2.3629831^{-8}, i.e. practically zero. We succeeded in moving all distances within the bounds. The additive constant is -3.0664907. The configuration is again pretty much the same with D’66 in the center. VVD moves closer to the Christian Democrats, and BP is more isolated. # References\n\n\n\n\nCooper, L. G. 1972. “A New Solution to the Additive Constant Problem in Metric Multidimensional Scaling.” Psychometrika 37 (3): 311–22.\n\n\nDe Gruijter, D. N. M. 1967. “The Cognitive Structure of Dutch Political Parties in 1966.” Report E019-67. Psychological Institute, University of Leiden.\n\n\nDe Leeuw, J., and I. Stoop. 1984. “Upper Bounds for Kruskal’s Stress.” Psychometrika 49: 391–402.\n\n\nEkman, G. 1954. “Dimensions of Color Vision.” Journal of Psychology 38: 467–74.\n\n\nHeiser, W. J. 1991. “A Generalized Majorization Method for Least Squares Multidimensional Scaling of Pseudodistances that May Be Negative.” Psychometrika 56 (1): 7–27.\n\n\nMessick, S. J., and R. P. Abelson. 1956. “The Additive Constant Problem in Multidimensional Scaling.” Psychometrika 21 (1–17).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Interval smacof</span>"
    ]
  }
]