[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "smacof at 50",
    "section": "",
    "text": "Note: This manual is a working manuscript which will be expanded/updated frequently. All suggestions for improvement are welcome. All Rmd, tex, html, pdf, R, and C files are in the public domain and can be copied, modified, and used by anybody in any way they see fit. Attribution will be appreciated, but is not required. The files can be found at https://github.com/deleeuw in the repositories smacofCode, smacofManual, and smacofExamples.\n\nPreface\nThis manual is definitely not an impartial and balanced review of all of multidimensional scaling (MDS) theory and history. It emphasizes computation, and the mathematics needed for computation. In addition, it is a summary of over 50 years of MDS work by me, either solo or together with my many excellent current or former co-workers and co-authors. It is heavily biased in favor of the smacof formulation of MDS (De Leeuw (1977), De Leeuw and Heiser (1977), De Leeuw and Mair (2009), Mair, Groenen, and De Leeuw (2022)), and the corresponding majorization (or MM) algorithms. And, moreover, I am shamelessly squeezing in as many references to my published and unpublished work as possible, with links to the corresponding pdf’s if they are available. Thus this book is also a jumpstation into my bibliography.\nI have not organized the book along historical lines because most of the early techniques and results have been either drastically improved or completely abandoned. Nevertheless, some personal historical perspective may be useful. I will put most of it in this preface, so uninterested readers can easily skip it.\nI got involved in MDS in 1968 when John van de Geer returned from a visit to Clyde Coombs in Michigan and started the Department of Data Theory in the Division of Social Sciences at Leiden University. I was John’s first hire, although I was still a graduate student at the time.\nRemember that Clyde Coombs was chairing the Michigan Mathematical Psychology Program, and he had just published his remarkable book “A Theory of Data” (Coombs (1964)). The name of the new department in Leiden was taken from the title of that book, and Coombs was one of the first visitors to give a guest lecture there.\nThis is maybe the place to clear up some possible misunderstandings about the name “Data Theory”. Coombs was mainly interested in a taxonomy of data types, and in pointing out that “data” were not limited to a table or data-frame of objects by variables. In addition, there were also similarity ratings, paired comparisons, and unfolding data. Coombs also emphasized that data were often non-metric, i.e. ordinal or categorical, and that it was possible to analyze these ordinal or categorical relationships directly, without first constructing numerical scales to which classical techniques could be applied. One of the new techniques discussed in Coombs (1964) was a ordinal form of MDS, in which not only the data but also the representation of the data in Euclidean space were non-metric.\nJohn van de Geer had just published Van de Geer (1967). In that book, and in the subsequent book Van de Geer (1971), he developed his unique geometric approach to multivariate analysis. Relationship between variables, and between variables and individuals, were not just discussed using matrix algebra, but were also visualized in diagrams. This was related to the geometric representations in Coombs’ Theory of Data, but it concentrated on numerical data in the form of rectangular matrices of objects by variables.\nLooking back it is easy to see that both Van de Geer and Coombs influenced my approach to data analysis. I inherited the emphasis on non-metric data and on visualization. But, from the beginning, I interpreted “Data Theory” as “Data Analysis”, with my emphasis shifting to techniques, loss functions, implementations, algorithms, optimization, computing, and programming. This is of interest because in 2020 my former Department of Statistics at UCLA, together with the Department of Mathematics, started a bachelor’s program in Data Theory, in which “Emphasis is placed on the development and theoretical support of a statistical model or algorithmic approach. Alternatively, students may undertake research on the foundations of data science, studying advanced topics and writing a senior thesis.” This sounds like a nice hybrid of Data Theory and Data Analysis, with a dash of computer science mixed in.\nComputing and optimization were in the air in 1968, not so much because of Coombs, but mainly because of Roger Shepard, Joe Kruskal, and Doug Carroll at Bell Labs in Murray Hill. John’s other student Eddie Roskam and I were fascinated by getting numerical representations from ordinal data by minimizing explicit least squares loss functions. Eddie wrote his dissertation in 1968 (Roskam (1968)). In 1973 I went to Bell Labs for a year, and Eddie went to Michigan around the same time to work with Jim Lingoes, resulting in Lingoes and Roskam (1973).\nMy first semi-publication was De Leeuw (1968), quickly followed by a long sequence of other, admittedly rambling, internal reports. Despite this very informal form of publication the sheer volume of them got the attention of Joe Kruskal and Doug Carroll, and I was invited to spend the academic year 1973-1974 at Bell Laboratories. That visit somewhat modified my cavalier approach to publication, but I did not become half-serious in that respect until meeting with Forrest Young and Yoshio Takane at the August 1975 US-Japan seminar on MDS in La Jolla. Together we used the alternating least squares approach to algorithm construction that I had developed since 1968 into a quite formidable five-year publication machine, with at its zenith Takane, Young, and De Leeuw (1977).\nIn La Jolla I gave the first presentation of the majorization method for MDS, later known as smacof, with the first formal convergence proof. The canonical account of smacof was published in a conference paper (De Leeuw (1977)). Again I did not bother to get the results into a journal or into some other more effective form of publication. The basic theory for what became known as smacof was also presented around the same time in another book chapter De Leeuw and Heiser (1977).\nIn 1978 I was invited to the Fifth International Symposium on Multivariate Analysis in Pittsburgh to present what eventually became De Leeuw and Heiser (1980). There I met Nan Laird, one of the authors of the basic paper on the EM algorithm (Dempster, Laird, and Rubin (1977)). I remember enthusiastically telling her on the conference bus that EM and smacof were both special case of the general majorization approach to algorithm construction, which was consequently born around the same time. But that is a story for a companion volume, which currently only exists in a very preliminary stage (https://github.com/deleeuw/bras).\nMy 1973 PhD thesis (De Leeuw (1973), reprinted as De Leeuw (1984)) was actually my second attempt at a dissertation. I had to get a PhD, any PhD, before going to Bell Labs, because of the difference between the Dutch and American academic title and reward systems. I started writing a dissertation on MDS, in the spirit of what later became De Leeuw and Heiser (1982). But halfway through I lost interest and got impatient, and I decided to switch to nonlinear multivariate analysis. This second attempt did produced a finished dissertation (De Leeuw (1973)), which grew over time, with the help of multitudes, into Gifi (1990). But that again is a different history, which I will tell some other time in yet another companion volume (https://github.com/deleeuw/gifi). For a long time I did not do much work on MDS, until the arrival of Patrick Mair and the R language led to a resurgence of my interest, and ultimately to De Leeuw and Mair (2009) and Mair, Groenen, and De Leeuw (2022).\nI consider this MDS book to be a summary and extension of the basic papers De Leeuw (1977), De Leeuw and Heiser (1977), De Leeuw and Heiser (1980), De Leeuw and Heiser (1982), and De Leeuw (1988), all written 30-40 years ago. Footprints in the sands of time. It can also be seen as an elaboration of the more mathematical and computational sections of the excellent and comprehensive textbook of Borg and Groenen (2005). That book has much more information about the origins, the data, and the applications of MDS, as well as on the interpretation of MDS solutions. In this book I concentrate almost exclusively on the mathematical, computational, and programming aspects of MDS.\nFor those who cannot get enough of me, there is a data base of my published and unpublished reports and papers since 1965, with links to pdf’s, at https://jansweb.netlify.app/publication/.\nThere are many, many people I have to thank for my scientific education. Sixty years is a long time, and consequently many excellent teachers and researchers have crossed my path. I will gratefully mention the academics who had a major influence on my work and who are not with us any more, since I will join them in the not too distant future: Louis Guttman (died 1987), Clyde Coombs (died 1988), Warren Torgerson (died 1999), Forrest Young (died 2006), John van de Geer (died 2008), Joe Kruskal (died 2010), Doug Carroll (died 2011), and Rod McDonald (died 2012).\nI will also use this preface to thank Rstudio, in particular J.J. Allaire, Hadley Wickham, and Yihui Xi, for their contributions to the R universe, and for their promotion of open source software and open access publications. Not too long ago I was an ardent LaTeX user, firmly convinced I would never use anything else again in my lifetime. In the same way that I was convinced before that I would never use anything besides, in that order, FORTRAN, PL/I, APL, and (X)Lisp. And PHP/Apache/MySQL. But I lived too long. And then, in my dotage, lo and behold, R, Rstudio, (R)Markdown, Quarto, ggplot, bookdown, blogdown, Git, Github, and Netlify came along.\n\n\n\n\n\nForrest Young, Bepi Pinner, Jean-Marie Bouroche, Yoshio Takane, Jan de Leeuw at La Jolla, August 1975\n\n\n\n\nIn this manual we study the smacof family of Multidimensional Scaling (MDS) techniques. In MDS the data consist of some type of information about the dissimilarities between a pairs of objects. These objects can be anything: individuals, variables, colors, locations, chemicals, molecules, works of Plato, political parties, Morse code signals, and so on. The dissimilarities can be approximate or imprecise distances, dissimilarity judgments, import/export tables, sociometric choices, and so on. They generally are distance-like, but we do not expect them to satisfy the triangle inequality, and in general not even non-negativity and symmetry. Similarities, such as confusion probabilities, correlations, or preferences, are always converted in some way or another to dissimilarities before they can serve as data for MDS.\nThe information we have about these dissimilarities can be numerical, ordinal, or categorical. Thus we may have the actual values of some or all of the dissimilarities, we may know their rank order, or we may have a classification of them into a small number of qualitative bins.\nLet’s formalize this, and introduce some notation at the same time. The set of ojects is \\(\\mathfrak{O}\\). For example, it can be the set of all cities with more than 10,000 inhabitants. In our MDS analysis we only use \\(O:=(o_1,\\cdots,o_n)\\), an n-tuple (i.e. a finite sequence) of \\(n\\) different elements of \\(\\mathfrak{O}\\), for example \\(n\\) capital cities selected from \\(\\mathfrak{O}\\). If you want to, you can call \\(O\\) a sample from \\(\\mathfrak{O}\\). It is entirely possible, however, that \\(\\mathfrak{O}\\) has only \\(n\\) elements, in which case \\(O\\) is just an permutation of the elements of \\(\\mathfrak{O}\\).\nA dissimilarity is a function \\(\\delta\\) on all pairs of objects, with values in a set \\(\\mathfrak{D}\\). It can be, for example, the time in seconds for an airline flight from city one to city two. Thus \\(\\delta:\\mathfrak{O}\\otimes\\mathfrak{O}\\Rightarrow\\mathfrak{D}\\). A dissimilaritry is numerical if \\(\\mathfrak{D}\\) is subset of real line, it is ordinal if \\(\\mathfrak{D}\\) is a partially ordered set, and it is nominal if \\(\\mathfrak{D}\\) is neither. Or a dissimilarty is nominal if \\(\\mathfrak{D}\\) is any set, and we choose to ignore the ordinal and numerical information if it is there. No matter what \\(\\mathfrak{D}\\) is, we suppose it always has the element \\(\\mathit{NA}\\) to indicate missing dissimilarities. Cities may not have airports, for example, or we just don’t have the information about the airline distances. Define \\(\\delta_{ij}:=\\delta(o_i,o_j)\\) and \\(\\Delta:=\\delta(O\\times O)\\). We can think of \\(\\Delta\\) and an \\(n\\times n\\) matrix with elements in \\(\\mathfrak{D}\\).\nMDS techniques map the objects \\(o_i\\) into points \\(x_i\\) in some metric space \\(\\langle\\mathfrak{X},d\\rangle\\) in such a way that the distances between pairs of points approximate the dissimilarities of the corresponding pairs of objects. Thus we want to find a map \\(x:\\mathfrak{O}\\rightarrow\\mathfrak{X}\\) that produces an n-tuple \\(X=(x_1,\\cdots,x_n)\\) of elements of \\(\\mathfrak{X}\\), where \\(x_i:=x(o_i)\\). Also define \\(d_{ij}:=d(x_i,x_j)\\) and \\(D(X):=d(X\\times X\\). Unlike the dissimilarities the \\(d_{ij}\\) are always numerical, because distances are. So MDS finds \\(X\\) such that \\(D(X)\\approx\\Delta\\).\nFor numerical dissimilarities it is clear what “approximation” means, we simply want the distances and the corresponding dissimilarities to be numerically close. Because there are generally many dissimilarities and distances a combined measure of closeness can still be defined in many different ways. For ordinal and nominal dissimilarities the notion of approximation is less clear, and we have to develop more specialized techniques to measure how well the distances fit the dissimilarities.\n\n\n\n\nBorg, I., and P. J. F. Groenen. 2005. Modern Multidimensional Scaling. Second Edition. Springer.\n\n\nCoombs, C. H. 1964. A Theory of Data. Wiley.\n\n\nDe Leeuw, J. 1968. “Nonmetric Multidimensional Scaling.” Research Note 010-68. Department of Data Theory FSW/RUL. https://jansweb.netlify.app/publication/deleeuw-r-68-g/deleeuw-r-68-g.pdf.\n\n\n———. 1973. “Canonical Analysis of Categorical Data.” PhD thesis, University of Leiden, The Netherlands. https://jansweb.netlify.app/publication/deleeuw-b-73/deleeuw-b-73.pdf.\n\n\n———. 1977. “Applications of Convex Analysis to Multidimensional Scaling.” In Recent Developments in Statistics, edited by J. R. Barra, F. Brodeau, G. Romier, and B. Van Cutsem, 133–45. Amsterdam, The Netherlands: North Holland Publishing Company.\n\n\n———. 1984. Canonical Analysis of Categorical Data. Leiden, The Netherlands: DSWO Press. https://jansweb.netlify.app/publication/deleeuw-b-84/deleeuw-b-84.pdf.\n\n\n———. 1988. “Convergence of the Majorization Method for Multidimensional Scaling.” Journal of Classification 5: 163–80.\n\n\nDe Leeuw, J., and W. J. Heiser. 1977. “Convergence of Correction Matrix Algorithms for Multidimensional Scaling.” In Geometric Representations of Relational Data, edited by J. C. Lingoes, 735–53. Ann Arbor, Michigan: Mathesis Press.\n\n\n———. 1980. “Multidimensional Scaling with Restrictions on the Configuration.” In Multivariate Analysis, Volume V, edited by P. R. Krishnaiah, 501–22. Amsterdam, The Netherlands: North Holland Publishing Company.\n\n\n———. 1982. “Theory of Multidimensional Scaling.” In Handbook of Statistics, Volume II, edited by P. R. Krishnaiah and L. Kanal. Amsterdam, The Netherlands: North Holland Publishing Company.\n\n\nDe Leeuw, J., and P. Mair. 2009. “Multidimensional Scaling Using Majorization: SMACOF in R.” Journal of Statistical Software 31 (3): 1–30. https://www.jstatsoft.org/article/view/v031i03.\n\n\nDempster, A. P., N. M. Laird, and D. B. Rubin. 1977. “Maximum Likelihood for Incomplete Data via the EM Algorithm.” Journal of the Royal Statistical Society B39: 1–38.\n\n\nGifi, A. 1990. Nonlinear Multivariate Analysis. New York, N.Y.: Wiley.\n\n\nLingoes, J. C., and E. E. Roskam. 1973. “A Mathematical and Empirical Analysis of Two Multidimensional Scaling Algorithms.” Psychometrika 38: Monograph Supplement.\n\n\nMair, P., P. J. F. Groenen, and J. De Leeuw. 2022. “More on Multidimensional Scaling in R: smacof Version 2.” Journal of Statistical Software 102 (10): 1–47. https://www.jstatsoft.org/article/view/v102i10.\n\n\nRoskam, E. E. 1968. “Metric Analysis of Ordinal Data in Psychology.” PhD thesis, University of Leiden.\n\n\nTakane, Y., F. W. Young, and J. De Leeuw. 1977. “Nonmetric Individual Differences in Multidimensional Scaling: An Alternating Least Squares Method with Optimal Scaling Features.” Psychometrika 42: 7–67.\n\n\nVan de Geer, J. P. 1967. Inleiding in de Multivariate Analyse. Van Loghum Slaterus.\n\n\n———. 1971. Introduction to Multivariate Analysis for the Social Sciences. San Francisco, CA: Freeman.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "2 Brief History\nIn this manual we study the smacof family of Multidimensional Scaling (MDS) techniques. In MDS the data consist of some type of information about the dissimilarities between a pairs of objects. These objects can be anything: individuals, variables, colors, locations, chemicals, molecules, works of Plato, political parties, Morse code signals, and so on. The dissimilarities can be approximate or imprecise distances, dissimilarity judgments, import/export tables, sociometric choices, and so on. They generally are distance-like, but we do not expect them to satisfy the triangle inequality, and in general not even non-negativity and symmetry. Similarities, such as confusion probabilities, correlations, or preferences, are always converted in some way or another to dissimilarities before they can serve as data for MDS.\nThe information we have about these dissimilarities can be numerical, ordinal, or categorical. Thus we may have the actual values of some or all of the dissimilarities, we may know their rank order, or we may have a classification of them into a small number of qualitative bins.\nLet’s formalize this, and introduce some notation at the same time. The set of ojects is \\(\\mathfrak{O}\\). For example, it can be the set of all cities with more than 10,000 inhabitants. In our MDS analysis we only use \\(O:=(o_1,\\cdots,o_n)\\), an n-tuple (i.e. a finite sequence) of \\(n\\) different elements of \\(\\mathfrak{O}\\), for example \\(n\\) capital cities selected from \\(\\mathfrak{O}\\). If you want to, you can call \\(O\\) a sample from \\(\\mathfrak{O}\\). It is entirely possible, however, that \\(\\mathfrak{O}\\) has only \\(n\\) elements, in which case \\(O\\) is just an permutation of the elements of \\(\\mathfrak{O}\\).\nA dissimilarity is a function \\(\\delta\\) on all pairs of objects, with values in a set \\(\\mathfrak{D}\\). It can be, for example, the time in seconds for an airline flight from city one to city two. Thus \\(\\delta:\\mathfrak{O}\\otimes\\mathfrak{O}\\Rightarrow\\mathfrak{D}\\). A dissimilaritry is numerical if \\(\\mathfrak{D}\\) is subset of real line, it is ordinal if \\(\\mathfrak{D}\\) is a partially ordered set, and it is nominal if \\(\\mathfrak{D}\\) is neither. Or a dissimilarty is nominal if \\(\\mathfrak{D}\\) is any set, and we choose to ignore the ordinal and numerical information if it is there. No matter what \\(\\mathfrak{D}\\) is, we suppose it always has the element \\(\\mathit{NA}\\) to indicate missing dissimilarities. Cities may not have airports, for example, or we just don’t have the information about the airline distances. Define \\(\\delta_{ij}:=\\delta(o_i,o_j)\\) and \\(\\Delta:=\\delta(O\\times O)\\). We can think of \\(\\Delta\\) and an \\(n\\times n\\) matrix with elements in \\(\\mathfrak{D}\\).\nMDS techniques map the objects \\(o_i\\) into points \\(x_i\\) in some metric space \\(\\langle\\mathfrak{X},d\\rangle\\) in such a way that the distances between pairs of points approximate the dissimilarities of the corresponding pairs of objects. Thus we want to find a map \\(x:\\mathfrak{O}\\rightarrow\\mathfrak{X}\\) that produces an n-tuple \\(X=(x_1,\\cdots,x_n)\\) of elements of \\(\\mathfrak{X}\\), where \\(x_i:=x(o_i)\\). Also define \\(d_{ij}:=d(x_i,x_j)\\) and \\(D(X):=d(X\\times X\\). Unlike the dissimilarities the \\(d_{ij}\\) are always numerical, because distances are. So MDS finds \\(X\\) such that \\(D(X)\\approx\\Delta\\).\nFor numerical dissimilarities it is clear what “approximation” means, we simply want the distances and the corresponding dissimilarities to be numerically close. Because there are generally many dissimilarities and distances a combined measure of closeness can still be defined in many different ways. For ordinal and nominal dissimilarities the notion of approximation is less clear, and we have to develop more specialized techniques to measure how well the distances fit the dissimilarities.\nDe Leeuw and Heiser (1980)\nThis section has a different emphasis. We limit ourselves to developments in Euclidean MDS, and to contributions with direct computational consequences that have a direct or indirect link to psychometrics, and to work before 1960. This is reviewed ably in the presidential address of W. S. Torgerson (1965).\nOur history review takes the form of brief summaries of what we consider to be milestone papers or books.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#prehistory",
    "href": "intro.html#prehistory",
    "title": "1  Introduction",
    "section": "2.1 Prehistory",
    "text": "2.1 Prehistory\nThe prehistory of MDS is defined as any publication before Young and Householder (1938).\nstumpf 1883\n\nUnter Distanzen aber verstehen wir, das Wort hier ebenfalls in einem für Manche ungewohnt weiten Sinne nehmend, nicht blos räumliche und zeitliche sondern auch qualitative und solche der Intensität, und definieren das Wort durch: Grade der Unähnlichkeit.\n\nFrom the translation\n\nTaking the word “distances” in a sense uncommonly broad for many, however, we mean here not only spatial and temporal ones, but also qualitative ones as well as ones of intensity, and define the word by degrees of dissimilarity.\n\nfisher 1922 boyden 1932/1935 goldmeier 1937\nrichardson 1938 klingberg 1941 gulliksen 1946 attneave 1950 ekman 1954\ntorgerson 1951 torgerson 1952 messick_abelson 1956\nreviewed in De Leeuw and Heiser (1980).\nYoung-Householder, etc.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#torgerson",
    "href": "intro.html#torgerson",
    "title": "1  Introduction",
    "section": "2.2 Torgerson",
    "text": "2.2 Torgerson\nW. S. Torgerson (1952) W. S. Torgerson (1965)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#bell-laboratories",
    "href": "intro.html#bell-laboratories",
    "title": "1  Introduction",
    "section": "2.3 Bell Laboratories",
    "text": "2.3 Bell Laboratories\nShepard (1962a) Shepard (1962b)\nKruskal (1964a) Kruskal (1964b)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#guttman-lingoes",
    "href": "intro.html#guttman-lingoes",
    "title": "1  Introduction",
    "section": "2.4 Guttman-Lingoes",
    "text": "2.4 Guttman-Lingoes\nGuttman (1968)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#alternating-least-squares",
    "href": "intro.html#alternating-least-squares",
    "title": "1  Introduction",
    "section": "2.5 Alternating Least Squares",
    "text": "2.5 Alternating Least Squares",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#majorization",
    "href": "intro.html#majorization",
    "title": "1  Introduction",
    "section": "2.6 Majorization",
    "text": "2.6 Majorization\nDe Leeuw (1977) De Leeuw and Heiser (1977)\nThere was some early work by Richardson, Messick, Abelson and Torgerson who combined Thurstonian scaling of similarities with the mathematical results of Schoenberg (1935) and Young and Householder (1938).\nDespite these early contributions it makes sense, certainly from the point of view of my personal history, but probably more generally, to think of MDS as starting as a widely discussed, used, and accepted technique since the book by W. S. Torgerson (1958). This was despite the fact that in the fifties and sixties computing eigenvalues and eigenvectors of a matrix of size 20 or 30 was still a considerable challenge.\nA few years later the popularity of MDS got a large boost by developments centered at Bell Telephone Laboratories in Murray Hill, New Jersey, the magnificent precursor of Silicon Valley. First there was nonmetric MDS by Shepard (1962a), Shepard (1962b) and Kruskal (1964a), Kruskal (1964b), And later another major development was the introduction of individual difference scaling by Carroll and Chang (1970) and Harshman (1970). Perhaps even more important was the development of computer implementations of these new techniques. Some of the early history of nonmetric MDS is in De Leeuw (2017a).\nAround the same time there were interesting theoretical contributions in Coombs (1964), which however did not much influence the practice of MDS. ….. And several relatively minor variations of the Bell Laboratories approach were proposed by Guttman (1968), but Guttman’s influence on further MDS implementations turned out to be fairly localized and limited.\nThe main development in comptational MDS after the Bell Laboratories surge was probably smacof. Initially, in De Leeuw (1977), this stood for Scaling by Maximizing a Convex Function. Later it was also used to mean Scaling by Majorizing a Complicated Function. Whatever. In this book smacof just stands for smacof. No italics, no boldface, no capitals.\nThe first smacof programs were written in 1977 in FORTRAN at the Department of Data Theory in Leiden (Heiser and De Leeuw (1977)). Eventually they migrated to SPSS (for example, Meulman and Heiser (2012)) and to R (De Leeuw and Mair (2009)). The SPSS branch, now the IBM SPSS branch, and the R branch have diverged somewhat, and they continue to be developed independently.\nParallel to this book there is an attempt to rewrite the various smacof programs in C, with the necessary wrappers to call them from R (De Leeuw (2017b)). The C code, with makefiles and test routines, is at github.com/deleeuw/smacof",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#kruskals-stress",
    "href": "intro.html#kruskals-stress",
    "title": "1  Introduction",
    "section": "3.1 Kruskal’s stress",
    "text": "3.1 Kruskal’s stress\nDefinition @ref(eq:stressall) differs from Kruskal’s original stress in at least three ways: in Kruskal’s use of the square root, in our use of weights, and in our different approach to normalization.\nWe have paid so much attention to Kruskal’s original definition, because the choices made there will play a role in the normalization discussion in the ordinal scaling chapter (section @ref(nmdsnorm)), in the comparison of Kruskal’s and Guttman’s approach to ordinal MDS (sections @ref(nmdskruskal) and @ref(nmdsguttman)), and in our discussions about the differences between Kruskal’s stress @ref(eq:kruskalstressfinal) and smacof’s stress @ref(eq:stressall) in the next three sections of this chapter.\n\n3.1.0.1 Square root\nLet’s discuss the square root first. Using it or not using it does not make a difference for the minimization problem. Using the square root, however, does give a more sensible root-mean-square scale, in which stress is homogeneous of degree one, instead of degree two. But I do not want to compute all those unnecessary square roots in my algorithms, and I do not want to drag them along through my derivations. Moreover the square root potentially causes problems with differentiability at those \\(X\\) where \\(\\sigma(X)\\) is zero. Thus, througout the book, we do not use the square root in our formulas and derivations. In fact, we do not even use it in our computer programs, except at the very last moment when we return the final stress after the algorithm has completed.\n\n\n3.1.0.2 Weights\nThere were no weights \\(W=\\{w_{ij}\\}\\) in the original definition of stress by Kruskal (1964a), and neither are they there in most of the basic later contributions to MDS by Guttman, Lingoes, Roskam, Ramsay, or Young. We will use weights throughout the book, because they have various interesting applications within basic MDS, without unduly complicating the derivations and computations. In Groenen and Van de Velden (2016), section 6, the various uses of weights in the stress loss function are enumerated. They generously, and correctly, attribute the consistent use of weights in MDS to me. I quote from their paper:\n\n\nHandling missing data is done by specifying \\(w_{ij} = 0\\) for missings and 1 otherwise thereby ignoring the error corresponding to the missing dissimilarities.\nCorrecting for nonuniform distributions of the dissimilarities to avoid dominance of the most frequently occurring dissimilarities.\nMimicking alternative fit functions for MDS by minimizing Stress with \\(w_{ij}\\) being a function of the dissimilarities.\nUsing a power of the dissimilarities to emphasize the ﬁtting of either large or small dissimilarities.\nSpecial patterns of weights for speciﬁc models.\nUsing a speciﬁc choice of weights to avoid nonuniqueness.\n\n\nIn some situations, for example for huge data sets, it is computationally convenient, or even necessary, to minimize the influence of the weights on the computations. We can use majorization to turn the problem from a weighted least squares problem to an iterative unweighted least squares problem. The technique, which we call unweighting, is discussed in detail in section @ref(minunweight).\n\n\n3.1.0.3 Normalization\nThis section deals with a rather trivial problem, which has however caused problems in various stages of smacof’s 50-year development history. Because the problem is trivial, and the choices that must be made are to a large extent arbitrary, it has been overlooked and somewhat neglected.\nIn basic MDS we scale the weights and dissimilarities. It is clear that if we multiply the weights or dissimilarities by a constant, then the optimal approximating distances \\(D(X)\\) and the optimal configuration \\(X\\) will be multiplied by the same constant. That is exactly why Kruskal’s raw stress had to be normalized. Consequently we in basic MDS we always scale weights and dissimilarities by\n\\[\\begin{align}\n\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}&=1,(\\#eq:scaldiss1)\\\\\n\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}^{\\ }\\delta_{ij}^2&=1.(\\#eq:scaldiss2)\n\\end{align}\\]\nThis simplifies our formulas and makes them look better (see, for example, section @ref(propexpand) and section @ref(secrhostress)). It presupposes, of course, that \\(w_{ij}\\delta_{ij}\\not=0\\) for at least one \\(i\\not= j\\), which we will happily assume in the sequel, because otherwise the MDS problem is trivial. Note that if all weights are equal (which we call the unweighted case) then they are equal to \\(1/\\binom{n}{2}\\) and thus we require \\(\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}\\delta_{ij}^2=\\frac12n(n-1)\\).\nUsing normalized dissimilarities amounts to the same defining stress as \\[\\begin{equation}\n\\sigma(X)=\\frac12\\frac{\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}(\\delta_{ij}^2-d_{ij}(X))^2}{\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}\\delta_{ij}^2}.\n(\\#eq:stressrat)\n\\end{equation}\\]\nThis is useful to remember when we discuss the various normalizations for non-metric MDS in section @ref(nmdsnorm).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#seclocglob",
    "href": "intro.html#seclocglob",
    "title": "1  Introduction",
    "section": "3.2 Local and Global Minima",
    "text": "3.2 Local and Global Minima\nIn basic MDS our goal is to compute both \\(\\min_X\\sigma(X)\\) and \\(\\mathop{\\text{Argmin}}_X\\sigma(X)\\), where \\(\\sigma(X)\\) is defined as @ref(eq:stressall), and where we minimize over all configurations in \\(\\mathbb{R}^{n\\times p}\\).\nIn this book we study both the properties of the stress loss function and a some of its generalizations, and the various ways to minimize these loss functions over configurations (and sometimes over transformations of the dissimilarities as well).\nEmphasis local minima\nCompute stationary points",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#partitioning-loss",
    "href": "intro.html#partitioning-loss",
    "title": "1  Introduction",
    "section": "3.3 Partitioning Loss",
    "text": "3.3 Partitioning Loss",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#non-linear-mds",
    "href": "intro.html#non-linear-mds",
    "title": "1  Introduction",
    "section": "4.1 Non-linear MDS",
    "text": "4.1 Non-linear MDS",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#gennonmetric",
    "href": "intro.html#gennonmetric",
    "title": "1  Introduction",
    "section": "4.2 Non-metric MDS",
    "text": "4.2 Non-metric MDS\nBasic MDS is a form of Metric Multidimensional Scaling or MMDS, in which dissimilarities are either known or missing. In chapter @ref(nonmtrmds) we relax this assumption. Dissimilarities may be partly known, for example we may know they are in some interval, we may only know their order, or we may know them up to some smooth transformation. MDS with partly known dissimilarities is Non-metric Multidimensional Scaling or NMDS. Completely unknown (missing) dissimilarities are an exception, because we can just handle this in basic MDS by setting the corresponding weights equal to zero.\nIn NMDS we minimize stress over all configurations, but also over the unknown dissimilarities. What we know about them (the interval they are in, the transformations that are allowed, the order they are in) defines a subset of the space of non-negative, hollow, and symmetric matrices. Any matrix in that subset is a matrix of what Takane, Young, and De Leeuw (1977) call disparities, i.e. imputed dissimilarities. The imputation provides the missing information and transforms the non-numerical information we have about the dissimilarities into a numerical matrix of disparities. Clearly this is an optimistic imputation, in the sense that it chooses from the set of admissible disparities to minimize stress (for a given configuration).\nOne more terminological point. Often non-metric is reserved for ordinal MDS, in which we only know a (partial or complete) order of the dissimilarities. Allowing linear or polynomial transformations of the dissimilarities, or estimating an additive constant, is then supposed to be a form of metric MDS. There is something to be said for that. Maybe it makes sense to distinguish non-metric in the wide sense (in which stress must be minimized over both \\(X\\) and \\(\\Delta\\)) and non-metric in the narrow sense in which the set of admissible disparities is defined by linear inequalities. Nonmetric in the narrow sense will also be called ordinal MDS or OMDS.\nIt is perhaps useful to remember that Kruskal (1964a) introduced explicit loss functions in MDS to put the somewhat heuristic NMDS techniques of Shepard (1962a) onto a firm mathematical and computational foundation. Thus, more or less from the beginning of iterative least squares MDS, there was a focus on non-metric rather than metric MDS, and this actually contributed a great deal to the magic and success of the technique. In this book most of the results are derived for basic MDS, which is metric MDS, with non-metric MDS as a relatively straightforward extension not discussed until chapter @ref(nonmtrmds). So, at least initially, we take the numerical values of the dissimilarities seriously, as do W. S. Torgerson (1958) and Shepard (1962a), Shepard (1962b).\nIt may be the case that in the social and behavioural sciences only the ordinal information in the dissimilarities is reliable and useful. But, since 1964, MDS has also been applied in molecular conformation, chemometrics, genetic sequencing, archelogical seriation, and in network design and location analysis. In these areas the numerical information in the dissimilarities is usually meaningful and should not be thrown out right away. Also, the use of the Shepard plot, with dissimilarities on the horizontal axis and fitted distances on the vertical axis, suggests there is more to dissimilarities than just their rank order.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#genfstress",
    "href": "intro.html#genfstress",
    "title": "1  Introduction",
    "section": "4.3 Fstress and Friends",
    "text": "4.3 Fstress and Friends\nInstead of defining the residuals in the least squares loss function as \\(\\delta_{ij}-d_{ij}(X)\\) chapter @ref(chrstress) discusses the more general cases where the residuals are \\(f(\\delta_{ij})-g(d_{ij}(X))\\) for some known non-negative increasing function \\(f\\). This defines the fstress loss function.\nIf \\(f(x)=x^r\\) with \\(r&gt;0\\) then fstress is called rstress. Thus stress is rstress with \\(r=1\\), also written as 1stress or \\(\\sigma_1\\). In more detail we will also look at \\(r=2\\), which is called sstress by Takane, Young, and De Leeuw (1977). In chapter @ref(chsstressstrain) we look at the problem of minimizing sstress and weighted version strain. The case of rstress with \\(r\\rightarrow 0\\) is also of interest, because it leads to the loss function in Ramsay (1977).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#gencons",
    "href": "intro.html#gencons",
    "title": "1  Introduction",
    "section": "4.4 Constraints",
    "text": "4.4 Constraints\nInstead of minimizing stress over all \\(X\\) in \\(\\mathbb{R}^{n\\times p}\\) we will look in chapter @ref(cmds) at various generalizations where minimization is over a subset \\(\\mathcal{\\Omega}\\) of \\(\\mathbb{R}^{n\\times p}\\). This is often called Constrained Multidimensional Scaling or CMDS.\nThe distinction may be familiar from factor analysis, where we distinguish between exploratory and confirmatory factor analysis. If we have prior information about the parameters then incorporating that prior information in the analysis will generally lead to more precise and more interpretable estimates. The risk is, of course that if our prior information is wrong, if it is just prejudice, then we will have a solution which is precise but incorrect. We have the famous trade-off between bias and variance. In MDS this trade-off does not seem to apply directly, because the necessary replication frameworks are missing.\nand we do not attach much value to locating the true configuration.\nPrimal and Dual\n\\[\n\\min_{X\\in\\Omega}\\sigma(X)\n\\]\n\\[\n\\min_X\\sigma(X)+\\lambda\\kappa(X,\\Omega)\n\\] where \\(\\kappa(X,\\Omega)\\geq 0\\) and \\(\\kappa(X,\\Omega)=0\\) if and only if \\(X\\in\\Omega\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#inreplic",
    "href": "intro.html#inreplic",
    "title": "1  Introduction",
    "section": "4.5 Individual Differences",
    "text": "4.5 Individual Differences\nNow consider the situation in which we have \\(m\\) different dissimilarity matrices \\(\\Delta_k\\) and \\(m\\) different weight matrices \\(W_k\\). We generalize basic MDS by defining \\[\\begin{equation}\n\\sigma(X_1,\\cdots,X_m):=\\frac12\\sum_{k=1}^m\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ijk}(\\delta_{ijk}-d_{ij}(X_k))^2,\n(\\#eq:replistress)\n\\end{equation}\\] and minimize this over the \\(X_k\\).\nThere are two simple ways to deal with this generalization. The first is to put no further constraints on the \\(X_k\\). This means solving \\(m\\) separate basic MDS problems, one for each \\(k\\). The second way is to require that all \\(X_k\\) are equal. As shown in more detail in section @ref(indifrepl) this reduced to a single basic MDS problem with dissimilarities that are a weighted sum of the \\(\\Delta_k\\). So both these approaches do not really bring anything new.\nMinimizing @ref(eq:replistress) becomes more interesting if we constrain the \\(X_k\\) in various ways. Usually this is done by making sure they have a component that is common to all \\(k\\) and a component that is specific or unique to each \\(k\\). This approach, which generalizes constrained MDS, is discussed in detail in chapter @ref(chindif).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#genasym",
    "href": "intro.html#genasym",
    "title": "1  Introduction",
    "section": "4.6 Asymmetry",
    "text": "4.6 Asymmetry\nWe have seen in section @ref(datasym) of this chapter that in basic MDS the assumption that \\(W\\) and \\(\\Delta\\) are symmetric and hollow can be made without loss of generality. The simple partitioning which proved this was based on the fact that \\(D(X)\\) is always symmetric and hollow. By the way, the assumption that \\(W\\) and \\(D\\) are non-negative cannot be made without loss of generality, as we will see below.\nIn chapter @ref(asymmds) we relax the assumption that \\(D(X)\\) is symmetric (still requiring it to be non-negative and hollow). This could be called Asymmetric MDS, or AMDS. I was reluctant at first to include this chapter, because asymmetric distances do not exist. And certainly are not Euclidean distances, so they are not covered by the title of this book. But as long as we stay close to Euclidean distances, least squares, and the smacof approach, I now feel reasonably confident the chapter is not too much of a foreign body.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#non-euclidean-distances",
    "href": "intro.html#non-euclidean-distances",
    "title": "1  Introduction",
    "section": "4.7 Non-Euclidean Distances",
    "text": "4.7 Non-Euclidean Distances\nWhen Kruskal introduced gradient-based methods to minimize stress he also discussed the possibility to use Minkovski metrics other than the Euclidean metric. This certainly was part of the appeal of the new methods, in fact it seemed as if the gradient methods made it possible to use any distance function whatsoever. This initial feeling of empowerment was somewhat naive, because it ignored the seriousness of the local minimum problem, the combinatorial nature of one-dimensional and city block scaling, the problems with nonmetric unfolding, and the problematic nature of gradient methods if the distances are not everywhere differentiable. All these complications will be discussed in this book. But it made me decide to ignore Minkovski distances (and hyperbolic and elliptic non-Euclidean distances), because life with stress is complicated and challenging enough as it is.\nGroenen, Mathar, and Heiser (1995), Mathar and Meyer (1994)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#alternating-least-squares-1",
    "href": "intro.html#alternating-least-squares-1",
    "title": "1  Introduction",
    "section": "5.1 Alternating Least Squares",
    "text": "5.1 Alternating Least Squares",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#majorization-1",
    "href": "intro.html#majorization-1",
    "title": "1  Introduction",
    "section": "5.2 Majorization",
    "text": "5.2 Majorization",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#introduction-to-majorization",
    "href": "intro.html#introduction-to-majorization",
    "title": "1  Introduction",
    "section": "5.3 Introduction to Majorization",
    "text": "5.3 Introduction to Majorization\nMajorization, these days better known as MM (Lange (2016)), is a general approach for the construction of minimization algorithms. There is also minorization, which leads to maximization algorithms, which explains the MM acronym: minorization for maximization and majorization for minimization.\nBefore the MM principle was formulated as a general approach to algorithm construction there were some important predecessors. Major classes of MM algorithms avant la lettre were the EM Algorithm for maximum likelihood estimation of Dempster, Laird, and Rubin (1977), the Smacof Algorithm for MDS of De Leeuw (1977), the Generalized Weiszfeldt Method* of Vosz and Eckhardt (1980), and the Quadratic Approximation Method of Böhning and Lindsay (1988). The first formulation of the general majorization principle seems to be De Leeuw (1994).\nLet’s start with a brief introduction to majorization. Minimize a real valued function \\(\\sigma\\) over \\(x\\in\\mathbb{S}\\), where \\(\\mathbb{S}\\) is some subset of \\(\\mathbb{R}^n\\). There are obvious extensions of majorization to functions defined on more general spaces, with values in any partially ordered set, but we do not need that level of generality in this manual. Also majorization applied to \\(\\sigma\\) is minorization applied to \\(-\\sigma\\), so concentrating on majorization-minimization and ignoring minorization-maximization causes no loss of generality\nSuppose there is a real-valued function \\(\\omega\\) on \\(\\mathbb{S}\\otimes\\mathbb{S}\\) such that \\[\\begin{align}\n\\sigma(x)&\\leq\\omega(x,y)\\qquad\\forall x,y\\in\\mathbb{S},\\label{eq-maj1}\\\\\n\\sigma(x)&=\\omega(x,x)\\qquad\\forall x\\in\\mathbb{S}.\\label{eq-maj2}\n\\end{align}\\] The function \\(\\omega\\) is called a majorization scheme for \\(\\sigma\\) on \\(S\\). A majorization scheme is strict if \\(\\sigma(x)&lt;\\omega(x,y)\\) for all \\(x,y\\in S\\) withj \\(x\\not=y\\).\nDefine \\[\nx^{(k+1)}\\in\\mathop{\\text{argmin}}_{x\\in\\mathbb{S}}\\omega(x,x^{(k)}),\n\\tag{5.1}\\] assuming that \\(\\omega\\) attains its (not necessarily unique) minimum over \\(x\\in\\mathbb{S}\\) for each \\(y\\). If \\(x^{(k)}\\in\\mathop{\\text{argmin}}_{x\\in\\mathbb{S}}\\omega(x,x^{(k)})\\) then we stop.\nBy majorization property \\(\\eqref{eq-maj1}\\) \\(\\sigma(x^{(k+1)})\\leq\\omega(x^{(k+1)},x^{(k)})\\). Because we did not stop update rule Equation 5.1 implies \\(\\omega(x^{(k+1)},x^{(k)})&lt;\\omega(x^{(k)},x^{(k)})\\). and finally by majorization property \\(\\eqref{eq-maj2}\\) \\(\\omega(x^{(k)},x^{(k)})=\\sigma(x^{(k)})\\).\nIf the minimum in Equation 5.1 is attained for a unique \\(x\\) then \\(\\omega(x^{(k+1)},x^{(k)})&lt;\\omega(x^{(k)},x^{(k)})\\). If the majorization scheme is strict then \\(\\sigma(x^{(k+1)})&lt;\\omega(x^{(k+1)},x^{(k)})\\). Under either of these two additional conditions \\(\\sigma(x^{(k+1)})&lt;\\sigma(x^{(k)})\\), which means that the majorization algorithm is a monotone descent algorithm, and if \\(\\sigma\\) is bounded below on \\(\\mathbb{S}\\) the sequence \\(\\sigma(x^{(k)})\\) converges.\nNote that we only use the order relation to prove convergence of the sequence of function values. To prove convergence of the \\(x^{(k)}\\) we need stronger compactness and continuity assumptions to apply the general theory of Zangwill (1969). For such a proof the argmin in update formula Equation 5.1 can be generalized to \\(x^{(k+1)}=\\phi(x^{(k)})\\), where \\(\\phi\\) maps \\(\\mathbb{S}\\) into \\(\\mathbb{S}\\) such that \\(\\omega(\\phi(x),x)\\leq\\sigma(x)\\) for all \\(x\\).\nWe give a small illustration in which we minimize \\(\\sigma\\) with \\(\\sigma(x)=\\sqrt{x}-\\log{x}\\) over \\(x&gt;0\\). Obviously we do not need majorization here, because solving \\(\\mathcal{D}\\sigma(x)=0\\) immediately gives \\(x=4\\) as the solution we are looking for.\nTo arrive at this solution using majorization we start with \\[\\begin{equation}\n\\sqrt{x}\\leq\\sqrt{y}+\\frac12\\frac{x-y}{\\sqrt{y}},\n(\\#eq:sqrtmaj)\n\\end{equation}\\] which is true because a differentiable concave function such as the square root is majorized by its tangent everywhere. Inequality @ref(eq:sqrtmaj) implies \\[\\begin{equation}\n\\sigma(x)\\leq\\eta(x,y):=\\sqrt{y}+\\frac12\\frac{x-y}{\\sqrt{y}}-\\log{x}.\n(\\#eq:examplemaj)\n\\end{equation}\\] Note that \\(\\eta(\\bullet,y)\\) is convex in its first argument for each \\(y\\). We have \\(\\mathcal{D}_1\\eta(x,y)=0\\) if and only if \\(x=2\\sqrt{y}\\) and thus the majorization algorithm is \\[\\begin{equation}\nx^{(k+1)}=2\\sqrt{x^{(k)}}\n(\\#eq:examplealg)\n\\end{equation}\\] The sequence \\(x^{(k)}\\) converges monotonically to the fixed point \\(x=2\\sqrt{x}\\), i.e. to \\(x=4\\). If \\(x^{(0)}&lt;4\\) the sequence is increasing, if \\(x^{(0)}&lt;4\\) it is decreasing. Also, by l’Hôpital, \\[\\begin{equation}\n\\lim_{x\\rightarrow 4}\\frac{2\\sqrt{x}-4}{x-4}=\\frac12\n(\\#eq:hopi1)\n\\end{equation}\\] and thus convergence to the minimizer is linear with asymptotic convergence rate \\(\\frac12\\). By another application of l’Hôpital \\[\\begin{equation}\n\\lim_{x\\rightarrow 4}\\frac{\\sigma(2\\sqrt{x)})-\\sigma(4)}{\\sigma(x)-\\sigma(4)}=\\frac14,\n(\\#eq:hopi2)\n\\end{equation}\\] and convergence to the minimum is linear with asymptotic convergence rate \\(\\frac14\\). Linear convergence to the minimizer is typical for majorization algorithms, as is the twice-as-fast linear convergence to the minimum value.\nThis small example is also of interest, because we minimize a DC function, the difference of two convex functions. In our example the convex functions are minus the square root and minus the logarithm. Algorithms for minimizing DC functions define other important subclasses of MM algorithms, the DC Algorithm of Tao Pham Dinh (see Le Thi and Tao (2018) for a recent overview), the Concave-Convex Procedure of Yuille and Rangarajan (2003), and the Half-Quadratic Method of Donald Geman (see Niikolova and Ng (2005) for a recent overview). For each of these methods there is a huge literature, with surprisingly little non-overlapping literatures. The first phase of the smacof algorithm, in which we improve the configuration for given disparities, is DC, concave-convex, and half-quadratic.\nIn the table below we show convergence of @ref(eq:examplealg) starting at \\(x=1.5\\). The first column show how far \\(x^{(k)}\\) deviates from the minimizer (i.e. from 4), the second shows how far\\(\\sigma(x^{(k)})\\) deviates from the minimum (i.e. from \\(2-\\log 4\\)). We clearly see the convergence rates \\(\\frac12\\) and \\(\\frac14\\) in action.\n\n\nitel   1 2.5000000000 0.2055741244 \nitel   2 1.5505102572 0.0554992066 \nitel   3 0.8698308399 0.0144357214 \nitel   4 0.4615431837 0.0036822877 \nitel   5 0.2378427379 0.0009299530 \nitel   6 0.1207437506 0.0002336744 \nitel   7 0.0608344795 0.0000585677 \nitel   8 0.0305337787 0.0000146606 \nitel   9 0.0152961358 0.0000036675 \nitel  10 0.0076553935 0.0000009172 \nitel  11 0.0038295299 0.0000002293 \nitel  12 0.0019152235 0.0000000573 \nitel  13 0.0009577264 0.0000000143 \nitel  14 0.0004788919 0.0000000036 \nitel  15 0.0002394531 0.0000000009 \n\n\nThe first three iterations are shown in the figure below. The vertical lines indicate the value of \\(x\\), function is in red, and the first three majorizations are in blue.\n\n\n\n\n\n\n\n\n\n\n\n\n\nBöhning, D., and B. G. Lindsay. 1988. “Monotonicity of Quadratic-approximation Algorithms.” Annals of the Institute of Statistical Mathematics 40 (4): 641–63.\n\n\nCarroll, J. D., and J. J. Chang. 1970. “Analysis of Individual Differences in Multidimensional scaling via an N-way generalization of \"Eckart-Young\" Decomposition.” Psychometrika 35: 283–319.\n\n\nCoombs, C. H. 1964. A Theory of Data. Wiley.\n\n\nDatorro, J. 2018. Convex Optimization and Euclidean Distance Geometry. Second Edition. Palo Alto, CA: Meebo Publishing. https://ccrma.stanford.edu/~dattorro/0976401304.pdf.\n\n\nDe Leeuw, J. 1977. “Applications of Convex Analysis to Multidimensional Scaling.” In Recent Developments in Statistics, edited by J. R. Barra, F. Brodeau, G. Romier, and B. Van Cutsem, 133–45. Amsterdam, The Netherlands: North Holland Publishing Company.\n\n\n———. 1994. “Block Relaxation Algorithms in Statistics.” In Information Systems and Data Analysis, edited by H. H. Bock, W. Lenski, and M. M. Richter, 308–24. Berlin: Springer Verlag. https://jansweb.netlify.app/publication/deleeuw-c-94-c/deleeuw-c-94-c.pdf.\n\n\n———. 2017a. “Shepard Non-metric Multidimensional Scaling.” 2017. https://jansweb.netlify.app/publication/deleeuw-e-17-e/deleeuw-e-17-e.pdf.\n\n\n———. 2017b. “Tweaking the SMACOF Engine.” 2017. https://jansweb.netlify.app/publication/deleeuw-e-17-p/deleeuw-e-17-p.pdf.\n\n\nDe Leeuw, J., and W. J. Heiser. 1977. “Convergence of Correction Matrix Algorithms for Multidimensional Scaling.” In Geometric Representations of Relational Data, edited by J. C. Lingoes, 735–53. Ann Arbor, Michigan: Mathesis Press.\n\n\n———. 1980. “Multidimensional Scaling with Restrictions on the Configuration.” In Multivariate Analysis, Volume V, edited by P. R. Krishnaiah, 501–22. Amsterdam, The Netherlands: North Holland Publishing Company.\n\n\n———. 1982. “Theory of Multidimensional Scaling.” In Handbook of Statistics, Volume II, edited by P. R. Krishnaiah and L. Kanal. Amsterdam, The Netherlands: North Holland Publishing Company.\n\n\nDe Leeuw, J., and P. Mair. 2009. “Multidimensional Scaling Using Majorization: SMACOF in R.” Journal of Statistical Software 31 (3): 1–30. https://www.jstatsoft.org/article/view/v031i03.\n\n\nDempster, A. P., N. M. Laird, and D. B. Rubin. 1977. “Maximum Likelihood for Incomplete Data via the EM Algorithm.” Journal of the Royal Statistical Society B39: 1–38.\n\n\nGroenen, P. J. F., R. Mathar, and W. J. Heiser. 1995. “The Majorization Approach to Multidimensional Scaling for Minkowski Distances.” Journal of Classification 12: 3–19.\n\n\nGroenen, P. J. F., and M. Van de Velden. 2016. “Multidimensional Scaling by Majorization: A Review.” Journal of Statistical Software 73 (8): 1–26. https://www.jstatsoft.org/index.php/jss/article/view/v073i08.\n\n\nGuttman, L. 1968. “A General Nonmetric Technique for Fitting the Smallest Coordinate Space for a Configuration of Points.” Psychometrika 33: 469–506.\n\n\nHarshman, R. A. 1970. “Foundations of the PARAFAC Procedure.” Working Papers in Phonetics 16. UCLA.\n\n\nHeiser, W. J., and J. De Leeuw. 1977. “How to Use SMACOF-I.” Department of Data Theory FSW/RUL.\n\n\nKruskal, J. B. 1964a. “Multidimensional Scaling by Optimizing Goodness of Fit to a Nonmetric Hypothesis.” Psychometrika 29: 1–27.\n\n\n———. 1964b. “Nonmetric Multidimensional Scaling: a Numerical Method.” Psychometrika 29: 115–29.\n\n\nLange, K. 2016. MM Optimization Algorithms. SIAM.\n\n\nLe Thi, H. A., and P. D. Tao. 2018. “DC Programming and DCA: Thirty Years of Developments.” Mathematical Programming, Series B.\n\n\nMathar, R., and R. Meyer. 1994. “Algorithms in Convex Analysis to Fit l_p -Distance Matrices.” Journal of Multivariate Analysis 51: 102–20.\n\n\nMeulman, J. J., and W. J. Heiser. 2012. IBM SPSS Categories 21. IBM Corporation.\n\n\nNiikolova, M., and M. Ng. 2005. “Analysis of Half-Quadratic Minimization Methods for Signal and Image Recovery.” SIAM Journal Scientific Computing 27 (3): 937–66.\n\n\nRamsay, J. O. 1977. “Maximum Likelihood Estimation in Multidimensional Scaling.” Psychometrika 42: 241–66.\n\n\nSchoenberg, I. J. 1935. “Remarks to Maurice Frechet’s article: Sur la Definition Axiomatique d’une Classe d’Espaces Vectoriels Distancies Applicables Vectoriellement sur l’Espace de Hllbert.” Annals of Mathematics 36: 724–32.\n\n\nShepard, R. N. 1962a. “The Analysis of Proximities: Multidimensional Scaling with an Unknown Distance Function. I.” Psychometrika 27: 125–40.\n\n\n———. 1962b. “The Analysis of Proximities: Multidimensional Scaling with an Unknown Distance Function. II.” Psychometrika 27: 219–46.\n\n\nTakane, Y., F. W. Young, and J. De Leeuw. 1977. “Nonmetric Individual Differences in Multidimensional Scaling: An Alternating Least Squares Method with Optimal Scaling Features.” Psychometrika 42: 7–67.\n\n\nTorgerson, W. S. 1958. Theory and Methods of Scaling. New York: Wiley.\n\n\nTorgerson, W. S. 1952. “Multidimensional Scaling: I. Theory and Method.” Psychometrika 17 (4): 401–19.\n\n\n———. 1965. “Multidimensional Scaling of Similarity.” Psychometrika 30 (4): 379–93.\n\n\nVosz, H., and U. Eckhardt. 1980. “Linear Convergence of Generalized Weiszfeld’s Method.” Computing 25: 243–51.\n\n\nYoung, G., and A. S. Householder. 1938. “Discussion of a Set of Points in Terms of Their Mutual Distances.” Psychometrika 3 (19-22).\n\n\nYuille, A. L., and A. Rangarajan. 2003. “The Concave-Convex Procedure.” Neural Computation 15: 915–36.\n\n\nZangwill, W. I. 1969. Nonlinear Programming: a Unified Approach. Englewood-Cliffs, N.J.: Prentice-Hall.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "basics.html",
    "href": "basics.html",
    "title": "2  smacof Basics",
    "section": "",
    "text": "2.1 Loss Function\nIn the pioneering papers Kruskal (1964a) and Kruskal (1964b) the MDS problem was formulated for the first time as minimization of an explicit loss function or badness-of-fit function, which measures the quality of the approximation of the dissimilarities by the distances. To be historically accurate, we should mention that the non-metric MDS technique proposed by Shepard (1962a) and Shepard (1962b) can be reformulated as minimization of an explicit loss function (see, for example, De Leeuw (2017)). And the classical Young-Householder-Torgerson MDS technique (Torgerson (1952)) for metric MDS can be reformulated as minimizing an explicit least squares loss function (De Leeuw and Heiser (1982)) as well. But neither of these two predecessors was formulated originally as an explicit minimization problem for a specific loss function",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>smacof Basics</span>"
    ]
  },
  {
    "objectID": "basics.html#metric-mds",
    "href": "basics.html#metric-mds",
    "title": "2  smacof Basics",
    "section": "2.2 Metric MDS",
    "text": "2.2 Metric MDS\nThe loss function in least squares metric Euclidean MDS is called raw stress and is defined as \\[\\begin{equation}\n\\sigma_R(X):=\\frac12\\mathop{\\sum\\sum}_{1\\leq j&lt;i\\leq n}w_{ij}(\\delta_{ij}-d_{ij}(X))^2.\n(\\#eq:stressdef)\n\\end{equation}\\] The subscript R in \\(\\sigma_R\\) stands for “raw”, because we will discuss other least squares loss functions for which we will also use the symbol \\(\\sigma\\), but with other subscripts.\nIn definition @ref(eq:stressdef) the \\(w_{ij}\\) are known non-negative weights, the \\(\\delta_{ij}\\) are the known non-negative dissimilarities between objects \\(o_i\\) and \\(o_j\\), and the \\(d_{ij}(X)\\) are the distances between the corresponding points \\(x_i\\) and \\(x_j\\). The summation is over all pairs \\((i,j)\\) with \\(w_{ij}&gt;0\\). From now on we use “metric MDS” to mean the minimization of \\(\\sigma_R\\).\nThe \\(n\\times p\\) matrix \\(X\\), which has the coordinates \\(x_i\\) of the \\(n\\) points as its rows, is called the configuration, where \\(p\\) is the dimension of the Euclidean space in which we make the map. The metric MDS problem (of dimension \\(p\\), for given \\(W\\) and \\(\\Delta\\)) is the minimization of @ref(eq:stressdef) over the \\(n\\times p\\) configurations \\(X\\).\nThe weights \\(w_{ij}\\) can be used to quantify information about the precision or importance of the corresponding dissimilarities. Some of the weights may be zero, which can be used to code missing data. If all weights are positive we have complete data. If we have complete data, and all weights are equal to one, we have unweighted metric MDS. The pioneering papers by Shepard, Kruskal, and Guttman only consider the unweighted case. Weights were only introduced in MDS in De Leeuw (1977).\nWe assume throughout that the weights are irreducible (De Leeuw (1977)). This means there is no partitioning of the index set \\(I_n:=\\{1,2,\\cdots,n\\}\\) into subsets for which all between-subset weights are zero. A reducible metric MDS problems decomposes into a number of smaller independent metric MDS problems, so the irreducibility assumption causes no real loss of generality.\nThe fact that the summation in @ref(eq:stressdef) is over all \\(j&lt;i\\) indicates that the diagonal elements of \\(\\Delta\\) are not used (they are assumed to be zero) and the elements above the diagonal are not used either (they are assumed to be equal to the corresponding elements below the diagonal). The somewhat mysterious factor \\(\\frac12\\) in definition @ref(eq:stressdef) is there because it simplifies some of the formulas in later sections of this paper.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>smacof Basics</span>"
    ]
  },
  {
    "objectID": "basics.html#non-linear-mds",
    "href": "basics.html#non-linear-mds",
    "title": "2  smacof Basics",
    "section": "2.3 Non-linear MDS",
    "text": "2.3 Non-linear MDS\nKruskal was not really interested in metric MDS and the “raw” loss function @ref(eq:stressdef). His papers are really about non-metric MDS, by which we mean least squares non-metric Euclidean MDS. Non-metric MDS differs from metric MDS because we have incomplete information about the dissimilarities. As we have seen, that if some dissimilarities are missing metric MDS can handle this by using zero weights. In some situations, however, we only know the rank order of the non-missing dissimilarities. We do not know, or we refuse to use, their actual numeric values. Or, to put it differently, even if we have numerical dissimilarities we are looking for a transformation of the non-missing dissimilarities, where the transformation is chosen from a set of admissible transformations (for instance from all linear or monotone transformations). If the dissimilarities are non-numerical, for example rank orders or partitionings, we choose from the set of admissible quantifications.\nIn non-metric MDS raw stress becomes \\[\\begin{equation}\n\\sigma_R(X,\\Delta):=\\frac12\\sum w_{ij}(\\delta_{ij}-d_{ij}(X))^2,\n(\\#eq:rawstressdef)\n\\end{equation}\\] where \\(\\Delta\\) varies over the quantified or transformed dissimilarities. In MDS parlance they are also called pseudo-distances or disparities. Loss function @ref(eq:rawstressdef) must be minimized over both configurations and disparities, with the condition that the disparities \\(\\Delta\\) are an admissible transformation or quantification of the data. In Kruskal’s non-metric MDS this means requiring monotonicity. In this paper we will consider various other choices for the set of admissible transformations. We will use the symbol \\(\\mathfrak{D}\\) for the set of admissible transformations\nThe most familiar examples of \\(\\mathfrak{D}\\) (linear, polynomial, splines, monotone) define convex cones with apex at the origin. This means that if \\(\\Delta\\in\\mathfrak{D}\\) then so is \\(\\lambda\\Delta\\) for all \\(\\lambda\\geq 0\\). But consequently minimizing @ref(eq:rawstressdef) over all \\(\\Delta\\in\\mathfrak{D}\\) and over all configurations has the trivial solution \\(\\Delta=0\\) and \\(X=0\\), corresponding with the global minimum \\(\\sigma(X,\\Delta)=0\\). We need additional constraints to rule out this trivial solution, and in non-metric MDS this is done by choosing a normalization that keeps the solution away from zero.\nKruskal’s original solution is to define normalized stress as \\[\\begin{equation}\n\\sigma(X,\\Delta):=\\frac{\\sum w_{ij}(\\delta_{ij}-d_{ij}(X))^2}{\\sum w_{ij}d_{ij}^2(X)}.\n(\\#eq:nstressdef)\n\\end{equation}\\] To be precise, in Kruskal’s formulation there are no weights, and he actually takes the square root of @ref(eq:nstressdef) to define Kruskal’s stress. The non-metric Euclidean MDS problem now is to minimize loss function @ref(eq:nstressdef) over all \\(n\\times p\\) configurations \\(X\\) and all admissible disparities \\(\\Delta\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>smacof Basics</span>"
    ]
  },
  {
    "objectID": "basics.html#non-metric-mds",
    "href": "basics.html#non-metric-mds",
    "title": "2  smacof Basics",
    "section": "2.4 Non-metric MDS",
    "text": "2.4 Non-metric MDS",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>smacof Basics</span>"
    ]
  },
  {
    "objectID": "basics.html#normalization",
    "href": "basics.html#normalization",
    "title": "2  smacof Basics",
    "section": "2.5 Normalization",
    "text": "2.5 Normalization\nEquation @ref(eq:nstressdef) is only one way to normalize raw stress. Some obvious alternatives are discussed in detail in Kruskal and Carroll (1969) and De Leeuw (1975). In the terminology of De Leeuw (1975) there are explicit and implicit normalizations.\nIn implicit normalization we minimize either \\[\\begin{equation}\n\\sigma(X,\\hat D):=\\frac{\\sum  w_{ij}(\\hat d_{ij} -d_{ij}(X))^2}{\\sum   w_{ij}^{\\ }\\hat d_{ij}^2}\n(\\#eq:implicit1)\n\\end{equation}\\] or \\[\\begin{equation}\n\\sigma(X,\\hat D):=\\frac{\\sum   w_{ij}(\\hat d_{ij}-d_{ij}(X))^2}{\\sum   w_{ij}^{\\ }d_{ij}^2(X) }\n(\\#eq:implicit2)\n\\end{equation}\\] over \\(X\\) and \\(\\Delta\\in\\mathfrak{D}\\).\nAs we have seen, Kruskal (1964a) chooses definition @ref(eq:implicit2) and calls the explicitly normalized loss function normalized stress. Note that we overload the symbol \\(\\sigma\\) to denote any one of the least squares loss functions. It will always be clear from the text which \\(\\sigma\\) we are talking about.\nIn explicit normalization we minimize the raw stress \\(\\sigma_R(X,\\hat D)\\) from @ref(eq:rawstressdef), but we add the explicit constraint \\[\\begin{equation}\n\\sum   w_{ij}^{\\ }d_{ij}^2(X)=1,\n(\\#eq:explicit1)\n\\end{equation}\\] or the constraint \\[\\begin{equation}\n\\sum   w_{ij}^{\\ }\\hat d_{ij}^2=1.\n(\\#eq:explicit2)\n\\end{equation}\\] Kruskal and Carroll (1969) and De Leeuw (2019) show that these four normalizations all lead to essentially the same solution for \\(X\\) and \\(\\hat D\\), up to scale factors dictated by the choice of the particular normalization. It is also possible to normalize both \\(X\\) and \\(\\hat D\\), either explicitly or implicitly, and again this will give the same solutions, suitably normalized. These invariance results assume the admissible transformations form a closed cone with apex at the origin, i.e. if \\(\\hat D\\) is admissible and \\(\\lambda\\geq 0\\) then \\(\\lambda\\hat D\\) is admissible as well. The matrices of Euclidean distances \\(D(X)\\) form a similar closed cone as well. The non-metric MDS problem is to find an element of the \\(\\hat D\\) cone \\(\\mathcal{D}\\) and an element of the \\(D(X)\\) cone where the angle between the two is a small as possible.\nIn the R version of smacof (De Leeuw and Mair (2009), Mair, Groenen, and De Leeuw (2022)) we use explicit normalization @ref(eq:explicit2). This is supported by the result, also due to De Leeuw (1975), that projection on the intersection of the cone of disparities and the sphere defined by @ref(eq:explicit2) is equivalent to first projecting on the cone and then normalizing the projection (see also Bauschke, Bui, and Wang (2018)).\nIn the version of non-metric MDS discussed in this manual we need more flexibility. For algorithmic reasons that may become clear later on, we will go with the original @ref(eq:nstressdef), i.e. with the implicitly normalized Kruskal’s stress. For the final results the choice between normalizations should not make a difference, but the iterative computations will be different for the different choices.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>smacof Basics</span>"
    ]
  },
  {
    "objectID": "basics.html#some-thoughts-on-als",
    "href": "basics.html#some-thoughts-on-als",
    "title": "2  smacof Basics",
    "section": "2.6 Some thoughts on ALS",
    "text": "2.6 Some thoughts on ALS\nThe formulation in equations @ref(eq:gmdsdef1) and @ref(eq:gmdsdef2) neatly separates the metric MDS part @ref(eq:gmdsdef1) and the transformation/quantification part @ref(eq:gmdsdef2). This second part is also often called the optimal scaling part.\nEquations @ref(eq:gmdsdef1) and @ref(eq:gmdsdef2) corresponds with the way most iterative non-linear and non-metric MDS techniques are implemented. The algorithms use Alternating Least Squares (ALS). There have been quite a few ALS algorithms avant-la-lettre, but as far as I know both the name and ALS as a general approach to algorithm construction were first introduced in De Leeuw (1968), and then widely disseminated in a series of papers by De Leeuw, Young, and Takane in the 1970’s (work summarized in Young, De Leeuw, and Takane (1980) and Young (1981)).\nIn the ALS implementation of MDS two sub-algorithms are used in each iteration: one to improve the fit of the distances to the current disparities \\(\\Delta\\) and one to improve the fit of the disparities to the current distances. The two sub-algorithms define one major iteration of the MDS technique. In formulas (using superscript \\((k)\\) for major iteration number) we start with \\((X^{(0)},\\Delta^{(0)})\\) and then alternate the mimization problems where \\(\\ni\\) is short for “such that”. In MDS it is more realistic not to minimize loss in the sub-steps but merely to decrease it. Minimization in one or both of the two subproblems may itself require an infinite iterative method, which we have to truncate anyway. Thus \n\n2.6.1 The Single-Phase approach\nIn Kruskal (1964a) defines \\[\\begin{equation}\n\\sigma(X):=\\min_{\\hat D\\in\\mathfrak{D}}\\ \\sigma(\\hat D,X)=\\sigma(X,\\hat D(X)),\n(\\#eq:project)\n\\end{equation}\\] where \\(\\sigma(\\hat D,X)\\) is defined by @ref(eq:implicit2). The minimum in @ref(eq:project) is over admissible transformations. In definition @ref(eq:project) \\[\\begin{equation}\n\\hat D(X):=\\mathop{\\text{argmin}}_{\\hat D\\in\\mathfrak{D}}\\sigma(X, \\hat D).\n(\\#eq:optscal)\n\\end{equation}\\] Normalized stress defined by @ref(eq:project) is now a function of \\(X\\) only. Under some conditions, which are true in Kruskal’s definition of non-metric MDS, there is a simple relation between the partials of @ref(eq:implicit2) and those of @ref(eq:project). \\[\\begin{equation}\n\\mathcal{D}\\sigma(X)=\\mathcal{D}_1\\sigma(X,\\hat D(X)),\n(\\#eq:partials)\n\\end{equation}\\] where \\(\\mathcal{D}\\sigma(X)\\) are the derivatives of \\(\\sigma\\) from @ref(eq:project) and \\(\\mathcal{D}_1\\sigma(X,\\hat D(X))\\) are the partial derivatives of \\(\\sigma\\) from @ref(eq:implicit2) with respect to \\(X\\). Thus the partials of \\(\\sigma\\) from @ref(eq:project) can be computed by evaluating the partials of \\(\\sigma\\) from @ref(eq:implicit2) with respect to \\(X\\) at \\((X,\\hat D(X))\\). This has created much confusion in the past. The non-metric MDS problem in Kruskal’s original formulation is now to minimize \\(\\sigma\\) from @ref(eq:project), which is a function of \\(X\\) alone.\nGuttman (1968) calls this the single-phase approach. A variation of Kruskal’s single-phase approach defines \\[\\begin{equation}\n\\sigma(X)=\\sum w_{ij}(d_{ij}^\\#(X)-d_{ij}(X))^2,\n(\\#eq:rankimage)\n\\end{equation}\\] where the \\(d_{ij}^\\#(X)\\) are Guttman’s rank images, i.e. the permutation of the \\(d_{ij}(X)\\) that makes them monotone with the \\(\\delta_{ij}\\) (Guttman (1968)). Or, alternatively, define \\[\\begin{equation}\n\\sigma(X):=\\sum   w_{ij}(d_{ij}^\\%(X)-d_{ij}(X))^2,\n(\\#eq:shepard)\n\\end{equation}\\] where the \\(\\hat d_{ij}^\\%(X)\\) are Shepard’s rank images, i.e. the permutation of the \\(\\delta_{ij}\\) that makes them monotone with the \\(d_{ij}(X)\\) (Shepard (1962a), Shepard (1962b), De Leeuw (2017)).\nMinimizing the Shepard or Guttman single-phase loss functions is computationally more complicated than Kruskal’s monotone regression approach, mostly because the rank-image transformations are not differentiable, and there is no analog of @ref(eq:partials) and of the equivalence of the different implicit and explicit normalizations.\n\n\n2.6.2 The Two-Phase Approach\nThe two-phase approach or alternating least squares (ALS) approach alternates minimization of \\(\\sigma(\\hat D,X)\\) over \\(X\\) for our current best estimate of \\(\\hat D\\) with minimization of \\(\\sigma(\\hat D,X)\\) over \\(\\Delta\\in\\mathfrak{D}\\) for our current best value of \\(X\\). Thus an update from iteration \\(k\\) to iteration \\(k+1\\) looks like This ALS approach to MDS was in the air since the early (unsuccessful) attempts around 1968 of Young and De Leeuw to combine Torgerson’s classic metric MDS method with Kruskal’s monotone regression transformation. All previous implementations of non-metric smacof use the two-phase approach, and we will do the same in this paper.\nAs formulated, however, there are some problems with the ALS algorithm. Step @ref(eq:step1) is easy to carry out, using monotone regression. Step @ref(eq:step2) means solving a metric scaling problem, which is an iterative proces that requires an infinite number of iterations. Thus, in the usual implementations, step @ref(eq:step1) is combined with one of more iterations of a convergent iterative procedure for metric MDS, such as smacof. If we take only one of these inner iterations the algorithm becomes indistinguishable from Kruskal’s single-phase method. This has also created much confusion in the past.\nIn the usual implementations of the ALS approach we solve the first subproblem @ref(eq:step1) exactly, while we take only a single step towards the solution for given \\(\\hat D\\) in the second phase @ref(eq:step2). If we have an infinite iterative procedure to compute the optimal \\(\\hat D\\in\\mathfrak{D}\\) for given \\(X\\), then a more balanced approach would be to take several inner iterations in the first phase and several inner iterations in the second phase. How many of each, nobody knows. In our current implementation of smacof we take several inner iteration steps in the first phase and a single inner iteration step in the second phase.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>smacof Basics</span>"
    ]
  },
  {
    "objectID": "basics.html#smacof-notation-and-terminology",
    "href": "basics.html#smacof-notation-and-terminology",
    "title": "2  smacof Basics",
    "section": "2.7 Smacof Notation and Terminology",
    "text": "2.7 Smacof Notation and Terminology\nWe discuss some the MDS notation used in smacof, which was first introduced in De Leeuw (1977) and De Leeuw and Heiser (1977). More detailed De Leeuw and Heiser (1980), De Leeuw (1988), Borg and Groenen (2005), Groenen and Van de Velden (2016)\nThis notation is useful for the second phase of the ALS algorithm, in which solve the metric MDS problem of we minimizing unnormalized \\(\\sigma(X,\\hat D)\\) over \\(X\\) for fixed \\(\\hat D\\). We will discuss the first ALS phase later in the paper.\nStart with the unit vectors \\(e_i\\) of length \\(n\\). They have a non-zero element equal to one in position \\(i\\), all other elements are zero. Think of the \\(e_i\\) as the columns of the identity matrix.\nUsing the \\(e_i\\) we define for all \\(i\\not= j\\) the matrices \\[\\begin{equation}\nA_{ij}:=(e_i-e_j)(e_i-e_j)'.\n\\end{equation}\\] The \\(A_{ij}\\) are of order \\(n\\), symmetric, doubly-centered, and of rank one. They have four non-zero elements. Elements \\((i,i)\\) and \\((j,j)\\) are equal to \\(+1\\), elements \\((i,j)\\) and \\((j,i)\\) are \\(-1\\).\nThe importance of \\(A_{ij}\\) in MDS comes from the equation \\[\\begin{equation}\nd_{ij}^2(X)=\\text{tr}\\ X'A_{ij}X.\n(\\#eq:dfroma)\n\\end{equation}\\] In addition we use the fact that the \\(A_{ij}\\) form a basis for the \\(binom{n}{2}\\)-dimensional linear space of all doubly-centered symmetric matrices.\nExpanding the square in the definition of stress gives \\[\\begin{equation}\n\\sigma(X)=\\frac12\\{\\sum   w_k\\delta_k^2-2\\ \\sum   w_k\\delta_kd_k(X)+\\sum   w_kd_k^2(X)\\}.\n(\\#eq:expand)\n\\end{equation}\\] It is convenient to have notation for the three separate components of stress from equation @ref(eq:expand). Define \\[\\begin{align}\n\\eta_{\\hat D}^2&=\\sum   w_{ij}\\hat d_{ij}^2,(\\#eq:condef)\\\\\n\\rho(X)&=\\sum   w_{ij}\\hat d_{ij}d_{ij}(X),(\\#eq:rhodef)\\\\\n\\eta^2(X)&=\\sum   w_{ij}d_{ij}(X)^2.(\\#eq:etadef)\n\\end{align}\\] which lead to \\[\\begin{equation}\n\\sigma(X)=\\frac12\\left\\{\\eta_{\\hat D}^2-2\\rho(X)+\\eta^2(X)\\right\\}.\n(\\#eq:stressshort)\n\\end{equation}\\] We also need \\[\\begin{equation}\n\\lambda(X)=\\frac{\\rho(X)}{\\eta(X)}.\n(\\#eq:lambdadef)\n\\end{equation}\\]\nUsing the \\(A_{ij}\\) makes it possible to give matrix expressions for \\(\\rho\\) and \\(\\eta^2\\). First \\[\\begin{equation}\n\\eta^2(X)=\\text{tr}\\ X'VX,\n(\\#eq:etamat)\n\\end{equation}\\] with \\[\\begin{equation}\nV:=\\sum   w_{ij}A_{ij}.\n(\\#eq:vdef)\n\\end{equation}\\] In the same way \\[\\begin{equation}\n\\rho(X)=\\text{tr}\\ X'B(X)X,\n(\\#eq:rhomat)\n\\end{equation}\\] with \\[\\begin{equation}\nB(X):=\\sum   w_{ij}r_{ij}(X)A_{ij},\n(\\#eq:bdef)\n\\end{equation}\\] with \\[\\begin{equation}\nr_{ij}(X):=\\begin{cases}\\frac{\\delta_{ij}}{d_{ij}(X)}&\\text{ if }d_{ij}(X)&gt;0,\\\\\n0&\\text{ if }d_{ij}(X)=0.\n\\end{cases}\n\\end{equation}\\] Note that \\(B\\) is a function from the set of \\(n\\times p\\) configurations into the set of symmetric doubly-dentered matrices of order \\(n\\). All matrices of the form \\(\\sum x_{ij}A_{ij}\\), where summation is over all pairs \\((i,j)\\) with \\(j&lt;i\\), are symmetric and doubly-centered. They have \\(-x_{ij}\\) as off-diagonal elements while the diagonal elements \\((i,i)\\) are \\(\\sum_{j=1}^nx_{ij}\\).\nBecause \\(B(X)\\) and \\(V\\) are non-negative linear combinations of the \\(A_{ij}\\) they are both positive semi-definite. Because \\(W\\) is assumed to be irreducible the matrix \\(V\\) has rank \\(n-1\\), with only vectors proportional to the vector \\(e\\) with all elements equal to one in its null-space (De Leeuw (1977)).\nSummarizing the results so far we have \\[\\begin{equation}\n\\sigma(X)=\\frac12\\{\\eta_{\\hat D}^2-\\text{tr}\\ X'B(X)X+\\text{tr}\\ X'VX\\}.\n(\\#eq:sigmat)\n\\end{equation}\\]\nNext we define the Guttman transform of a configuration \\(X\\), for given \\(W\\) and \\(\\Delta\\), as \\[\\begin{equation}\nG(X)=V^+B(X)X,\n(\\#eq:gudef)\n\\end{equation}\\] with \\(V^+\\) the Moore-Penrose inverse of \\(V\\). In our computations we use \\[\\begin{equation}\nV^+=(V+\\frac{1}{n}ee')^{-1}-\\frac{1}{n}ee'\n\\end{equation}\\] Also note that in the unweighted case with complete data \\(V=nJ\\), where \\(J\\) is the centering matrix \\(I-\\frac{1}{n}ee'\\), and thus \\(V^+=\\frac{1}{n}J\\). The Guttman transform is then simply \\(G(X)=n^{-1}B(X)X\\).\nWe have defined stress as a function on \\(\\mathbb{R}^{n\\times p}\\), the space of \\(n\\times p\\) matrices. For some purposes it is convenient to use an alternative, but equivalent, definition of stress on \\(\\mathbb{R}^{np}\\), the space of all vectors of length \\(np\\). Define \\(\\mathfrak{A}_{ij}\\) as the direct sum of \\(p\\) copies of \\(A_{ij}\\). Thus \\(\\mathfrak{A}_{ij}\\) is block-diagonal of order \\(np\\). Now redefine stress as \\[\n\\sigma(x):=\\frac12\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}w_{ij}(\\delta_{ij}-x'\\mathfrak{A}_{ij}x)^2.\n\\] \\[\n\\sigma(x)=1-x'\\mathfrak{B}(x)x+\\frac12 x'\\mathfrak{V}x\n\\] where \\(\\mathfrak{B}(x)\\) and \\(\\mathfrak{V}\\) are direct sums of \\(p\\) copies of our previous \\(B(X)\\) and \\(V\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>smacof Basics</span>"
    ]
  },
  {
    "objectID": "basics.html#stress-formula-two",
    "href": "basics.html#stress-formula-two",
    "title": "2  smacof Basics",
    "section": "4.1 Stress formula two",
    "text": "4.1 Stress formula two\nMinorization result",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>smacof Basics</span>"
    ]
  },
  {
    "objectID": "basics.html#interval",
    "href": "basics.html#interval",
    "title": "2  smacof Basics",
    "section": "5.1 Interval",
    "text": "5.1 Interval\n\\((i, j, \\text{lower bound}, \\text{upper bound}, \\text{weight})\\)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>smacof Basics</span>"
    ]
  },
  {
    "objectID": "basics.html#ordinal",
    "href": "basics.html#ordinal",
    "title": "2  smacof Basics",
    "section": "5.2 Ordinal",
    "text": "5.2 Ordinal\n\\((i, j, \\text{tied}, text{weight})\\)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>smacof Basics</span>"
    ]
  },
  {
    "objectID": "basics.html#paired-comparisons",
    "href": "basics.html#paired-comparisons",
    "title": "2  smacof Basics",
    "section": "5.3 Paired Comparisons",
    "text": "5.3 Paired Comparisons\n\\((i, j, k, l, \\text{tied}, \\text{weight})\\)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>smacof Basics</span>"
    ]
  },
  {
    "objectID": "basics.html#complete-triads",
    "href": "basics.html#complete-triads",
    "title": "2  smacof Basics",
    "section": "5.4 Complete triads",
    "text": "5.4 Complete triads\n\\[(i, j, k, \\text{smallest}, \\text{largest}, \\text{weight})\\] ## Indicator\n\\[(i, l, \\text{weight})\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>smacof Basics</span>"
    ]
  },
  {
    "objectID": "metric.html",
    "href": "metric.html",
    "title": "3  Metric smacof",
    "section": "",
    "text": "3.1 Program\nIn this part of the manual we discuss metric MDS, and the program smacofME. Metric MDS is the core of all smacof programs, because they all have the majorization algorithm based on the Guttman transform in common.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Metric smacof</span>"
    ]
  },
  {
    "objectID": "metric.html#program",
    "href": "metric.html#program",
    "title": "3  Metric smacof",
    "section": "",
    "text": "3.1.1 Parameters\n\nsmacofME &lt;- function(thedata,\n                     ndim = 2,\n                     xold = NULL,\n                     labels = NULL,\n                     width = 15,\n                     precision = 10,\n                     itmax = 1000,\n                     eps = 1e-10,\n                     verbose = TRUE,\n                     jitmax = 20,\n                     jeps = 1e-10,\n                     jverbose = FALSE,\n                     kitmax = 20,\n                     keps = 1e-10,\n                     kverbose = FALSE,\n                     init = 1)\n\nParameter ndim is the number of dimensions, and init tells if an initial configuration is read from a file (init = 1), is computed using classical scaling (init = 2), or is a random configuration (init = 3). Parameters itmax, epsi, and verbose control the iterations. The maximum number of iterations is itmax, the iterations stop if the decrease of stress in an iteration is less than 1E-epsi, and if verbose is one intermediate iteration results are written to stdout. These intermediate iteration results are formatted with the R function formatC(), using width for the width argument and precision for the digits argument.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Metric smacof</span>"
    ]
  },
  {
    "objectID": "metric.html#algorithm",
    "href": "metric.html#algorithm",
    "title": "3  Metric smacof",
    "section": "3.2 Algorithm",
    "text": "3.2 Algorithm",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Metric smacof</span>"
    ]
  },
  {
    "objectID": "metric.html#output",
    "href": "metric.html#output",
    "title": "3  Metric smacof",
    "section": "3.3 Output",
    "text": "3.3 Output\n\n  h &lt;- list(\n    delta = delta,\n    nobj = nobj,\n    ndim = ndim,\n    snew = snew,\n    itel = itel,\n    xnew = xnew,\n    dmat = dmat,\n    labels = labels\n  )",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Metric smacof</span>"
    ]
  },
  {
    "objectID": "metric.html#plots",
    "href": "metric.html#plots",
    "title": "3  Metric smacof",
    "section": "3.4 Plots",
    "text": "3.4 Plots",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Metric smacof</span>"
    ]
  },
  {
    "objectID": "metric.html#degruijter_67",
    "href": "metric.html#degruijter_67",
    "title": "3  Metric smacof",
    "section": "4.1 De Gruijter (1967)",
    "text": "4.1 De Gruijter (1967)\n\n\n\n\nDe Gruijter, D. N. M. 1967. “The Cognitive Structure of Dutch Political Parties in 1966.” Report E019-67. Psychological Institute, University of Leiden.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Metric smacof</span>"
    ]
  },
  {
    "objectID": "notation.html",
    "href": "notation.html",
    "title": "Notations",
    "section": "",
    "text": "Conventions\nI number and label all displayed equations. Equations are displayed, instead of inlined, if and only if one of the following is true.\nAll code chunks in the text are named. Theorems, lemmas, chapters, sections, subsections, and so on are also named and numbered. I use the serial comma.\nThe dilemma of whether to use “we” or “I” throughout the book is solved in the usual way. If I feel that a result is the work of a group (me, my co-workers, and the giants on whose shoulders we stand) then I use “we”. If it’s an individual decision, or something personal, then I use “I”. The default is “we”, as it always should be in scientific writing.\nMost of the individual chapters also have some of the necessary mathematical background material, both notation and results, sometimes with specific elaborations that seem useful for the book. Sometimes this background material is quite extensive. Examples are splines, majorization, unweighting, monotone regression, and the basic fixed point theorems we need for convergence analysis of our algorithms.",
    "crumbs": [
      "Notations"
    ]
  },
  {
    "objectID": "notation.html#conventions",
    "href": "notation.html#conventions",
    "title": "Notations",
    "section": "",
    "text": "They are important.\nThey are referred to elsewhere in the text.\nNot displaying them messes up the line spacing.",
    "crumbs": [
      "Notations"
    ]
  },
  {
    "objectID": "notation.html#notations-and-reserved-symbols",
    "href": "notation.html#notations-and-reserved-symbols",
    "title": "Notations",
    "section": "Notations and Reserved Symbols",
    "text": "Notations and Reserved Symbols\n\nSpaces\n\n\\(\\mathbb{R}^n\\) is the space of all real vectors, i.e. all \\(n\\)-element tuples of real numbers. Typical elements of \\(\\mathbb{R}^n\\) are \\(x,y,z\\). The element of \\(x\\) in position \\(i\\) is \\(x_i\\). Defining a vector by its elements is done with \\(x=\\{x_i\\}\\).\n\\(\\mathbb{R}^n\\) is equipped with the inner product \\(\\langle x,y\\rangle=x'y=\\sum_{i=1}^nx_iy_i\\) and the norm \\(\\smash{\\|x\\|=\\sqrt{x'x}}\\).\nThe canonical basis for \\(\\mathbb{R}^n\\) is the \\(n-\\)tuple \\((e_1,cdots,e_n)\\), where \\(e_i\\) has element \\(i\\) equal to \\(+1\\) and all other elements equal to zero. Thus \\(\\|e_i\\|=1\\) and \\(\\langle e_i,e_j\\rangle=\\delta^{ij}\\), with \\(\\delta^{ij}\\) the Kronecker delta (equal to one if \\(i=j\\) and zero otherwise). Note that \\(x_i=\\langle e_i,x\\rangle\\).\n\\(\\mathbb{R}\\) is the real line and \\(\\mathbb{R}_+\\) is the half line of non-negative numbers. The postive reals are \\(\\mathbb{R}_{++}\\).\n\\(\\mathbb{R}^{n\\times m}\\) is the space of all \\(n\\times m\\) real matrices. Typical elements of \\(\\mathbb{R}^{n\\times m}\\) are \\(A,B,C\\). The element of \\(A\\) in row \\(i\\) and column \\(j\\) is \\(a_{ij}\\). Defining a matrix by its elements is done with \\(A=\\{a_{ij}\\}\\).\n\\(\\mathbb{R}^{n\\times m}\\) is equipped with the inner product \\(\\langle A,B\\rangle=\\text{tr} A'B=\\sum_{i=1}^n\\sum_{j=1}^ma_{ij}b_{ij}\\) and the norm \\(\\|A\\|=\\sqrt{\\text{tr}\\ A'A}\\).\nThe canonical basis for \\(\\mathbb{R}^{n\\times m}\\) is the \\(nm-\\)tuple \\((E_{11},cdots,E_{nm})\\), where \\(E_{ij}\\) has element \\((i,j)\\) equal to \\(+1\\) and all other elements equal to zero. Thus \\(\\|E_{ij}\\|=1\\) and \\(\\langle E_{ij},E_{kl}\\rangle=\\delta^{ik}\\delta^{jl}\\).\n\n\\(\\text{vec}\\) and \\(\\text{vec}^{-1}\\)\n\n\nMatrices\n\n\\(a_{i\\bullet}\\) is row \\(i\\) of matrix \\(A\\), \\(a_{\\bullet j}\\) is column \\(j\\).\n\\(a_{i\\star}\\) is the sum of row \\(i\\) of matrix \\(A\\), \\(a_{\\star j}\\) is the sum of column \\(j\\).\n\\(A'\\) is the transpose of \\(A\\), and \\(\\text{diag}(A)\\) is the diagonal matrix with the diagonal elements of \\(A\\). The inverse of a square matrix \\(A\\) is \\(A^{-1}\\), the Moore-Penrose generalized inverse of any matrix \\(A\\) is \\(A^+\\). The transpose of the inverse, and the inverse of the transpose, are \\(A^{-T}\\).\nIf \\(A\\) and \\(B\\) are two \\(n\\times m\\) matrices then their Hadamard (or elementwise) product \\(C=A\\times B\\) has elements \\(c_{ij}=a_{ij}b_{ij}\\). The Hadamard quotient is \\(C=A/B\\), with elements \\(c_{ij}=a_{ij}/b_{ij}\\). The Hadamard power is \\(A^{(k)}=A^{(p-1)}\\times A\\).\nDC matrices. Centering matrix. \\(J_n=I_n-n^{-1}E_n\\). We do not use the subscripts if the order is obvious from the context.\nMatrices of matrices. Partitioned matrices. \\(A_{ij}\\) and thus \\(\\{A_{ij}\\}_{kl}\\).\nDirect sum and Product\n\n\n\nFunctions\n\n\\(f,g,h,\\cdots\\) are used for functions or mappings. \\(f:X\\rightarrow Y\\) says that \\(f\\) maps \\(X\\) into \\(Y\\).\n\\(\\sigma\\) is used for all real-valued least squares loss functions.\n\n\n\nMDS\n\n\\(\\Delta=\\{\\delta_{ij\\cdots}\\}\\) is a matrix or array of dissimilarities.\n\\(\\langle \\mathbb{X},d\\rangle\\) is a metric space, with \\(d:\\mathcal{X}\\otimes\\mathcal{X}\\rightarrow\\mathbb{R}_+\\) the distance function. If \\(X\\) is is an ordered n-tuple \\((x_1,\\cdots,x_n)\\) of elements of \\(\\mathcal{X}\\) then \\(D(X)\\) is \\(\\{d(x_i,x_j)\\}\\), the elements of which we also write as \\(d_{ij}(X)\\).\nSummation over the elements of vector \\(x\\in\\mathbb{R}^n\\) is \\(\\sum_{i=1}^n x_i\\). Summation over the elements of matrix \\(A\\in\\mathbb{R}^{n\\times m}\\) is \\(\\sum_{i=1}^n\\sum_{j=1}^m a_{ij}\\). Summation over the elements above the diagonal of \\(A\\) is \\(\\mathop{\\sum\\sum}_{1\\leq i&lt;j\\leq n}a_{ij}\\).\nConditional summation is, for example, \\(\\sum_{i=1}^n \\{x_i\\mid x_i&gt;0\\}\\).",
    "crumbs": [
      "Notations"
    ]
  }
]