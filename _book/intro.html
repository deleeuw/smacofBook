<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.3">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>1&nbsp; Introduction – smacof at 50</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./basics.html" rel="next">
<link href="./index.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-81b5c3e63835cfde897ecd3d35a35a41.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-dc3f42f4870e7cff4dba1e0f25adc8dc.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./intro.html"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">smacof at 50</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./basics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">smacof Basics</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./metric.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Metric smacof</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./notation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notations</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introhist" id="toc-introhist" class="nav-link active" data-scroll-target="#introhist"><span class="header-section-number">2</span> Brief History</a>
  <ul class="collapse">
  <li><a href="#prehistory" id="toc-prehistory" class="nav-link" data-scroll-target="#prehistory"><span class="header-section-number">2.1</span> Prehistory</a></li>
  <li><a href="#torgerson" id="toc-torgerson" class="nav-link" data-scroll-target="#torgerson"><span class="header-section-number">2.2</span> Torgerson</a></li>
  <li><a href="#bell-laboratories" id="toc-bell-laboratories" class="nav-link" data-scroll-target="#bell-laboratories"><span class="header-section-number">2.3</span> Bell Laboratories</a></li>
  <li><a href="#guttman-lingoes" id="toc-guttman-lingoes" class="nav-link" data-scroll-target="#guttman-lingoes"><span class="header-section-number">2.4</span> Guttman-Lingoes</a></li>
  <li><a href="#alternating-least-squares" id="toc-alternating-least-squares" class="nav-link" data-scroll-target="#alternating-least-squares"><span class="header-section-number">2.5</span> Alternating Least Squares</a></li>
  <li><a href="#majorization" id="toc-majorization" class="nav-link" data-scroll-target="#majorization"><span class="header-section-number">2.6</span> Majorization</a></li>
  </ul></li>
  <li><a href="#introbasic" id="toc-introbasic" class="nav-link" data-scroll-target="#introbasic"><span class="header-section-number">3</span> Basic MDS</a>
  <ul class="collapse">
  <li><a href="#kruskals-stress" id="toc-kruskals-stress" class="nav-link" data-scroll-target="#kruskals-stress"><span class="header-section-number">3.1</span> Kruskal’s stress</a></li>
  <li><a href="#seclocglob" id="toc-seclocglob" class="nav-link" data-scroll-target="#seclocglob"><span class="header-section-number">3.2</span> Local and Global Minima</a></li>
  <li><a href="#partitioning-loss" id="toc-partitioning-loss" class="nav-link" data-scroll-target="#partitioning-loss"><span class="header-section-number">3.3</span> Partitioning Loss</a></li>
  </ul></li>
  <li><a href="#introgeneralize" id="toc-introgeneralize" class="nav-link" data-scroll-target="#introgeneralize"><span class="header-section-number">4</span> Generalizations</a>
  <ul class="collapse">
  <li><a href="#non-linear-mds" id="toc-non-linear-mds" class="nav-link" data-scroll-target="#non-linear-mds"><span class="header-section-number">4.1</span> Non-linear MDS</a></li>
  <li><a href="#gennonmetric" id="toc-gennonmetric" class="nav-link" data-scroll-target="#gennonmetric"><span class="header-section-number">4.2</span> Non-metric MDS</a></li>
  <li><a href="#genfstress" id="toc-genfstress" class="nav-link" data-scroll-target="#genfstress"><span class="header-section-number">4.3</span> Fstress and Friends</a></li>
  <li><a href="#gencons" id="toc-gencons" class="nav-link" data-scroll-target="#gencons"><span class="header-section-number">4.4</span> Constraints</a></li>
  <li><a href="#inreplic" id="toc-inreplic" class="nav-link" data-scroll-target="#inreplic"><span class="header-section-number">4.5</span> Individual Differences</a></li>
  <li><a href="#genasym" id="toc-genasym" class="nav-link" data-scroll-target="#genasym"><span class="header-section-number">4.6</span> Asymmetry</a></li>
  <li><a href="#non-euclidean-distances" id="toc-non-euclidean-distances" class="nav-link" data-scroll-target="#non-euclidean-distances"><span class="header-section-number">4.7</span> Non-Euclidean Distances</a></li>
  </ul></li>
  <li><a href="#principles-of-algorithm-constuction" id="toc-principles-of-algorithm-constuction" class="nav-link" data-scroll-target="#principles-of-algorithm-constuction"><span class="header-section-number">5</span> Principles of Algorithm Constuction</a>
  <ul class="collapse">
  <li><a href="#alternating-least-squares-1" id="toc-alternating-least-squares-1" class="nav-link" data-scroll-target="#alternating-least-squares-1"><span class="header-section-number">5.1</span> Alternating Least Squares</a></li>
  <li><a href="#majorization-1" id="toc-majorization-1" class="nav-link" data-scroll-target="#majorization-1"><span class="header-section-number">5.2</span> Majorization</a></li>
  <li><a href="#introduction-to-majorization" id="toc-introduction-to-majorization" class="nav-link" data-scroll-target="#introduction-to-majorization"><span class="header-section-number">5.3</span> Introduction to Majorization</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>In this manual we study the smacof family of <em>Multidimensional Scaling (MDS)</em> techniques. In MDS the data consist of some type of information about the <em>dissimilarities</em> between a pairs of <em>objects</em>. These objects can be anything: individuals, variables, colors, locations, chemicals, molecules, works of Plato, political parties, Morse code signals, and so on. The dissimilarities can be approximate or imprecise distances, dissimilarity judgments, import/export tables, sociometric choices, and so on. They generally are <em>distance-like</em>, but we do not expect them to satisfy the triangle inequality, and in general not even non-negativity and symmetry. <em>Similarities</em>, such as confusion probabilities, correlations, or preferences, are always converted in some way or another to dissimilarities before they can serve as data for MDS.</p>
<p>The information we have about these dissimilarities can be numerical, ordinal, or categorical. Thus we may have the actual values of some or all of the dissimilarities, we may know their rank order, or we may have a classification of them into a small number of qualitative bins.</p>
<p>Let’s formalize this, and introduce some notation at the same time. The set of ojects is <span class="math inline">\(\mathfrak{O}\)</span>. For example, it can be the set of all cities with more than 10,000 inhabitants. In our MDS analysis we only use <span class="math inline">\(O:=(o_1,\cdots,o_n)\)</span>, an n-tuple (i.e.&nbsp;a finite sequence) of <span class="math inline">\(n\)</span> <em>different</em> elements of <span class="math inline">\(\mathfrak{O}\)</span>, for example <span class="math inline">\(n\)</span> capital cities selected from <span class="math inline">\(\mathfrak{O}\)</span>. If you want to, you can call <span class="math inline">\(O\)</span> a <em>sample</em> from <span class="math inline">\(\mathfrak{O}\)</span>. It is entirely possible, however, that <span class="math inline">\(\mathfrak{O}\)</span> has only <span class="math inline">\(n\)</span> elements, in which case <span class="math inline">\(O\)</span> is just an permutation of the elements of <span class="math inline">\(\mathfrak{O}\)</span>.</p>
<p>A dissimilarity is a function <span class="math inline">\(\delta\)</span> on all pairs of objects, with values in a set <span class="math inline">\(\mathfrak{D}\)</span>. It can be, for example, the time in seconds for an airline flight from city one to city two. Thus <span class="math inline">\(\delta:\mathfrak{O}\otimes\mathfrak{O}\Rightarrow\mathfrak{D}\)</span>. A dissimilaritry is <em>numerical</em> if <span class="math inline">\(\mathfrak{D}\)</span> is subset of real line, it is <em>ordinal</em> if <span class="math inline">\(\mathfrak{D}\)</span> is a partially ordered set, and it is <em>nominal</em> if <span class="math inline">\(\mathfrak{D}\)</span> is neither. Or a dissimilarty is nominal if <span class="math inline">\(\mathfrak{D}\)</span> is any set, and we choose to ignore the ordinal and numerical information if it is there. No matter what <span class="math inline">\(\mathfrak{D}\)</span> is, we suppose it always has the element <span class="math inline">\(\mathit{NA}\)</span> to indicate missing dissimilarities. Cities may not have airports, for example, or we just don’t have the information about the airline distances. Define <span class="math inline">\(\delta_{ij}:=\delta(o_i,o_j)\)</span> and <span class="math inline">\(\Delta:=\delta(O\times O)\)</span>. We can think of <span class="math inline">\(\Delta\)</span> and an <span class="math inline">\(n\times n\)</span> matrix with elements in <span class="math inline">\(\mathfrak{D}\)</span>.</p>
<p>MDS techniques map the objects <span class="math inline">\(o_i\)</span> into <em>points</em> <span class="math inline">\(x_i\)</span> in some metric space <span class="math inline">\(\langle\mathfrak{X},d\rangle\)</span> in such a way that the distances between pairs of points approximate the dissimilarities of the corresponding pairs of objects. Thus we want to find a map <span class="math inline">\(x:\mathfrak{O}\rightarrow\mathfrak{X}\)</span> that produces an n-tuple <span class="math inline">\(X=(x_1,\cdots,x_n)\)</span> of elements of <span class="math inline">\(\mathfrak{X}\)</span>, where <span class="math inline">\(x_i:=x(o_i)\)</span>. Also define <span class="math inline">\(d_{ij}:=d(x_i,x_j)\)</span> and <span class="math inline">\(D(X):=d(X\times X\)</span>. Unlike the dissimilarities the <span class="math inline">\(d_{ij}\)</span> are always numerical, because distances are. So MDS finds <span class="math inline">\(X\)</span> such that <span class="math inline">\(D(X)\approx\Delta\)</span>.</p>
<p>For numerical dissimilarities it is clear what “approximation” means, we simply want the distances and the corresponding dissimilarities to be numerically close. Because there are generally many dissimilarities and distances a combined measure of closeness can still be defined in many different ways. For ordinal and nominal dissimilarities the notion of approximation is less clear, and we have to develop more specialized techniques to measure how well the distances fit the dissimilarities.</p>
<section id="introhist" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Brief History</h1>
<p><span class="citation" data-cites="deleeuw_heiser_C_80">De Leeuw and Heiser (<a href="#ref-deleeuw_heiser_C_80" role="doc-biblioref">1980</a>)</span></p>
<p>This section has a different emphasis. We limit ourselves to developments in Euclidean MDS, and to contributions with direct computational consequences that have a direct or indirect link to psychometrics, and to work before 1960. This is reviewed ably in the presidential address of <span class="citation" data-cites="torgerson_65">W. S. Torgerson (<a href="#ref-torgerson_65" role="doc-biblioref">1965</a>)</span>.</p>
<p>Our history review takes the form of brief summaries of what we consider to be milestone papers or books.</p>
<section id="prehistory" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="prehistory"><span class="header-section-number">2.1</span> Prehistory</h2>
<p>The prehistory of MDS is defined as any publication before <span class="citation" data-cites="young_householder_38">Young and Householder (<a href="#ref-young_householder_38" role="doc-biblioref">1938</a>)</span>.</p>
<p>stumpf 1883</p>
<blockquote class="blockquote">
<p>Unter Distanzen aber verstehen wir, das Wort hier ebenfalls in einem für Manche ungewohnt weiten Sinne nehmend, nicht blos räumliche und zeitliche sondern auch qualitative und solche der Intensität, und definieren das Wort durch: Grade der Unähnlichkeit.</p>
</blockquote>
<p>From the translation</p>
<blockquote class="blockquote">
<p>Taking the word “distances” in a sense uncommonly broad for many, however, we mean here not only spatial and temporal ones, but also qualitative ones as well as ones of intensity, and define the word by degrees of dissimilarity.</p>
</blockquote>
<p>fisher 1922 boyden 1932/1935 goldmeier 1937</p>
<p>richardson 1938 klingberg 1941 gulliksen 1946 attneave 1950 ekman 1954</p>
<p>torgerson 1951 torgerson 1952 messick_abelson 1956</p>
<p>reviewed in <span class="citation" data-cites="deleeuw_heiser_C_80">De Leeuw and Heiser (<a href="#ref-deleeuw_heiser_C_80" role="doc-biblioref">1980</a>)</span>.</p>
<p>Young-Householder, etc.</p>
</section>
<section id="torgerson" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="torgerson"><span class="header-section-number">2.2</span> Torgerson</h2>
<p><span class="citation" data-cites="torgerson_52">W. S. Torgerson (<a href="#ref-torgerson_52" role="doc-biblioref">1952</a>)</span> <span class="citation" data-cites="torgerson_65">W. S. Torgerson (<a href="#ref-torgerson_65" role="doc-biblioref">1965</a>)</span></p>
</section>
<section id="bell-laboratories" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="bell-laboratories"><span class="header-section-number">2.3</span> Bell Laboratories</h2>
<p><span class="citation" data-cites="shepard_62a">Shepard (<a href="#ref-shepard_62a" role="doc-biblioref">1962a</a>)</span> <span class="citation" data-cites="shepard_62b">Shepard (<a href="#ref-shepard_62b" role="doc-biblioref">1962b</a>)</span></p>
<p><span class="citation" data-cites="kruskal_64a">Kruskal (<a href="#ref-kruskal_64a" role="doc-biblioref">1964a</a>)</span> <span class="citation" data-cites="kruskal_64b">Kruskal (<a href="#ref-kruskal_64b" role="doc-biblioref">1964b</a>)</span></p>
</section>
<section id="guttman-lingoes" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="guttman-lingoes"><span class="header-section-number">2.4</span> Guttman-Lingoes</h2>
<p><span class="citation" data-cites="guttman_68">Guttman (<a href="#ref-guttman_68" role="doc-biblioref">1968</a>)</span></p>
</section>
<section id="alternating-least-squares" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="alternating-least-squares"><span class="header-section-number">2.5</span> Alternating Least Squares</h2>
</section>
<section id="majorization" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="majorization"><span class="header-section-number">2.6</span> Majorization</h2>
<p><span class="citation" data-cites="deleeuw_C_77">De Leeuw (<a href="#ref-deleeuw_C_77" role="doc-biblioref">1977</a>)</span> <span class="citation" data-cites="deleeuw_heiser_C_77">De Leeuw and Heiser (<a href="#ref-deleeuw_heiser_C_77" role="doc-biblioref">1977</a>)</span></p>
<p>There was some early work by Richardson, Messick, Abelson and Torgerson who combined Thurstonian scaling of similarities with the mathematical results of <span class="citation" data-cites="schoenberg_35">Schoenberg (<a href="#ref-schoenberg_35" role="doc-biblioref">1935</a>)</span> and <span class="citation" data-cites="young_householder_38">Young and Householder (<a href="#ref-young_householder_38" role="doc-biblioref">1938</a>)</span>.</p>
<p>Despite these early contributions it makes sense, certainly from the point of view of my personal history, but probably more generally, to think of MDS as starting as a widely discussed, used, and accepted technique since the book by <span class="citation" data-cites="torgerson_58">W. S. Torgerson (<a href="#ref-torgerson_58" role="doc-biblioref">1958</a>)</span>. This was despite the fact that in the fifties and sixties computing eigenvalues and eigenvectors of a matrix of size 20 or 30 was still a considerable challenge.</p>
<p>A few years later the popularity of MDS got a large boost by developments centered at Bell Telephone Laboratories in Murray Hill, New Jersey, the magnificent precursor of Silicon Valley. First there was nonmetric MDS by <span class="citation" data-cites="shepard_62a">Shepard (<a href="#ref-shepard_62a" role="doc-biblioref">1962a</a>)</span>, <span class="citation" data-cites="shepard_62b">Shepard (<a href="#ref-shepard_62b" role="doc-biblioref">1962b</a>)</span> and <span class="citation" data-cites="kruskal_64a">Kruskal (<a href="#ref-kruskal_64a" role="doc-biblioref">1964a</a>)</span>, <span class="citation" data-cites="kruskal_64b">Kruskal (<a href="#ref-kruskal_64b" role="doc-biblioref">1964b</a>)</span>, And later another major development was the introduction of individual difference scaling by <span class="citation" data-cites="carroll_chang_70">Carroll and Chang (<a href="#ref-carroll_chang_70" role="doc-biblioref">1970</a>)</span> and <span class="citation" data-cites="harshman_70">Harshman (<a href="#ref-harshman_70" role="doc-biblioref">1970</a>)</span>. Perhaps even more important was the development of computer implementations of these new techniques. Some of the early history of nonmetric MDS is in <span class="citation" data-cites="deleeuw_E_17e">De Leeuw (<a href="#ref-deleeuw_E_17e" role="doc-biblioref">2017a</a>)</span>.</p>
<p>Around the same time there were interesting theoretical contributions in <span class="citation" data-cites="coombs_64">Coombs (<a href="#ref-coombs_64" role="doc-biblioref">1964</a>)</span>, which however did not much influence the practice of MDS. ….. And several relatively minor variations of the Bell Laboratories approach were proposed by <span class="citation" data-cites="guttman_68">Guttman (<a href="#ref-guttman_68" role="doc-biblioref">1968</a>)</span>, but Guttman’s influence on further MDS implementations turned out to be fairly localized and limited.</p>
<p>The main development in comptational MDS after the Bell Laboratories surge was probably smacof. Initially, in <span class="citation" data-cites="deleeuw_C_77">De Leeuw (<a href="#ref-deleeuw_C_77" role="doc-biblioref">1977</a>)</span>, this stood for <em>Scaling by Maximizing a Convex Function</em>. Later it was also used to mean <em>Scaling by Majorizing a Complicated Function</em>. Whatever. In this book smacof just stands for smacof. No italics, no boldface, no capitals.</p>
<p>The first smacof programs were written in 1977 in FORTRAN at the Department of Data Theory in Leiden (<span class="citation" data-cites="heiser_deleeuw_R_77">Heiser and De Leeuw (<a href="#ref-heiser_deleeuw_R_77" role="doc-biblioref">1977</a>)</span>). Eventually they migrated to SPSS (for example, <span class="citation" data-cites="meulman_heiser_12">Meulman and Heiser (<a href="#ref-meulman_heiser_12" role="doc-biblioref">2012</a>)</span>) and to R (<span class="citation" data-cites="deleeuw_mair_A_09c">De Leeuw and Mair (<a href="#ref-deleeuw_mair_A_09c" role="doc-biblioref">2009</a>)</span>). The SPSS branch, now the IBM SPSS branch, and the R branch have diverged somewhat, and they continue to be developed independently.</p>
<p>Parallel to this book there is an attempt to rewrite the various smacof programs in C, with the necessary wrappers to call them from R (<span class="citation" data-cites="deleeuw_E_17p">De Leeuw (<a href="#ref-deleeuw_E_17p" role="doc-biblioref">2017b</a>)</span>). The C code, with makefiles and test routines, is at <a href="https://github.com/deleeuw/smacof">github.com/deleeuw/smacof</a></p>
</section>
</section>
<section id="introbasic" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Basic MDS</h1>
<p>Following Kruskal, and to a lesser extent Shepard, we measure the fit of distances to dissimilarities using an explicit real-valued <em>loss function</em> (or <em>badness-of-fit measure</em>), which is minimized over the possible maps of the objects into the metric space. This is a very general definition of MDS, covering all kinds of variations of the target metric space and of the way fit is measured. Obviously we will not discuss <em>all</em> these possible forms of MDS, which also includes various techniques more properly discussed as cluster analysis, classification, or discrimination.</p>
<p>To fix our scope we first define <em>basic MDS</em>, which is short for <em>Least Squares Euclidean Metric MDS</em>. It is defined as MDS with the following characteristics.</p>
<ol type="1">
<li>The metric space is a Euclidean space.</li>
<li>The dissimilarities are numerical, symmetric, and non-negative.</li>
<li>The loss function is a weighted sum of squares of the <em>residuals</em>, which are the differences between dissimilarities and Euclidean distances.</li>
<li>Weights are numerical, symmetric, and non-negative.</li>
<li>Self-dissimilarities are zero and the corresponding terms in the loss function also have weight zero.</li>
</ol>
<p>By a <em>Euclidean space</em> we mean a finite dimensional vector space, with addition and scalar multiplication, and with an inner product that defines the distances. For the <em>inner product</em> of vectors <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> we write <span class="math inline">\(\langle x,y\rangle\)</span>. The <em>norm</em> of <span class="math inline">\(x\)</span> is defined as <span class="math inline">\(\|x\|:=\sqrt{\langle x,x\rangle}\)</span>, and the <em>distance</em> between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> is <span class="math inline">\(d(x,y):=\|x-y\|\)</span>.</p>
<p>The <em>loss function</em> we use is called <em>stress</em>. It was first explicitly introduced in MDS as <em>raw stress</em> by <span class="citation" data-cites="kruskal_64a">Kruskal (<a href="#ref-kruskal_64a" role="doc-biblioref">1964a</a>)</span> and <span class="citation" data-cites="kruskal_64b">Kruskal (<a href="#ref-kruskal_64b" role="doc-biblioref">1964b</a>)</span>. We define stress in a slightly different way, because we want to be consistent over the whole range of the smacof versions and implementations. In smacof stress is the real-valued function <span class="math inline">\(\sigma\)</span>, defined on the space <span class="math inline">\(\mathbb{R}^{n\times p}\)</span> of configurations, as</p>
<p><span class="math display">\[\begin{equation}
\sigma(X):=\frac14\sum_{i=1}^n\sum_{j=1}^n w_{ij}(\delta_{ij}-d_{ij}(X))^2.
(\#eq:stressall)
\end{equation}\]</span></p>
<p>Note that we use <span class="math inline">\(:=\)</span> for definitions, i.e.&nbsp;for concepts and symbols that are not standard mathematical usage, when they occur for the first time in this book. Through the course of the book it will probably become clear why the mysterious factor <span class="math inline">\(\frac14\)</span> is there. Clearly it has no influence on the actual minimization of the loss function.</p>
<p>In definition @ref(eq:stressall) we use the following objects and symbols.</p>
<ol type="1">
<li><span class="math inline">\(W=\{w_{ij}\}\)</span> is a symmetric, non-negative, and hollow matrix of <em>weights</em>, where <em>hollow</em> means zero diagonal.</li>
<li><span class="math inline">\(\Delta=\{\delta_{ij}\}\)</span> is a symmetric, non-negative, and hollow matrix of <em>dissimilarities</em>.</li>
<li><span class="math inline">\(X\)</span> is an <span class="math inline">\(n\times p\)</span> <em>configuration</em>, containing coordinates of <span class="math inline">\(n\)</span> <em>points</em> in <span class="math inline">\(p\)</span> dimensions.</li>
<li><span class="math inline">\(D(X)=\{d_{ij}(X)\}\)</span> is a symmetric, non-negative, and hollow matrix of <em>Euclidean distances</em> between the <span class="math inline">\(n\)</span> points in <span class="math inline">\(X\)</span>. Thus <span class="math inline">\(d_{ij}(X):=\sqrt{\sum_{s=1}^p(x_{is}-x_{js})^2}\)</span>.</li>
</ol>
<p>Note that symmetry and hollowness of the basic objects <span class="math inline">\(W\)</span>, <span class="math inline">\(\Delta\)</span>, and <span class="math inline">\(D\)</span> allows us carry out the summation of the weighted squared residuals in formula @ref(eq:stressall) over the upper (or lower) diagonal elements only. Thus we can also write <span class="math display">\[\begin{equation}
\sigma(X):=\frac12\mathop{\sum\sum}_{1\leq i&lt;j\leq n} w_{ij}(\delta_{ij}-d_{ij}(X))^2.
(\#eq:stresshalf)
\end{equation}\]</span> We use the notation <span class="math inline">\(\mathop{\sum\sum}_{1\leq i&lt;j\leq n}\)</span> for summation over the lower-diagonal elements of a matrix.</p>
<p>The function <span class="math inline">\(D\)</span>, which computes the distance matrix <span class="math inline">\(D(X)\)</span> from a configuration <span class="math inline">\(X\)</span>, is matrix-valued. It maps the <span class="math inline">\(n\times p\)</span>-dimensional <em>configuration space</em> <span class="math inline">\(\mathbb{R}^{n\times p}\)</span> into the set <span class="math inline">\(D(\mathbb{R}^{n\times p})\)</span> of Euclidean distance matrices between <span class="math inline">\(n\)</span> points in <span class="math inline">\(\mathbb{R}^p\)</span>, which is a subset of the convex cone of hollow, symmetric, non-negative matrices in the linear space <span class="math inline">\(\mathbb{R}^{n\times n}\)</span> (<span class="citation" data-cites="datorro_18">Datorro (<a href="#ref-datorro_18" role="doc-biblioref">2018</a>)</span>).</p>
<p>In basic MDS the weights and dissimilarities are given numbers, and we minimize stress over all <span class="math inline">\(n\times p\)</span> configurations <span class="math inline">\(X\)</span>. Note that the <em>dimensionality</em> <span class="math inline">\(p\)</span> is also supposed to be known beforehand, and that MDS in <span class="math inline">\(p\)</span> dimensions is different from MDS in <span class="math inline">\(q\not= p\)</span> dimensions. We sometimes emphasize this by writing <span class="math inline">\(pMDS\)</span>, which indicates that we will map the points into <span class="math inline">\(p\)</span>-dimensional space.</p>
<p>Two boundary cases that will interest us are <em>Unidimensional Scaling</em> or <em>UDS</em>, where <span class="math inline">\(p=1\)</span>, and <em>Full-dimensional Scaling</em> or <em>FDS</em>, where <span class="math inline">\(p=n\)</span>. Thus UDS is 1MDS and FDS is nMDS. Most actual MDS applications in the sciences use 1MDS, 2MDS or 3MDS, because configurations in one, two, or three dimensions can easily be plotted with standard graphics tools. Note that MDS is not primarily a tool to tests hypotheses about dimensionality and to find meaningful dimensions. It is a mostly a mapping tool for data reduction, to graphically find interesting aspects of dissimilarity matrices.</p>
<p>The projections on the dimensions are usually ignored, it is the configuration of points that is the interesting outcome. This distinguishes MDS from, for example, factor analysis. There is no Varimax, Oblimax, Quartimax, and so on. Exceptions are confirmatory applications of MDS in genetic mapping along the chromosome, in archeological seriation, in testing psychological theories of cognition and representation, in the conformation of molecules, and in geographic and geological applications. In these areas the dimensionality and general structure of the configuration are given by prior knowledge, we just do not know the precise location and distances of the points. For more discussion of the different uses of MDS we refer to <span class="citation" data-cites="deleeuw_heiser_C_82">De Leeuw and Heiser (<a href="#ref-deleeuw_heiser_C_82" role="doc-biblioref">1982</a>)</span>.</p>
<section id="kruskals-stress" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="kruskals-stress"><span class="header-section-number">3.1</span> Kruskal’s stress</h2>
<p>Definition @ref(eq:stressall) differs from Kruskal’s original stress in at least three ways: in Kruskal’s use of the square root, in our use of weights, and in our different approach to normalization.</p>
<p>We have paid so much attention to Kruskal’s original definition, because the choices made there will play a role in the normalization discussion in the ordinal scaling chapter (section @ref(nmdsnorm)), in the comparison of Kruskal’s and Guttman’s approach to ordinal MDS (sections @ref(nmdskruskal) and @ref(nmdsguttman)), and in our discussions about the differences between Kruskal’s stress @ref(eq:kruskalstressfinal) and smacof’s stress @ref(eq:stressall) in the next three sections of this chapter.</p>
<section id="square-root" class="level4" data-number="3.1.0.1">
<h4 data-number="3.1.0.1" class="anchored" data-anchor-id="square-root"><span class="header-section-number">3.1.0.1</span> Square root</h4>
<p>Let’s discuss the square root first. Using it or not using it does not make a difference for the minimization problem. Using the square root, however, does give a more sensible root-mean-square scale, in which stress is homogeneous of degree one, instead of degree two. But I do not want to compute all those unnecessary square roots in my algorithms, and I do not want to drag them along through my derivations. Moreover the square root potentially causes problems with differentiability at those <span class="math inline">\(X\)</span> where <span class="math inline">\(\sigma(X)\)</span> is zero. Thus, througout the book, we do not use the square root in our formulas and derivations. In fact, we do not even use it in our computer programs, except at the very last moment when we return the final stress after the algorithm has completed.</p>
</section>
<section id="bweights" class="level4" data-number="3.1.0.2">
<h4 data-number="3.1.0.2" class="anchored" data-anchor-id="bweights"><span class="header-section-number">3.1.0.2</span> Weights</h4>
<p>There were no weights <span class="math inline">\(W=\{w_{ij}\}\)</span> in the original definition of stress by <span class="citation" data-cites="kruskal_64a">Kruskal (<a href="#ref-kruskal_64a" role="doc-biblioref">1964a</a>)</span>, and neither are they there in most of the basic later contributions to MDS by Guttman, Lingoes, Roskam, Ramsay, or Young. We will use weights throughout the book, because they have various interesting applications within basic MDS, without unduly complicating the derivations and computations. In <span class="citation" data-cites="groenen_vandevelden_16">Groenen and Van de Velden (<a href="#ref-groenen_vandevelden_16" role="doc-biblioref">2016</a>)</span>, section 6, the various uses of weights in the stress loss function are enumerated. They generously, and correctly, attribute the consistent use of weights in MDS to me. I quote from their paper:</p>
<blockquote class="blockquote">
<ol type="1">
<li>Handling missing data is done by specifying <span class="math inline">\(w_{ij} = 0\)</span> for missings and 1 otherwise thereby ignoring the error corresponding to the missing dissimilarities.</li>
<li>Correcting for nonuniform distributions of the dissimilarities to avoid dominance of the most frequently occurring dissimilarities.</li>
<li>Mimicking alternative fit functions for MDS by minimizing Stress with <span class="math inline">\(w_{ij}\)</span> being a function of the dissimilarities.</li>
<li>Using a power of the dissimilarities to emphasize the ﬁtting of either large or small dissimilarities.</li>
<li>Special patterns of weights for speciﬁc models.</li>
<li>Using a speciﬁc choice of weights to avoid nonuniqueness.</li>
</ol>
</blockquote>
<p>In some situations, for example for huge data sets, it is computationally convenient, or even necessary, to minimize the influence of the weights on the computations. We can use <em>majorization</em> to turn the problem from a weighted least squares problem to an iterative unweighted least squares problem. The technique, which we call <em>unweighting</em>, is discussed in detail in section @ref(minunweight).</p>
</section>
<section id="intronorm" class="level4" data-number="3.1.0.3">
<h4 data-number="3.1.0.3" class="anchored" data-anchor-id="intronorm"><span class="header-section-number">3.1.0.3</span> Normalization</h4>
<p>This section deals with a rather trivial problem, which has however caused problems in various stages of smacof’s 50-year development history. Because the problem is trivial, and the choices that must be made are to a large extent arbitrary, it has been overlooked and somewhat neglected.</p>
<p>In basic MDS we scale the weights and dissimilarities. It is clear that if we multiply the weights or dissimilarities by a constant, then the optimal approximating distances <span class="math inline">\(D(X)\)</span> and the optimal configuration <span class="math inline">\(X\)</span> will be multiplied by the same constant. That is exactly why Kruskal’s raw stress had to be normalized. Consequently we in basic MDS we always scale weights and dissimilarities by</p>
<p><span class="math display">\[\begin{align}
\mathop{\sum\sum}_{1\leq i&lt;j\leq n}w_{ij}&amp;=1,(\#eq:scaldiss1)\\
\mathop{\sum\sum}_{1\leq i&lt;j\leq n}w_{ij}^{\ }\delta_{ij}^2&amp;=1.(\#eq:scaldiss2)
\end{align}\]</span></p>
<p>This simplifies our formulas and makes them look better (see, for example, section @ref(propexpand) and section @ref(secrhostress)). It presupposes, of course, that <span class="math inline">\(w_{ij}\delta_{ij}\not=0\)</span> for at least one <span class="math inline">\(i\not= j\)</span>, which we will happily assume in the sequel, because otherwise the MDS problem is trivial. Note that if all weights are equal (which we call the <em>unweighted case</em>) then they are equal to <span class="math inline">\(1/\binom{n}{2}\)</span> and thus we require <span class="math inline">\(\mathop{\sum\sum}_{1\leq i&lt;j\leq n}\delta_{ij}^2=\frac12n(n-1)\)</span>.</p>
<p>Using normalized dissimilarities amounts to the same defining stress as <span class="math display">\[\begin{equation}
\sigma(X)=\frac12\frac{\mathop{\sum\sum}_{1\leq i&lt;j\leq n}w_{ij}(\delta_{ij}^2-d_{ij}(X))^2}{\mathop{\sum\sum}_{1\leq i&lt;j\leq n}w_{ij}\delta_{ij}^2}.
(\#eq:stressrat)
\end{equation}\]</span></p>
<p>This is useful to remember when we discuss the various normalizations for non-metric MDS in section @ref(nmdsnorm).</p>
</section>
</section>
<section id="seclocglob" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="seclocglob"><span class="header-section-number">3.2</span> Local and Global Minima</h2>
<p>In basic MDS our goal is to compute both <span class="math inline">\(\min_X\sigma(X)\)</span> and <span class="math inline">\(\mathop{\text{Argmin}}_X\sigma(X)\)</span>, where <span class="math inline">\(\sigma(X)\)</span> is defined as @ref(eq:stressall), and where we minimize over all configurations in <span class="math inline">\(\mathbb{R}^{n\times p}\)</span>.</p>
<p>In this book we study both the properties of the stress loss function and a some of its generalizations, and the various ways to minimize these loss functions over configurations (and sometimes over transformations of the dissimilarities as well).</p>
<p>Emphasis local minima</p>
<p>Compute stationary points</p>
</section>
<section id="partitioning-loss" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="partitioning-loss"><span class="header-section-number">3.3</span> Partitioning Loss</h2>
</section>
</section>
<section id="introgeneralize" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Generalizations</h1>
<p>The most important generalizations of basic MDS we will study in later chapters of this book are discussed briefly in the following sections.</p>
<section id="non-linear-mds" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="non-linear-mds"><span class="header-section-number">4.1</span> Non-linear MDS</h2>
</section>
<section id="gennonmetric" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="gennonmetric"><span class="header-section-number">4.2</span> Non-metric MDS</h2>
<p>Basic MDS is a form of <em>Metric Multidimensional Scaling</em> or <em>MMDS</em>, in which dissimilarities are either known or missing. In chapter @ref(nonmtrmds) we relax this assumption. Dissimilarities may be partly known, for example we may know they are in some interval, we may only know their order, or we may know them up to some smooth transformation. MDS with partly known dissimilarities is <em>Non-metric Multidimensional Scaling</em> or <em>NMDS</em>. Completely unknown (missing) dissimilarities are an exception, because we can just handle this in basic MDS by setting the corresponding weights equal to zero.</p>
<p>In NMDS we minimize stress over all configurations, but also over the unknown dissimilarities. What we know about them (the interval they are in, the transformations that are allowed, the order they are in) defines a subset of the space of non-negative, hollow, and symmetric matrices. Any matrix in that subset is a matrix of what <span class="citation" data-cites="takane_young_deleeuw_A_77">Takane, Young, and De Leeuw (<a href="#ref-takane_young_deleeuw_A_77" role="doc-biblioref">1977</a>)</span> call <em>disparities</em>, i.e.&nbsp;imputed dissimilarities. The imputation provides the missing information and transforms the non-numerical information we have about the dissimilarities into a numerical matrix of disparities. Clearly this is an <em>optimistic imputation</em>, in the sense that it chooses from the set of admissible disparities to minimize stress (for a given configuration).</p>
<p>One more terminological point. Often <em>non-metric</em> is reserved for ordinal MDS, in which we only know a (partial or complete) order of the dissimilarities. Allowing linear or polynomial transformations of the dissimilarities, or estimating an additive constant, is then supposed to be a form of metric MDS. There is something to be said for that. Maybe it makes sense to distinguish non-metric <em>in the wide sense</em> (in which stress must be minimized over both <span class="math inline">\(X\)</span> and <span class="math inline">\(\Delta\)</span>) and <em>non-metric in the narrow sense</em> in which the set of admissible disparities is defined by linear inequalities. Nonmetric in the narrow sense will also be called <em>ordinal MDS</em> or <em>OMDS</em>.</p>
<p>It is perhaps useful to remember that <span class="citation" data-cites="kruskal_64a">Kruskal (<a href="#ref-kruskal_64a" role="doc-biblioref">1964a</a>)</span> introduced explicit loss functions in MDS to put the somewhat heuristic NMDS techniques of <span class="citation" data-cites="shepard_62a">Shepard (<a href="#ref-shepard_62a" role="doc-biblioref">1962a</a>)</span> onto a firm mathematical and computational foundation. Thus, more or less from the beginning of iterative least squares MDS, there was a focus on non-metric rather than metric MDS, and this actually contributed a great deal to the magic and success of the technique. In this book most of the results are derived for basic MDS, which is metric MDS, with non-metric MDS as a relatively straightforward extension not discussed until chapter @ref(nonmtrmds). So, at least initially, we take the numerical values of the dissimilarities seriously, as do <span class="citation" data-cites="torgerson_58">W. S. Torgerson (<a href="#ref-torgerson_58" role="doc-biblioref">1958</a>)</span> and <span class="citation" data-cites="shepard_62a">Shepard (<a href="#ref-shepard_62a" role="doc-biblioref">1962a</a>)</span>, <span class="citation" data-cites="shepard_62b">Shepard (<a href="#ref-shepard_62b" role="doc-biblioref">1962b</a>)</span>.</p>
<p>It may be the case that in the social and behavioural sciences only the ordinal information in the dissimilarities is reliable and useful. But, since 1964, MDS has also been applied in molecular conformation, chemometrics, genetic sequencing, archelogical seriation, and in network design and location analysis. In these areas the numerical information in the dissimilarities is usually meaningful and should not be thrown out right away. Also, the use of the Shepard plot, with dissimilarities on the horizontal axis and fitted distances on the vertical axis, suggests there is more to dissimilarities than just their rank order.</p>
</section>
<section id="genfstress" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="genfstress"><span class="header-section-number">4.3</span> Fstress and Friends</h2>
<p>Instead of defining the residuals in the least squares loss function as <span class="math inline">\(\delta_{ij}-d_{ij}(X)\)</span> chapter @ref(chrstress) discusses the more general cases where the residuals are <span class="math inline">\(f(\delta_{ij})-g(d_{ij}(X))\)</span> for some known non-negative increasing function <span class="math inline">\(f\)</span>. This defines the <em>fstress</em> loss function.</p>
<p>If <span class="math inline">\(f(x)=x^r\)</span> with <span class="math inline">\(r&gt;0\)</span> then fstress is called <em>rstress</em>. Thus stress is rstress with <span class="math inline">\(r=1\)</span>, also written as <em>1stress</em> or <span class="math inline">\(\sigma_1\)</span>. In more detail we will also look at <span class="math inline">\(r=2\)</span>, which is called <em>sstress</em> by <span class="citation" data-cites="takane_young_deleeuw_A_77">Takane, Young, and De Leeuw (<a href="#ref-takane_young_deleeuw_A_77" role="doc-biblioref">1977</a>)</span>. In chapter @ref(chsstressstrain) we look at the problem of minimizing sstress and weighted version <em>strain</em>. The case of rstress with <span class="math inline">\(r\rightarrow 0\)</span> is also of interest, because it leads to the loss function in <span class="citation" data-cites="ramsay_77">Ramsay (<a href="#ref-ramsay_77" role="doc-biblioref">1977</a>)</span>.</p>
</section>
<section id="gencons" class="level2" data-number="4.4">
<h2 data-number="4.4" class="anchored" data-anchor-id="gencons"><span class="header-section-number">4.4</span> Constraints</h2>
<p>Instead of minimizing stress over all <span class="math inline">\(X\)</span> in <span class="math inline">\(\mathbb{R}^{n\times p}\)</span> we will look in chapter @ref(cmds) at various generalizations where minimization is over a subset <span class="math inline">\(\mathcal{\Omega}\)</span> of <span class="math inline">\(\mathbb{R}^{n\times p}\)</span>. This is often called <em>Constrained Multidimensional Scaling</em> or <em>CMDS</em>.</p>
<p>The distinction may be familiar from factor analysis, where we distinguish between exploratory and confirmatory factor analysis. If we have prior information about the parameters then incorporating that prior information in the analysis will generally lead to more precise and more interpretable estimates. The risk is, of course that if our prior information is wrong, if it is just prejudice, then we will have a solution which is precise but incorrect. We have the famous trade-off between bias and variance. In MDS this trade-off does not seem to apply directly, because the necessary replication frameworks are missing.</p>
<p>and we do not attach much value to locating the true configuration.</p>
<p>Primal and Dual</p>
<p><span class="math display">\[
\min_{X\in\Omega}\sigma(X)
\]</span></p>
<p><span class="math display">\[
\min_X\sigma(X)+\lambda\kappa(X,\Omega)
\]</span> where <span class="math inline">\(\kappa(X,\Omega)\geq 0\)</span> and <span class="math inline">\(\kappa(X,\Omega)=0\)</span> if and only if <span class="math inline">\(X\in\Omega\)</span>.</p>
</section>
<section id="inreplic" class="level2" data-number="4.5">
<h2 data-number="4.5" class="anchored" data-anchor-id="inreplic"><span class="header-section-number">4.5</span> Individual Differences</h2>
<p>Now consider the situation in which we have <span class="math inline">\(m\)</span> different dissimilarity matrices <span class="math inline">\(\Delta_k\)</span> and <span class="math inline">\(m\)</span> different weight matrices <span class="math inline">\(W_k\)</span>. We generalize basic MDS by defining <span class="math display">\[\begin{equation}
\sigma(X_1,\cdots,X_m):=\frac12\sum_{k=1}^m\mathop{\sum\sum}_{1\leq i&lt;j\leq n}w_{ijk}(\delta_{ijk}-d_{ij}(X_k))^2,
(\#eq:replistress)
\end{equation}\]</span> and minimize this over the <span class="math inline">\(X_k\)</span>.</p>
<p>There are two simple ways to deal with this generalization. The first is to put no further constraints on the <span class="math inline">\(X_k\)</span>. This means solving <span class="math inline">\(m\)</span> separate basic MDS problems, one for each <span class="math inline">\(k\)</span>. The second way is to require that all <span class="math inline">\(X_k\)</span> are equal. As shown in more detail in section @ref(indifrepl) this reduced to a single basic MDS problem with dissimilarities that are a weighted sum of the <span class="math inline">\(\Delta_k\)</span>. So both these approaches do not really bring anything new.</p>
<p>Minimizing @ref(eq:replistress) becomes more interesting if we constrain the <span class="math inline">\(X_k\)</span> in various ways. Usually this is done by making sure they have a component that is common to all <span class="math inline">\(k\)</span> and a component that is specific or unique to each <span class="math inline">\(k\)</span>. This approach, which generalizes constrained MDS, is discussed in detail in chapter @ref(chindif).</p>
</section>
<section id="genasym" class="level2" data-number="4.6">
<h2 data-number="4.6" class="anchored" data-anchor-id="genasym"><span class="header-section-number">4.6</span> Asymmetry</h2>
<p>We have seen in section @ref(datasym) of this chapter that in basic MDS the assumption that <span class="math inline">\(W\)</span> and <span class="math inline">\(\Delta\)</span> are symmetric and hollow can be made without loss of generality. The simple partitioning which proved this was based on the fact that <span class="math inline">\(D(X)\)</span> is always symmetric and hollow. By the way, the assumption that <span class="math inline">\(W\)</span> and <span class="math inline">\(D\)</span> are non-negative cannot be made without loss of generality, as we will see below.</p>
<p>In chapter @ref(asymmds) we relax the assumption that <span class="math inline">\(D(X)\)</span> is symmetric (still requiring it to be non-negative and hollow). This could be called <em>Asymmetric MDS</em>, or <em>AMDS</em>. I was reluctant at first to include this chapter, because asymmetric distances do not exist. And certainly are not Euclidean distances, so they are not covered by the title of this book. But as long as we stay close to Euclidean distances, least squares, and the smacof approach, I now feel reasonably confident the chapter is not too much of a foreign body.</p>
</section>
<section id="non-euclidean-distances" class="level2" data-number="4.7">
<h2 data-number="4.7" class="anchored" data-anchor-id="non-euclidean-distances"><span class="header-section-number">4.7</span> Non-Euclidean Distances</h2>
<p>When Kruskal introduced gradient-based methods to minimize stress he also discussed the possibility to use Minkovski metrics other than the Euclidean metric. This certainly was part of the appeal of the new methods, in fact it seemed as if the gradient methods made it possible to use any distance function whatsoever. This initial feeling of empowerment was somewhat naive, because it ignored the seriousness of the local minimum problem, the combinatorial nature of one-dimensional and city block scaling, the problems with nonmetric unfolding, and the problematic nature of gradient methods if the distances are not everywhere differentiable. All these complications will be discussed in this book. But it made me decide to ignore Minkovski distances (and hyperbolic and elliptic non-Euclidean distances), because life with stress is complicated and challenging enough as it is.</p>
<p><span class="citation" data-cites="groenen_mathar_heiser_95">Groenen, Mathar, and Heiser (<a href="#ref-groenen_mathar_heiser_95" role="doc-biblioref">1995</a>)</span>, <span class="citation" data-cites="mathar_meyer_94">Mathar and Meyer (<a href="#ref-mathar_meyer_94" role="doc-biblioref">1994</a>)</span></p>
</section>
</section>
<section id="principles-of-algorithm-constuction" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Principles of Algorithm Constuction</h1>
<section id="alternating-least-squares-1" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="alternating-least-squares-1"><span class="header-section-number">5.1</span> Alternating Least Squares</h2>
</section>
<section id="majorization-1" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="majorization-1"><span class="header-section-number">5.2</span> Majorization</h2>
</section>
<section id="introduction-to-majorization" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="introduction-to-majorization"><span class="header-section-number">5.3</span> Introduction to Majorization</h2>
<p>Majorization, these days better known as MM (<span class="citation" data-cites="lange_16">Lange (<a href="#ref-lange_16" role="doc-biblioref">2016</a>)</span>), is a general approach for the construction of minimization algorithms. There is also minorization, which leads to maximization algorithms, which explains the MM acronym: minorization for maximization and majorization for minimization.</p>
<p>Before the MM principle was formulated as a general approach to algorithm construction there were some important predecessors. Major classes of MM algorithms avant la lettre were the EM Algorithm for maximum likelihood estimation of <span class="citation" data-cites="dempster_laird_rubin_77">Dempster, Laird, and Rubin (<a href="#ref-dempster_laird_rubin_77" role="doc-biblioref">1977</a>)</span>, the <em>Smacof Algorithm</em> for MDS of <span class="citation" data-cites="deleeuw_C_77">De Leeuw (<a href="#ref-deleeuw_C_77" role="doc-biblioref">1977</a>)</span>, the Generalized Weiszfeldt Method* of <span class="citation" data-cites="vosz_eckhardt_80">Vosz and Eckhardt (<a href="#ref-vosz_eckhardt_80" role="doc-biblioref">1980</a>)</span>, and the Quadratic Approximation Method of <span class="citation" data-cites="boehning_lindsay_88">Böhning and Lindsay (<a href="#ref-boehning_lindsay_88" role="doc-biblioref">1988</a>)</span>. The first formulation of the general majorization principle seems to be <span class="citation" data-cites="deleeuw_C_94c">De Leeuw (<a href="#ref-deleeuw_C_94c" role="doc-biblioref">1994</a>)</span>.</p>
<p>Let’s start with a brief introduction to majorization. Minimize a real valued function <span class="math inline">\(\sigma\)</span> over <span class="math inline">\(x\in\mathbb{S}\)</span>, where <span class="math inline">\(\mathbb{S}\)</span> is some subset of <span class="math inline">\(\mathbb{R}^n\)</span>. There are obvious extensions of majorization to functions defined on more general spaces, with values in any partially ordered set, but we do not need that level of generality in this manual. Also majorization applied to <span class="math inline">\(\sigma\)</span> is minorization applied to <span class="math inline">\(-\sigma\)</span>, so concentrating on majorization-minimization and ignoring minorization-maximization causes no loss of generality</p>
<p>Suppose there is a real-valued function <span class="math inline">\(\omega\)</span> on <span class="math inline">\(\mathbb{S}\otimes\mathbb{S}\)</span> such that <span class="math display">\[\begin{align}
\sigma(x)&amp;\leq\omega(x,y)\qquad\forall x,y\in\mathbb{S},\label{eq-maj1}\\
\sigma(x)&amp;=\omega(x,x)\qquad\forall x\in\mathbb{S}.\label{eq-maj2}
\end{align}\]</span> The function <span class="math inline">\(\omega\)</span> is called a <em>majorization scheme</em> for <span class="math inline">\(\sigma\)</span> on <span class="math inline">\(S\)</span>. A majorization scheme is <em>strict</em> if <span class="math inline">\(\sigma(x)&lt;\omega(x,y)\)</span> for all <span class="math inline">\(x,y\in S\)</span> withj <span class="math inline">\(x\not=y\)</span>.</p>
<p>Define <span id="eq-majalg"><span class="math display">\[
x^{(k+1)}\in\mathop{\text{argmin}}_{x\in\mathbb{S}}\omega(x,x^{(k)}),
\tag{5.1}\]</span></span> assuming that <span class="math inline">\(\omega\)</span> attains its (not necessarily unique) minimum over <span class="math inline">\(x\in\mathbb{S}\)</span> for each <span class="math inline">\(y\)</span>. If <span class="math inline">\(x^{(k)}\in\mathop{\text{argmin}}_{x\in\mathbb{S}}\omega(x,x^{(k)})\)</span> then we stop.</p>
<p>By majorization property <span class="math inline">\(\eqref{eq-maj1}\)</span> <span class="math inline">\(\sigma(x^{(k+1)})\leq\omega(x^{(k+1)},x^{(k)})\)</span>. Because we did not stop update rule <a href="#eq-majalg" class="quarto-xref">Equation&nbsp;<span>5.1</span></a> implies <span class="math inline">\(\omega(x^{(k+1)},x^{(k)})&lt;\omega(x^{(k)},x^{(k)})\)</span>. and finally by majorization property <span class="math inline">\(\eqref{eq-maj2}\)</span> <span class="math inline">\(\omega(x^{(k)},x^{(k)})=\sigma(x^{(k)})\)</span>.</p>
<p>If the minimum in <a href="#eq-majalg" class="quarto-xref">Equation&nbsp;<span>5.1</span></a> is attained for a unique <span class="math inline">\(x\)</span> then <span class="math inline">\(\omega(x^{(k+1)},x^{(k)})&lt;\omega(x^{(k)},x^{(k)})\)</span>. If the majorization scheme is strict then <span class="math inline">\(\sigma(x^{(k+1)})&lt;\omega(x^{(k+1)},x^{(k)})\)</span>. Under either of these two additional conditions <span class="math inline">\(\sigma(x^{(k+1)})&lt;\sigma(x^{(k)})\)</span>, which means that the majorization algorithm is a monotone descent algorithm, and if <span class="math inline">\(\sigma\)</span> is bounded below on <span class="math inline">\(\mathbb{S}\)</span> the sequence <span class="math inline">\(\sigma(x^{(k)})\)</span> converges.</p>
<p>Note that we only use the order relation to prove convergence of the sequence of function values. To prove convergence of the <span class="math inline">\(x^{(k)}\)</span> we need stronger compactness and continuity assumptions to apply the general theory of <span class="citation" data-cites="zangwill_69a">Zangwill (<a href="#ref-zangwill_69a" role="doc-biblioref">1969</a>)</span>. For such a proof the argmin in update formula <a href="#eq-majalg" class="quarto-xref">Equation&nbsp;<span>5.1</span></a> can be generalized to <span class="math inline">\(x^{(k+1)}=\phi(x^{(k)})\)</span>, where <span class="math inline">\(\phi\)</span> maps <span class="math inline">\(\mathbb{S}\)</span> into <span class="math inline">\(\mathbb{S}\)</span> such that <span class="math inline">\(\omega(\phi(x),x)\leq\sigma(x)\)</span> for all <span class="math inline">\(x\)</span>.</p>
<p>We give a small illustration in which we minimize <span class="math inline">\(\sigma\)</span> with <span class="math inline">\(\sigma(x)=\sqrt{x}-\log{x}\)</span> over <span class="math inline">\(x&gt;0\)</span>. Obviously we do not need majorization here, because solving <span class="math inline">\(\mathcal{D}\sigma(x)=0\)</span> immediately gives <span class="math inline">\(x=4\)</span> as the solution we are looking for.</p>
<p>To arrive at this solution using majorization we start with <span class="math display">\[\begin{equation}
\sqrt{x}\leq\sqrt{y}+\frac12\frac{x-y}{\sqrt{y}},
(\#eq:sqrtmaj)
\end{equation}\]</span> which is true because a differentiable concave function such as the square root is majorized by its tangent everywhere. Inequality @ref(eq:sqrtmaj) implies <span class="math display">\[\begin{equation}
\sigma(x)\leq\eta(x,y):=\sqrt{y}+\frac12\frac{x-y}{\sqrt{y}}-\log{x}.
(\#eq:examplemaj)
\end{equation}\]</span> Note that <span class="math inline">\(\eta(\bullet,y)\)</span> is convex in its first argument for each <span class="math inline">\(y\)</span>. We have <span class="math inline">\(\mathcal{D}_1\eta(x,y)=0\)</span> if and only if <span class="math inline">\(x=2\sqrt{y}\)</span> and thus the majorization algorithm is <span class="math display">\[\begin{equation}
x^{(k+1)}=2\sqrt{x^{(k)}}
(\#eq:examplealg)
\end{equation}\]</span> The sequence <span class="math inline">\(x^{(k)}\)</span> converges monotonically to the fixed point <span class="math inline">\(x=2\sqrt{x}\)</span>, i.e.&nbsp;to <span class="math inline">\(x=4\)</span>. If <span class="math inline">\(x^{(0)}&lt;4\)</span> the sequence is increasing, if <span class="math inline">\(x^{(0)}&lt;4\)</span> it is decreasing. Also, by l’Hôpital, <span class="math display">\[\begin{equation}
\lim_{x\rightarrow 4}\frac{2\sqrt{x}-4}{x-4}=\frac12
(\#eq:hopi1)
\end{equation}\]</span> and thus convergence to the minimizer is linear with asymptotic convergence rate <span class="math inline">\(\frac12\)</span>. By another application of l’Hôpital <span class="math display">\[\begin{equation}
\lim_{x\rightarrow 4}\frac{\sigma(2\sqrt{x)})-\sigma(4)}{\sigma(x)-\sigma(4)}=\frac14,
(\#eq:hopi2)
\end{equation}\]</span> and convergence to the minimum is linear with asymptotic convergence rate <span class="math inline">\(\frac14\)</span>. Linear convergence to the minimizer is typical for majorization algorithms, as is the twice-as-fast linear convergence to the minimum value.</p>
<p>This small example is also of interest, because we minimize a <em>DC function</em>, the difference of two convex functions. In our example the convex functions are minus the square root and minus the logarithm. Algorithms for minimizing DC functions define other important subclasses of MM algorithms, the <em>DC Algorithm</em> of Tao Pham Dinh (see <span class="citation" data-cites="lethi_tao_18">Le Thi and Tao (<a href="#ref-lethi_tao_18" role="doc-biblioref">2018</a>)</span> for a recent overview), the <em>Concave-Convex Procedure</em> of <span class="citation" data-cites="yuille_rangarajan_03">Yuille and Rangarajan (<a href="#ref-yuille_rangarajan_03" role="doc-biblioref">2003</a>)</span>, and the <em>Half-Quadratic Method</em> of Donald Geman (see <span class="citation" data-cites="nikolova_ng_05">Niikolova and Ng (<a href="#ref-nikolova_ng_05" role="doc-biblioref">2005</a>)</span> for a recent overview). For each of these methods there is a huge literature, with surprisingly little non-overlapping literatures. The first phase of the smacof algorithm, in which we improve the configuration for given disparities, is DC, concave-convex, and half-quadratic.</p>
<p>In the table below we show convergence of @ref(eq:examplealg) starting at <span class="math inline">\(x=1.5\)</span>. The first column show how far <span class="math inline">\(x^{(k)}\)</span> deviates from the minimizer (i.e.&nbsp;from 4), the second shows how far<span class="math inline">\(\sigma(x^{(k)})\)</span> deviates from the minimum (i.e.&nbsp;from <span class="math inline">\(2-\log 4\)</span>). We clearly see the convergence rates <span class="math inline">\(\frac12\)</span> and <span class="math inline">\(\frac14\)</span> in action.</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>itel   1 2.5000000000 0.2055741244 
itel   2 1.5505102572 0.0554992066 
itel   3 0.8698308399 0.0144357214 
itel   4 0.4615431837 0.0036822877 
itel   5 0.2378427379 0.0009299530 
itel   6 0.1207437506 0.0002336744 
itel   7 0.0608344795 0.0000585677 
itel   8 0.0305337787 0.0000146606 
itel   9 0.0152961358 0.0000036675 
itel  10 0.0076553935 0.0000009172 
itel  11 0.0038295299 0.0000002293 
itel  12 0.0019152235 0.0000000573 
itel  13 0.0009577264 0.0000000143 
itel  14 0.0004788919 0.0000000036 
itel  15 0.0002394531 0.0000000009 </code></pre>
</div>
</div>
<p>The first three iterations are shown in the figure below. The vertical lines indicate the value of <span class="math inline">\(x\)</span>, function is in red, and the first three majorizations are in blue.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="intro_files/figure-html/majplot-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-boehning_lindsay_88" class="csl-entry" role="listitem">
Böhning, D., and B. G. Lindsay. 1988. <span>“<span class="nocase">Monotonicity of Quadratic-approximation Algorithms</span>.”</span> <em>Annals of the Institute of Statistical Mathematics</em> 40 (4): 641–63.
</div>
<div id="ref-carroll_chang_70" class="csl-entry" role="listitem">
Carroll, J. D., and J. J. Chang. 1970. <span>“<span class="nocase">Analysis of Individual Differences in Multidimensional scaling via an N-way generalization of "Eckart-Young" Decomposition.</span>”</span> <em>Psychometrika</em> 35: 283–319.
</div>
<div id="ref-coombs_64" class="csl-entry" role="listitem">
Coombs, C. H. 1964. <em><span class="nocase">A Theory of Data</span></em>. Wiley.
</div>
<div id="ref-datorro_18" class="csl-entry" role="listitem">
Datorro, J. 2018. <em>Convex Optimization and Euclidean Distance Geometry</em>. Second Edition. Palo Alto, CA: Meebo Publishing. <a href="https://ccrma.stanford.edu/~dattorro/0976401304.pdf">https://ccrma.stanford.edu/~dattorro/0976401304.pdf</a>.
</div>
<div id="ref-deleeuw_C_77" class="csl-entry" role="listitem">
De Leeuw, J. 1977. <span>“Applications of Convex Analysis to Multidimensional Scaling.”</span> In <em>Recent Developments in Statistics</em>, edited by J. R. Barra, F. Brodeau, G. Romier, and B. Van Cutsem, 133–45. Amsterdam, The Netherlands: North Holland Publishing Company.
</div>
<div id="ref-deleeuw_C_94c" class="csl-entry" role="listitem">
———. 1994. <span>“<span class="nocase">Block Relaxation Algorithms in Statistics</span>.”</span> In <em>Information Systems and Data Analysis</em>, edited by H. H. Bock, W. Lenski, and M. M. Richter, 308–24. Berlin: Springer Verlag. <a href="https://jansweb.netlify.app/publication/deleeuw-c-94-c/deleeuw-c-94-c.pdf">https://jansweb.netlify.app/publication/deleeuw-c-94-c/deleeuw-c-94-c.pdf</a>.
</div>
<div id="ref-deleeuw_E_17e" class="csl-entry" role="listitem">
———. 2017a. <span>“<span class="nocase">Shepard Non-metric Multidimensional Scaling</span>.”</span> 2017. <a href="https://jansweb.netlify.app/publication/deleeuw-e-17-e/deleeuw-e-17-e.pdf">https://jansweb.netlify.app/publication/deleeuw-e-17-e/deleeuw-e-17-e.pdf</a>.
</div>
<div id="ref-deleeuw_E_17p" class="csl-entry" role="listitem">
———. 2017b. <span>“<span class="nocase">Tweaking the SMACOF Engine</span>.”</span> 2017. <a href="https://jansweb.netlify.app/publication/deleeuw-e-17-p/deleeuw-e-17-p.pdf">https://jansweb.netlify.app/publication/deleeuw-e-17-p/deleeuw-e-17-p.pdf</a>.
</div>
<div id="ref-deleeuw_heiser_C_77" class="csl-entry" role="listitem">
De Leeuw, J., and W. J. Heiser. 1977. <span>“Convergence of Correction Matrix Algorithms for Multidimensional Scaling.”</span> In <em>Geometric Representations of Relational Data</em>, edited by J. C. Lingoes, 735–53. Ann Arbor, Michigan: Mathesis Press.
</div>
<div id="ref-deleeuw_heiser_C_80" class="csl-entry" role="listitem">
———. 1980. <span>“Multidimensional Scaling with Restrictions on the Configuration.”</span> In <em>Multivariate Analysis, Volume <span>V</span></em>, edited by P. R. Krishnaiah, 501–22. Amsterdam, The Netherlands: North Holland Publishing Company.
</div>
<div id="ref-deleeuw_heiser_C_82" class="csl-entry" role="listitem">
———. 1982. <span>“Theory of Multidimensional Scaling.”</span> In <em>Handbook of Statistics, Volume <span>II</span></em>, edited by P. R. Krishnaiah and L. Kanal. Amsterdam, The Netherlands: North Holland Publishing Company.
</div>
<div id="ref-deleeuw_mair_A_09c" class="csl-entry" role="listitem">
De Leeuw, J., and P. Mair. 2009. <span>“<span class="nocase">Multidimensional Scaling Using Majorization: SMACOF in R</span>.”</span> <em>Journal of Statistical Software</em> 31 (3): 1–30. <a href="https://www.jstatsoft.org/article/view/v031i03">https://www.jstatsoft.org/article/view/v031i03</a>.
</div>
<div id="ref-dempster_laird_rubin_77" class="csl-entry" role="listitem">
Dempster, A. P., N. M. Laird, and D. B. Rubin. 1977. <span>“<span class="nocase">Maximum Likelihood for Incomplete Data via the EM Algorithm</span>.”</span> <em>Journal of the Royal Statistical Society</em> B39: 1–38.
</div>
<div id="ref-groenen_mathar_heiser_95" class="csl-entry" role="listitem">
Groenen, P. J. F., R. Mathar, and W. J. Heiser. 1995. <span>“<span class="nocase">The Majorization Approach to Multidimensional Scaling for Minkowski Distances</span>.”</span> <em>Journal of Classification</em> 12: 3–19.
</div>
<div id="ref-groenen_vandevelden_16" class="csl-entry" role="listitem">
Groenen, P. J. F., and M. Van de Velden. 2016. <span>“<span class="nocase">Multidimensional Scaling by Majorization: A Review</span>.”</span> <em>Journal of Statistical Software</em> 73 (8): 1–26. <a href="https://www.jstatsoft.org/index.php/jss/article/view/v073i08">https://www.jstatsoft.org/index.php/jss/article/view/v073i08</a>.
</div>
<div id="ref-guttman_68" class="csl-entry" role="listitem">
Guttman, L. 1968. <span>“<span class="nocase">A General Nonmetric Technique for Fitting the Smallest Coordinate Space for a Configuration of Points</span>.”</span> <em>Psychometrika</em> 33: 469–506.
</div>
<div id="ref-harshman_70" class="csl-entry" role="listitem">
Harshman, R. A. 1970. <span>“<span class="nocase">Foundations of the PARAFAC Procedure</span>.”</span> Working Papers in Phonetics 16. UCLA.
</div>
<div id="ref-heiser_deleeuw_R_77" class="csl-entry" role="listitem">
Heiser, W. J., and J. De Leeuw. 1977. <span>“How to Use <span>SMACOF-I</span>.”</span> Department of Data Theory FSW/RUL.
</div>
<div id="ref-kruskal_64a" class="csl-entry" role="listitem">
Kruskal, J. B. 1964a. <span>“<span class="nocase">Multidimensional Scaling by Optimizing Goodness of Fit to a Nonmetric Hypothesis</span>.”</span> <em>Psychometrika</em> 29: 1–27.
</div>
<div id="ref-kruskal_64b" class="csl-entry" role="listitem">
———. 1964b. <span>“<span class="nocase">Nonmetric Multidimensional Scaling: a Numerical Method</span>.”</span> <em>Psychometrika</em> 29: 115–29.
</div>
<div id="ref-lange_16" class="csl-entry" role="listitem">
Lange, K. 2016. <em>MM Optimization Algorithms</em>. SIAM.
</div>
<div id="ref-lethi_tao_18" class="csl-entry" role="listitem">
Le Thi, H. A., and P. D. Tao. 2018. <span>“<span class="nocase">DC Programming and DCA: Thirty Years of Developments</span>.”</span> <em>Mathematical Programming, Series B</em>.
</div>
<div id="ref-mathar_meyer_94" class="csl-entry" role="listitem">
Mathar, R., and R. Meyer. 1994. <span>“Algorithms in Convex Analysis to Fit l_p -Distance Matrices.”</span> <em>Journal of Multivariate Analysis</em> 51: 102–20.
</div>
<div id="ref-meulman_heiser_12" class="csl-entry" role="listitem">
Meulman, J. J., and W. J. Heiser. 2012. <em>IBM SPSS Categories 21</em>. IBM Corporation.
</div>
<div id="ref-nikolova_ng_05" class="csl-entry" role="listitem">
Niikolova, M., and M. Ng. 2005. <span>“Analysis of Half-Quadratic Minimization Methods for Signal and Image Recovery.”</span> <em>SIAM Journal Scientific Computing</em> 27 (3): 937–66.
</div>
<div id="ref-ramsay_77" class="csl-entry" role="listitem">
Ramsay, J. O. 1977. <span>“<span class="nocase">Maximum Likelihood Estimation in Multidimensional Scaling</span>.”</span> <em>Psychometrika</em> 42: 241–66.
</div>
<div id="ref-schoenberg_35" class="csl-entry" role="listitem">
Schoenberg, I. J. 1935. <span>“<span class="nocase">Remarks to Maurice Frechet’s article: Sur la Definition Axiomatique d’une Classe d’Espaces Vectoriels Distancies Applicables Vectoriellement sur l’Espace de Hllbert</span>.”</span> <em>Annals of Mathematics</em> 36: 724–32.
</div>
<div id="ref-shepard_62a" class="csl-entry" role="listitem">
Shepard, R. N. 1962a. <span>“<span class="nocase">The Analysis of Proximities: Multidimensional Scaling with an Unknown Distance Function. I</span>.”</span> <em>Psychometrika</em> 27: 125–40.
</div>
<div id="ref-shepard_62b" class="csl-entry" role="listitem">
———. 1962b. <span>“<span class="nocase">The Analysis of Proximities: Multidimensional Scaling with an Unknown Distance Function. II</span>.”</span> <em>Psychometrika</em> 27: 219–46.
</div>
<div id="ref-takane_young_deleeuw_A_77" class="csl-entry" role="listitem">
Takane, Y., F. W. Young, and J. De Leeuw. 1977. <span>“Nonmetric Individual Differences in Multidimensional Scaling: An Alternating Least Squares Method with Optimal Scaling Features.”</span> <em>Psychometrika</em> 42: 7–67.
</div>
<div id="ref-torgerson_58" class="csl-entry" role="listitem">
Torgerson, W. S. 1958. <em><span class="nocase">Theory and Methods of Scaling</span></em>. New York: Wiley.
</div>
<div id="ref-torgerson_52" class="csl-entry" role="listitem">
Torgerson, W. S. 1952. <span>“<span class="nocase">Multidimensional Scaling: I. Theory and Method</span>.”</span> <em>Psychometrika</em> 17 (4): 401–19.
</div>
<div id="ref-torgerson_65" class="csl-entry" role="listitem">
———. 1965. <span>“<span class="nocase">Multidimensional Scaling of Similarity</span>.”</span> <em>Psychometrika</em> 30 (4): 379–93.
</div>
<div id="ref-vosz_eckhardt_80" class="csl-entry" role="listitem">
Vosz, H., and U. Eckhardt. 1980. <span>“<span class="nocase">Linear Convergence of Generalized <span>W</span>eiszfeld’s Method</span>.”</span> <em>Computing</em> 25: 243–51.
</div>
<div id="ref-young_householder_38" class="csl-entry" role="listitem">
Young, G., and A. S. Householder. 1938. <span>“<span class="nocase">Discussion of a Set of Points in Terms of Their Mutual Distances</span>.”</span> <em>Psychometrika</em> 3 (19-22).
</div>
<div id="ref-yuille_rangarajan_03" class="csl-entry" role="listitem">
Yuille, A. L., and A. Rangarajan. 2003. <span>“<span>The Concave-Convex Procedure</span>.”</span> <em>Neural Computation</em> 15: 915–36.
</div>
<div id="ref-zangwill_69a" class="csl-entry" role="listitem">
Zangwill, W. I. 1969. <em><span class="nocase">Nonlinear Programming: a Unified Approach</span></em>. Englewood-Cliffs, N.J.: Prentice-Hall.
</div>
</div>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./index.html" class="pagination-link" aria-label="Preface">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Preface</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./basics.html" class="pagination-link" aria-label="smacof Basics">
        <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">smacof Basics</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>